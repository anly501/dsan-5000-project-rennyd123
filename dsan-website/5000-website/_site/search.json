[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Homepage",
    "section": "",
    "text": "Hello! My name is Renee DeMaio (jrd154) and this is the homepage for my DSAN 5000 website which will be home to my research project about injuries and injury prevention in sports. Feel free to click through the tabs to see more of my work.\nTo learn more about me, click here."
  },
  {
    "objectID": "tabs/data_cleaning/data_cleaning.html",
    "href": "tabs/data_cleaning/data_cleaning.html",
    "title": "General Injury Data",
    "section": "",
    "text": "Injury & Injury Prevention Data Soccer\nIn order to clean this data set, I dropped all unnecessary columns, added binary values to columns that expressed binary data, and renamed columns appropriately.\nThe cleaned data can be found here and the cleaning code can be found here..\n\n\nInjury Data From Different Activites\nIn order to clean this data, I first had to reformat it into a tabular format and then had drop unnecessary columns.\nThe cleaned data can be found here and the cleaning code can be found here.\n\n\nNBA Data\n\nNBA Injury Data\nThe main cleaning that I had to do for this data set was subsetting to ensure that I had the columns that I needed. I dropped the “Acquired” column and then also split the “Notes” column into “Notes” and “Injury Status” which extracts information about the duration of the stint on the Injured List into a new column.\nThe link to my cleaned can be found here and the link my code can be found here.\n\n\nNBA Data\nThe cleaning done for this data set included joining all three data sets - the players, teams, and rankings data sets - together. I dropped unnecessary columns and filtered out NA values. The cleaned data set and associated code can be found below.\nThe cleaned data set can be found here and the cleaning code can be found here.\n\n\n\nNFL Data\n\nNFL Concussion Data\nThe cleaning done to this data set at this point has been minimal. I simply dropped unnecessary columns in order to make the dataset more suitable to my project.\nThe link to the cleaned data set is here and the link to the cleaning code is here.\n\n\nNFL Game Injury Data\nThe data cleaning for this data set involved merging two data sets together and dropping columns to make the data set more suitable for my project.\nThe link to the cleaned data can be found here and the link to the cleaning code can be found here."
  },
  {
    "objectID": "tabs/arm/arm.html",
    "href": "tabs/arm/arm.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "1+1"
  },
  {
    "objectID": "tabs/dimensionality_reduction/dimensionality_reduction.html",
    "href": "tabs/dimensionality_reduction/dimensionality_reduction.html",
    "title": "Project Proposal",
    "section": "",
    "text": "The objective of my project is to explore what factors can affect athletes ability to get injuries. There are various factors that can be involved, such as how an athlete warms up, the nature of the position or sport that they play, the location that they play in or on, etc. My goal is to explore if any factors have positive correlations with injuries so that athletes can be sure to do what they can avoid injury. For this section of the paper, I’ve decided to use a dataset regarding injuries that soccer players have since it has a lot of features including what injury prevention measures they took, what risk factors they have, and information about their position and number of injuries. To explore my data further, I plan to use Python, Pandas, scikit-learn, numpy, and matplotlib.\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd \nimport matplotlib as plt\nfrom sklearn.preprocessing import StandardScaler\n\nfile_path = \"../../../../data/01-modified-data/injury_prevention_data_soccer.csv\"\nsoccer_injury = pd.read_csv(file_path)\n\nsoccer_injury.columns = soccer_injury.columns.str.strip()\n\nsoccer_injury = soccer_injury.replace({\"yes\": 1, \"no\": 0})\nsoccer_injury.head()\n\nx_vars = ['Prevention Measure Stretching', 'Prevention Measure Warm Up',\n       'Prevention Measure Specific Strength Exercises',\n       'Prevention Measure Bracing', 'Prevention Measure Taping',\n       'Prevention Measure Shoe Insoles', 'Prevention Measure Face Masks',\n       'Prevention Measure Medical Corset']\ntarget_var = \"Position\"\n\nsi_subset = soccer_injury[x_vars]\nprint(si_subset.head())\n\nX = si_subset.copy()\ny = soccer_injury[target_var].values.copy()\nX = StandardScaler().fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5000)\n\n   Prevention Measure Stretching  Prevention Measure Warm Up  \\\n0                              1                           0   \n1                              1                           1   \n2                              1                           0   \n3                              1                           1   \n4                              1                           1   \n\n   Prevention Measure Specific Strength Exercises  Prevention Measure Bracing  \\\n0                                               1                           0   \n1                                               0                           0   \n2                                               0                           0   \n3                                               1                           0   \n4                                               0                           0   \n\n   Prevention Measure Taping  Prevention Measure Shoe Insoles  \\\n0                          0                                0   \n1                          0                                0   \n2                          0                                1   \n3                          0                                0   \n4                          1                                0   \n\n   Prevention Measure Face Masks  Prevention Measure Medical Corset  \n0                              0                                  0  \n1                              0                                  0  \n2                              0                                  0  \n3                              0                                  0  \n4                              0                                  0"
  },
  {
    "objectID": "tabs/dimensionality_reduction/dimensionality_reduction.html#principal-component-analysis-pca",
    "href": "tabs/dimensionality_reduction/dimensionality_reduction.html#principal-component-analysis-pca",
    "title": "Project Proposal",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\nIn the following section, I will apply PCA to my dataset, determine the optimal number of principle components to retain, and create several visualizations of my results. My analysis of the findings will be included in the project report section.\n\nDetermining Optimal Number of Principal Components to Retain\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nclf = RandomForestClassifier(random_state=5000)\n\nnum_features_list = []\ntrain_accuracy_list = []\ntest_accuracy_list = []\n\nfor num_features in range(1, X.shape[1] + 1):\n    X_train_subset = X_train[:, :num_features]\n    X_test_subset = X_test[:, :num_features]\n\n    clf.fit(X_train_subset, y_train)\n\n    y_train_pred = clf.predict(X_train_subset)\n    y_test_pred = clf.predict(X_test_subset)\n\n    train_accuracy = accuracy_score(y_train, y_train_pred)\n    test_accuracy = accuracy_score(y_test, y_test_pred)\n\n    num_features_list.append(num_features)\n    train_accuracy_list.append(train_accuracy)\n    test_accuracy_list.append(test_accuracy)\n\nmax_test_accuracy_index = np.argmax(test_accuracy_list)\noptimal_num_features = num_features_list[max_test_accuracy_index]\nmax_test_accuracy = test_accuracy_list[max_test_accuracy_index]\n\nprint(\"--------------------\")\nprint(f\"The optimal numbers of features is: {optimal_num_features}\")\nprint(\"--------------------\")\n\nplt.figure(figsize=(10, 6))\nplt.plot(num_features_list, train_accuracy_list, label='Training Accuracy', marker='o')\nplt.plot(num_features_list, test_accuracy_list, label='Test Accuracy', marker='o')\n\nplt.axhline(y=max_test_accuracy, color='gray', linestyle='--', label=f'Max Test Accuracy: {max_test_accuracy:.3f}')\n\nplt.xlabel('Number of Features')\nplt.ylabel('Accuracy')\nplt.title('Training and Test Accuracy vs Number of Features')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n--------------------\nThe optimal numbers of features is: 5\n--------------------\n\n\n\n\n\nWhen visualizing the relationship between the number of features and the training/test accuracies, we can see that the ideal number of components to preserve is 5. At this point, both training and test accuracies peak. Further inclusion of features beyond this juncture results in diminishing returns, therefore opting for 5 principal components is ideal to achieve the best possible model performance.\n\n\nVisualizing Components in 2D and 3D\n\nfrom sklearn.decomposition import PCA\nimport numpy as np\nfrom numpy import linalg as LA\nimport matplotlib.pyplot as plt\nimport plotly.io as pio\n\nprint(\"--------------------\")\nprint('\\nNUMERIC MEAN:',np.mean(X,axis=0))\nprint(\"X SHAPE\",X.shape)\nprint(\"NUMERIC COV:\")\nprint(np.cov(X.T))\nprint(\"--------------------\")\n\nw, v1 = LA.eig(np.cov(X.T))\nprint(\"--------------------\")\nprint(\"\\nCOV EIGENVALUES:\",w)\nprint(\"COV EIGENVECTORS (across rows):\")\nprint(v1.T)\nprint(\"--------------------\")\n\npca = PCA(n_components=5) \npca.fit(X)\nprint(\"--------------------\")\nprint('\\nPCA')\nprint(pca.components_)\nv2=pca.components_\nprint(\"--------------------\")\n\n#print(v1/v2)\n\nprint(\"--------------------\")\nprint(\"2D Visualization\")\nprint(\"--------------------\")\n\nX_2d = np.dot(X, v2[:2, :].T)\nplt.figure(figsize=(8, 6))\nplt.scatter(X_2d[:, 0], X_2d[:, 1], marker=\".\", cmap=\"viridis\")\n\nplt.quiver(0, 0, v2[0, 0], v2[0, 1], color='r', scale=3, label='PCA 1')\nplt.quiver(0, 0, v2[1, 0], v2[1, 1], color='g', scale=3, label='PCA 2')\n\nplt.xlabel('PCA 1')\nplt.ylabel('PCA 2')\nplt.title('2D Projection of Data with PCA Components')\nplt.legend()\nplt.show()\n\nprint(\"--------------------\")\nprint(\"3D Visualization\")\nprint(\"--------------------\")\n\njitter = 0.01 * np.random.randn(*X.shape)\nX += jitter\n\nfig = plt.figure()\nax = fig.add_subplot(projection='3d')\nax.scatter(X[:, 0], X[:, 1], X[:, 2], marker=\".\")\nv1=v1*1000\nv2=v2*1000\n\nax.quiver(0,0,0,v1[0,0],v1[1,0],v1[2,0])\nax.quiver(0,0,0,v1[0,1],v1[1,1],v1[2,1])\nax.quiver(0,0,0,v1[0,2],v1[1,2],v1[2,2])\n\nax.quiver(0,0,0,v2[0,0],v2[1,0],v2[2,0])\nax.quiver(0,0,0,v2[0,1],v2[1,1],v2[2,1])\nax.quiver(0,0,0,v2[0,2],v2[1,2],v2[2,2])\nplt.show()\n\n--------------------\n\nNUMERIC MEAN: [-0.00201034  0.01068603 -0.00032657 -0.00938756  0.00730824  0.01352997\n -0.05410685 -0.02640217]\nX SHAPE (139, 8)\nNUMERIC COV:\n[[ 1.01069218  0.18935776 -0.00196516  0.05282107  0.00661368 -0.0647079\n  -0.01144854  0.03709847]\n [ 0.18935776  1.0701004   0.04656088  0.08867232  0.0978689   0.04056186\n  -0.00317229  0.07436123]\n [-0.00196516  0.04656088  1.02891192  0.02010355  0.02930677 -0.0087439\n   0.01279287  0.12234954]\n [ 0.05282107  0.08867232  0.02010355  1.10248723  0.29394645  0.0408016\n   0.00382599 -0.04527452]\n [ 0.00661368  0.0978689   0.02930677  0.29394645  1.06298682  0.11608761\n   0.04166169  0.08416888]\n [-0.0647079   0.04056186 -0.0087439   0.0408016   0.11608761  1.05299611\n  -0.02847412 -0.07680258]\n [-0.01144854 -0.00317229  0.01279287  0.00382599  0.04166169 -0.02847412\n   0.07348617 -0.00430721]\n [ 0.03709847  0.07436123  0.12234954 -0.04527452  0.08416888 -0.07680258\n  -0.00430721  1.11144041]]\n--------------------\n--------------------\n\nCOV EIGENVALUES: [0.07003203 1.49266597 1.27841776 0.74063531 0.81583674 1.13377912\n 0.95030564 1.03142866]\nCOV EIGENVECTORS (across rows):\n[[-1.33683435e-02 -2.97466331e-03  1.33068460e-02 -8.88273193e-03\n   4.97145708e-02 -3.59882108e-02 -9.97819789e-01 -1.20599490e-02]\n [ 2.26642156e-01  4.24301544e-01  1.47777672e-01  5.77906328e-01\n   5.90375626e-01  1.80501381e-01  1.32844095e-02  1.77445669e-01]\n [-3.35007842e-01 -3.00308507e-01 -3.26799046e-01  2.95386218e-01\n   1.95538493e-01  4.21120366e-01  4.77774670e-04 -6.22861156e-01]\n [-1.06989016e-01  2.27160808e-02 -9.94989265e-02  5.91283697e-01\n  -6.90799512e-01  2.24055749e-01 -5.15162714e-02  3.13786571e-01]\n [-6.92887139e-01  6.28699099e-01 -8.96758129e-02 -8.98398522e-03\n  -2.31688373e-02 -3.21106863e-01  1.80737231e-02 -1.12023749e-01]\n [ 5.84894793e-01  4.28028380e-01 -4.21865229e-01  9.59288174e-03\n  -2.57602374e-01 -1.27636013e-01 -1.74678276e-02 -4.62247155e-01]\n [-4.41512706e-02 -1.48491137e-02 -8.03697013e-01 -1.85254246e-01\n   2.26968448e-01  1.02731819e-01 -6.93886337e-03  5.05435945e-01]\n [ 2.51768948e-02 -3.88029360e-01 -1.71307105e-01  4.41116848e-01\n   1.23025847e-01 -7.80145091e-01  2.90707107e-02 -1.61960842e-02]]\n--------------------\n--------------------\n\nPCA\n[[ 2.26642156e-01  4.24301544e-01  1.47777672e-01  5.77906328e-01\n   5.90375626e-01  1.80501381e-01  1.32844095e-02  1.77445669e-01]\n [ 3.35007842e-01  3.00308507e-01  3.26799046e-01 -2.95386218e-01\n  -1.95538493e-01 -4.21120366e-01 -4.77774670e-04  6.22861156e-01]\n [-5.84894793e-01 -4.28028380e-01  4.21865229e-01 -9.59288174e-03\n   2.57602374e-01  1.27636013e-01  1.74678276e-02  4.62247155e-01]\n [ 2.51768948e-02 -3.88029360e-01 -1.71307105e-01  4.41116848e-01\n   1.23025847e-01 -7.80145091e-01  2.90707107e-02 -1.61960842e-02]\n [-4.41512706e-02 -1.48491137e-02 -8.03697013e-01 -1.85254246e-01\n   2.26968448e-01  1.02731819e-01 -6.93886337e-03  5.05435945e-01]]\n--------------------\n--------------------\n2D Visualization\n--------------------\n--------------------\n3D Visualization\n--------------------\n\n\n/var/folders/kw/p7j6fdpx3vvgm6c0krhwpvb80000gn/T/ipykernel_33979/625416821.py:36: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  plt.scatter(X_2d[:, 0], X_2d[:, 1], marker=\".\", cmap=\"viridis\")\n\n\n\n\n\n\n\n\n\n\nInteractive Versions of 2D and 3D Graphs\n\nimport numpy as np\nimport plotly.graph_objs as go\nfrom sklearn.decomposition import PCA\nimport plotly\nimport plotly.io as pio\n\n\npca = PCA(n_components=5)\npca.fit(X)\n\nv2 = pca.components_\n\nX_2d = np.dot(X, v2[:2, :].T)\n\nscatter_2d = go.Scatter(\n    x=X_2d[:, 0],\n    y=X_2d[:, 1],\n    mode='markers',\n    marker=dict(\n        size=8,\n        color=np.arange(len(X)),  \n        colorscale='Viridis',\n        opacity=0.8\n    ),\n    text=['Point {}'.format(i) for i in range(len(X))],  \n)\n\nlayout_2d = go.Layout(\n    title='2D Projection of Data with PCA Components',\n    xaxis=dict(title='PCA 1'),\n    yaxis=dict(title='PCA 2'),\n    showlegend=False,\n)\n\nfig_2d = go.Figure(data=[scatter_2d], layout=layout_2d)\n\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\nplotly.offline.plot(fig_2d, filename='PCA_2d_interactive.html')\n\nfrom IPython.display import IFrame\nIFrame(src='PCA_2d_interactive.html', width=800, height=600)\n\n\n                                                \n\n\n\n        \n        \n\n\n\njitter = 0.01 * np.random.randn(*X.shape)\nX += jitter\n\nscatter_3d = go.Scatter3d(\n    x=X[:, 0],\n    y=X[:, 1],\n    z=X[:, 2],\n    mode='markers',\n    marker=dict(\n        size=6,\n        color=np.arange(len(X)),  \n        colorscale='Viridis',\n        opacity=0.8\n    ),\n    text=['Point {}'.format(i) for i in range(len(X))],  \n)\n\nlayout_3d = go.Layout(\n    title='3D Projection of Data with PCA Components',\n    scene=dict(\n        xaxis=dict(title='PCA 1'),\n        yaxis=dict(title='PCA 2'),\n        zaxis=dict(title='PCA 3'),\n    ),\n)\n\nfig_3d = go.Figure(data=[scatter_3d], layout=layout_3d)\n\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\nplotly.offline.plot(fig_3d, filename='PCA_3d_interactive.html')\n\nIFrame(src='PCA_3d_interactive.html', width=800, height=600)\n\n\n                                                \n\n\n\n        \n        \n\n\n\n\nMost Frequently Used Prevention Measures\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npm_counts = si_subset.sum()\npm_counts = pm_counts.sort_values(ascending=False)\n\nsns.barplot(x=pm_counts.index, y=pm_counts.values, color='skyblue')\nplt.title('Prevention Measures Counts')\nplt.xlabel('Prevention Measures')\nplt.ylabel('Count')\nplt.xticks(rotation=45, ha='right')  \nplt.tight_layout()\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):"
  },
  {
    "objectID": "tabs/dimensionality_reduction/dimensionality_reduction.html#dimensionality-reduction-with-t-sne",
    "href": "tabs/dimensionality_reduction/dimensionality_reduction.html#dimensionality-reduction-with-t-sne",
    "title": "Project Proposal",
    "section": "Dimensionality Reduction with t-SNE",
    "text": "Dimensionality Reduction with t-SNE\nIn order to find the optimal parameter for t-SNE, we can test out multiple values of perplexity. In the following code, I test the following values of perplexity: 5, 10, 30, 50, and 100. In order to explore what effect different values of perplexity have, I, again, utilize interactive graphs.\n\nPerplexity = 5\n\nfrom sklearn.manifold import TSNE\nimport plotly.express as px\n\nX_embedded = TSNE(n_components=2, learning_rate='auto',init='random', perplexity=5).fit_transform(X)\n\ndata = {\n    'x': X_embedded[:, 0],\n    'y': X_embedded[:, 1],\n    'Stretching': si_subset[\"Prevention Measure Stretching\"],\n    'Warm Up': si_subset[\"Prevention Measure Warm Up\"],\n    'Strength Exercises': si_subset[\"Prevention Measure Specific Strength Exercises\"],\n    'Bracing': si_subset[\"Prevention Measure Bracing\"],\n    'Taping': si_subset[\"Prevention Measure Taping\"],\n    'Shoe Insoles': si_subset[\"Prevention Measure Shoe Insoles\"],\n    'Face Masks': si_subset[\"Prevention Measure Face Masks\"],\n    'Medical Corset': si_subset[\"Prevention Measure Medical Corset\"]\n}\n\ndf = pd.DataFrame(data)\n\nprint(\"RESULTS\") \nprint(\"shape : \",X_embedded.shape)\nprint(\"First few points : \\n\",X_embedded[0:2,:])\n\nscatter_fig_5 = px.scatter(df, x='x', y='y', \n                         hover_data=['Stretching', 'Warm Up', \"Strength Exercises\", \"Bracing\", \"Taping\", \"Shoe Insoles\", \"Face Masks\", \"Medical Corset\"], \n                         template='simple_white',\n                         title='Perplexity = 5')\n\nscatter_fig_5.show()\n\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\nplotly.offline.plot(scatter_fig_5, filename='per5_interactive.html')\n\nIFrame(src='per5_interactive.html', width=800, height=600)\n\nRESULTS\nshape :  (139, 2)\nFirst few points : \n [[-41.897953  27.206026]\n [ 29.91853   25.520626]]\n\n\n\n                                                \n\n\n\n        \n        \n\n\n\n\nPerplexity = 10\n\n\nX_embedded = TSNE(n_components=2, learning_rate='auto',init='random', perplexity=10).fit_transform(X)\n\ndata = {\n    'x': X_embedded[:, 0],\n    'y': X_embedded[:, 1],\n    'Stretching': si_subset[\"Prevention Measure Stretching\"],\n    'Warm Up': si_subset[\"Prevention Measure Warm Up\"],\n    'Strength Exercises': si_subset[\"Prevention Measure Specific Strength Exercises\"],\n    'Bracing': si_subset[\"Prevention Measure Bracing\"],\n    'Taping': si_subset[\"Prevention Measure Taping\"],\n    'Shoe Insoles': si_subset[\"Prevention Measure Shoe Insoles\"],\n    'Face Masks': si_subset[\"Prevention Measure Face Masks\"],\n    'Medical Corset': si_subset[\"Prevention Measure Medical Corset\"]\n}\n\ndf = pd.DataFrame(data)\n\nprint(\"RESULTS\") \nprint(\"shape : \",X_embedded.shape)\nprint(\"First few points : \\n\",X_embedded[0:2,:])\n\nscatter_fig_10 = px.scatter(df, x='x', y='y', \n                         hover_data=['Stretching', 'Warm Up', \"Strength Exercises\", \"Bracing\", \"Taping\", \"Shoe Insoles\", \"Face Masks\", \"Medical Corset\"], \n                         template='simple_white',\n                         title='Perplexity = 10')\n\nscatter_fig_10.show()\n\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\nplotly.offline.plot(scatter_fig_10, filename='per10_interactive.html')\n\nIFrame(src='per10_interactive.html', width=800, height=600)\n\nRESULTS\nshape :  (139, 2)\nFirst few points : \n [[  1.0702138  32.93463  ]\n [-21.92765   -10.702206 ]]\n\n\n\n                                                \n\n\n\n        \n        \n\n\n\n\nPerplexity = 30\n\nX_embedded = TSNE(n_components=2, learning_rate='auto',init='random', perplexity=30).fit_transform(X)\n\ndata = {\n    'x': X_embedded[:, 0],\n    'y': X_embedded[:, 1],\n    'Stretching': si_subset[\"Prevention Measure Stretching\"],\n    'Warm Up': si_subset[\"Prevention Measure Warm Up\"],\n    'Strength Exercises': si_subset[\"Prevention Measure Specific Strength Exercises\"],\n    'Bracing': si_subset[\"Prevention Measure Bracing\"],\n    'Taping': si_subset[\"Prevention Measure Taping\"],\n    'Shoe Insoles': si_subset[\"Prevention Measure Shoe Insoles\"],\n    'Face Masks': si_subset[\"Prevention Measure Face Masks\"],\n    'Medical Corset': si_subset[\"Prevention Measure Medical Corset\"]\n}\n\ndf = pd.DataFrame(data)\n\nprint(\"RESULTS\") \nprint(\"shape : \",X_embedded.shape)\nprint(\"First few points : \\n\",X_embedded[0:2,:])\n\nscatter_fig_30 = px.scatter(df, x='x', y='y', \n                         hover_data=['Stretching', 'Warm Up', \"Strength Exercises\", \"Bracing\", \"Taping\", \"Shoe Insoles\", \"Face Masks\", \"Medical Corset\"], \n                         template='simple_white',\n                         title='Perplexity = 30')\n\nscatter_fig_30.show()\n\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\nplotly.offline.plot(scatter_fig_30, filename='per30_interactive.html')\n\nIFrame(src='per30_interactive.html', width=800, height=600)\n\nRESULTS\nshape :  (139, 2)\nFirst few points : \n [[ 5.318501   3.3097527]\n [-0.8121508 -6.4789705]]\n\n\n\n                                                \n\n\n\n        \n        \n\n\n\n\nPerplexity = 50\n\nX_embedded = TSNE(n_components=2, learning_rate='auto',init='random', perplexity=50).fit_transform(X)\n\ndata = {\n    'x': X_embedded[:, 0],\n    'y': X_embedded[:, 1],\n    'Stretching': si_subset[\"Prevention Measure Stretching\"],\n    'Warm Up': si_subset[\"Prevention Measure Warm Up\"],\n    'Strength Exercises': si_subset[\"Prevention Measure Specific Strength Exercises\"],\n    'Bracing': si_subset[\"Prevention Measure Bracing\"],\n    'Taping': si_subset[\"Prevention Measure Taping\"],\n    'Shoe Insoles': si_subset[\"Prevention Measure Shoe Insoles\"],\n    'Face Masks': si_subset[\"Prevention Measure Face Masks\"],\n    'Medical Corset': si_subset[\"Prevention Measure Medical Corset\"]\n}\n\ndf = pd.DataFrame(data)\n\nprint(\"RESULTS\") \nprint(\"shape : \",X_embedded.shape)\nprint(\"First few points : \\n\",X_embedded[0:2,:])\n\nscatter_fig_50 = px.scatter(df, x='x', y='y', \n                         hover_data=['Stretching', 'Warm Up', \"Strength Exercises\", \"Bracing\", \"Taping\", \"Shoe Insoles\", \"Face Masks\", \"Medical Corset\"], \n                         template='simple_white',\n                         title='Perplexity = 50')\n\nscatter_fig_50.show()\n\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\nplotly.offline.plot(scatter_fig_50, filename='per50_interactive.html')\n\nIFrame(src='per50_interactive.html', width=800, height=600)\n\nRESULTS\nshape :  (139, 2)\nFirst few points : \n [[ 3.780566  -0.6897387]\n [ 4.4418926 -3.2397008]]\n\n\n\n                                                \n\n\n\n        \n        \n\n\n\n\nPerplexity = 100\n\nX_embedded = TSNE(n_components=2, learning_rate='auto',init='random', perplexity=100).fit_transform(X)\n\ndata = {\n    'x': X_embedded[:, 0],\n    'y': X_embedded[:, 1],\n    'Stretching': si_subset[\"Prevention Measure Stretching\"],\n    'Warm Up': si_subset[\"Prevention Measure Warm Up\"],\n    'Strength Exercises': si_subset[\"Prevention Measure Specific Strength Exercises\"],\n    'Bracing': si_subset[\"Prevention Measure Bracing\"],\n    'Taping': si_subset[\"Prevention Measure Taping\"],\n    'Shoe Insoles': si_subset[\"Prevention Measure Shoe Insoles\"],\n    'Face Masks': si_subset[\"Prevention Measure Face Masks\"],\n    'Medical Corset': si_subset[\"Prevention Measure Medical Corset\"]\n}\n\ndf = pd.DataFrame(data)\n\nprint(\"RESULTS\") \nprint(\"shape : \",X_embedded.shape)\nprint(\"First few points : \\n\",X_embedded[0:2,:])\n\nscatter_fig_100 = px.scatter(df, x='x', y='y', \n                         hover_data=['Stretching', 'Warm Up', \"Strength Exercises\", \"Bracing\", \"Taping\", \"Shoe Insoles\", \"Face Masks\", \"Medical Corset\"], \n                         template='simple_white',\n                         title='Perplexity = 100')\n\nscatter_fig_100.show()\n\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\nplotly.offline.plot(scatter_fig_100, filename='per100_interactive.html')\n\nIFrame(src='per100_interactive.html', width=800, height=600)\n\nRESULTS\nshape :  (139, 2)\nFirst few points : \n [[-0.2851685   1.3840965 ]\n [-0.0115345   0.28123257]]"
  },
  {
    "objectID": "tabs/dimensionality_reduction/dimensionality_reduction.html#project-report",
    "href": "tabs/dimensionality_reduction/dimensionality_reduction.html#project-report",
    "title": "Project Proposal",
    "section": "Project Report",
    "text": "Project Report\nIn this dimensionality reduction section of my project, I first applied PCA to my dataset and determined the optimal number of prinicpal components to retain which was 5. I also was able to create a number of visualizations to display the structure and PCA’s groupings of my data. Next, I applied t-SNE to my dataset and created multiple visualizations displaying the impact of the different values of perplexity, all of which are interactive plots. My analyses of the results of my application of PCA and t-SNE can be found below.\n\nAnalysis of PCA Results\nUsing PCA, we can determine that the optimal number of principal components to retain is 5. Based on the 3D visualization, we can see that PCA is grouping the data into the clusters, likely based on what injury prevention measures that an athlete took. The interactive version of the plot confirms that the clusters are created using prevention measures as each colour represents a different prevention measure that was used to group the points. While it is common for athletes to do more than one prevention measure, PCA is grouping them based off of one common one.\nThere seem to be one or two evident outliers based on the non-interactive 3D plot. The first is most easily seen in the 3D plot, and it sits in the position (1.5, -3.5, 0.25). This point is not around many other points, which could be due to the athlete using an uncommon injury prevention measure. The other notable outlier has the approximate position (1.25, -4, -1.5). The reasons for this outlier are likely the same as the first.\nThe 2D visualization is a little less obvious, as it fails the capture the dimensionality of the data in the same way that the 3D plot does. However, we do see one obvious outlier in the top right corner of the plot, with the approximate position of (3.25, 8.25).\n\n\nAnalysis of t-SNE Results\nThe different values of perplexity seem to effect how tightly the points are clustered together. When values of perplexity are lower, points are very tightly grouped, whereas when perplexity is higher, points are much more spread out. When perplexity is 10 and 30, we see relatively tightly grouped points, but not too tight. I believe that a perplexity value of 10 gives a good indication of points that are similar to each other as, compared to other perplexity values, more tightly groups the points that are obviously in groups (such as the ones around (0, -7)). When we hover over the points, we can see that clusters are formed by what prevention measures an athlete took. The more obvious clusters are formed of points in which athletes took all of the same prevention measures, whereas the ones that don’t obviously belong to clusters may have one or two differing prevention measures.\nPCA and t-SNE have relatively similar results. Both of the dimensionality reduction techniques form groups based on clusters, indicating that they both recognize that the target may be closely related to what prevention measures an athlete took.\n\n\nEvaluation and Comparison\nIn my specific case, PCA and t-SNE were both very effective as retaining the data structure and information. Both methods were able to effectively group the data together into meaningful clusters. Visually, I found t-SNE more useful for visualizing how clusters were formed and what common variables clusters have. However, I found PCA more useful for visualizing the clusters and how similar or different each of the clusters were. The nature of the 3D plot made it quite easy to see how similar or dissimilar the clusters were since the distance between the clusters can easily be seen.\nPCA is more effective for linear data, whereas t-SNE can be more effective for non-linear data. If your objective is to preserve the overall structure of the data, because PCA focuses on overall variance, PCA may do a better job that t-SNE, as t-SNE focuses on smaller-scale relationships between points. However, because PCA focuses on variance, it is very sensitive to outliers, so if a dataset has a lot of outliers, PCA may have a hard time with it. In that case, t-SNE would be a better choice as it focuses on local relationships rather than variance. Both methods create opportunity for various visualizations. Due to PCA’s focus on overall structure and variance and t-SNE’s focus on preserving smaller-scale relationships within the data, each method’s visualizations will have its benefits and caveats. PCA’s visualizations will give a more comprehensive picture of the overarching structure of the data but may be more affected by outliers, whereas t-SNE will be more robust when it comes to outliers but may be less effective in showing overall data structure."
  },
  {
    "objectID": "tabs/code/code.html",
    "href": "tabs/code/code.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "You can access my github repo here."
  },
  {
    "objectID": "tabs/decision_tree/classification.html",
    "href": "tabs/decision_tree/classification.html",
    "title": "Methods",
    "section": "",
    "text": "Classification decision trees are a great tool for helping you figure out information about a topic. Let’s use the game 20 questions as an example. Say I ask you to guess what food I am thinking of. The food I’ve selected is gummy worms. We can use a series of yes or no questions to figure out what I’ve selected. You could ask if it is a healthy food, to which my answer would be no. You could ask if it was protein, I’d say no. You could ask if it was a carb, I’d say yes. You could ask if it was sweet, I’d say yes. You could ask if there is dairy in it, I’d say no. You can ask if it was a candy, I’d say yes. And so on until you finally ask if I selected gummy worms, to which I’d say yes.\nThe appeal of decision trees is their ability to make the identification process very simple. Each question you ask helps to narrow down the number of possibilities. We can think of it as a visual flowchart that categorizes items based on their features. Going back to our previous example, we can classify foods based on their flavour profile or nutritional category. As we ask more questions, we will eventually get to our gummy worms.\nMaking an effective decision tree involves initially posing questions that will reveal the most amount of information about the object’s group. We want to figure out distinctive information as fast as possible, so we can distinguish between the groups. Decision trees have applications in many different fields, ranging from sorting out spam emails to diagnosing medical conditions. They are a tool that streamlines intricate decision-making by breaking it down into a sequence of straightforward choices."
  },
  {
    "objectID": "tabs/introduction/introduction.html",
    "href": "tabs/introduction/introduction.html",
    "title": "Introduction to sports injuries and injury prevention",
    "section": "",
    "text": "Injuries and injury prevention in sports is an increasingly important topic in sports as athletes continue to increase the ability to perform well. As the world continues to get more competitive and as sport remains an important part of our cultures and societies, keeping athletes healthy becomes more important. So far, the field has not had a lot of research conducted, with studies mainly focusing on how we can use machine learning and artificial intelligence to predict and prevent sports injuries. There are many different voices in this realm including voices discussing the machine learning and artificial intelligence side of things and how we can make those tools more efficient, as well as voices coming from the medical point of view striving to reduce the frequency of injuries. I would like to explore both the machine learning and artificial intelligence perspective and how we can apply those findings to athletes’ daily lives.\n\n10 questions I aim to answer\n\nIn what sports is machine learning and artificial intelligence most useful for predicting and prevention injuries?\nHow can we use machine learning and artificial intelligence to predict and prevent sports injuries?\nIs there a difference in the data between predicting injuries in male athletes versus predicting injuries in female athletes?\nHow can we make the results of many of these studies more useful to clinicians?\nHow can we more efficiently use machine learning to predict and prevent sports injuries?\nHow can we more efficiently use artificial intelligence to predict and prevent sports injuries?\nWhat does it look like on the day to day sports level when advice from machine learning findings are incorporated?\nWhat does using machine learning and artificial intelligence mean for the future of sports?\nWhat effect has machine learning and artificial intelligence had on predicting and preventing sports injuries so far?\nWhat are the goals of the field?\n\n\n\nSummary: The Severity of Sports Injuries by Willem van Mechelen\nThe severity of sports injuries can be broken down into “6 criteria: (i) nature of sports injury; (ii) duration and nature of treatment; (iii) sporting time lost; (iv) working time lost; (v) permanent damage; and (vi) monetary cost” (Mechelen 1997). Sports injuries can be divided into several broad categories based on type of injury. In more traditional and popular sports such as football and basketball, the most common sports injuries are in the contusions or sprains on the lower extremities, though other sports such as skiing, parachute jumping, horse riding, etc, see fractures more often. We can use “data on the duration and nature of treatment” to “determine the severity of an injury” and help us evaluate what medical treatments will be most effective. From there, we can evaluate how much, if any, permanent damage there will be following the injury. (Mechelen 1997). Time away from the sport can also have an effect on an athlete’s psychosocial wellness. Fortunately 50 to 60% of all sports injuries do not lead to a substantial loss of sporting time (Mechelen 1997). Additionally, there are many costs that come from sports injuries including financial, social and financial. The two main types of social costs can be subdivided into quantifiable and unquantifiable. Quantifiable costs include “insurance costs and legal expenses” whereas unquantifiable costs include any harm caused to the athlete’s psychosocial wellness. The two main types of financial costs of sports injuries include direct costs which includes “the cost of medical treatment” and indirect costs which are considered to be “expenditure incurred in connection with the loss of productivity due to increased morbidity and mortality levels” (Mechelen 1997). An integral part to preventing such costs and effects is implementing proper sports injury prevention techniques.\n\n\nSummary: Machine learning methods in sport injury prediction and prevention: a systematic review by Hans Van Eetvelde, Luciana D. Mendonça, Christophe Ley, Romain Seil, and Thomas Tischer\nSports injuries have a complex cause with the “interactions of multiple risk factors and inciting events making a comprehensive model necessary” and difficult due to the large number of factors that play a role in an injury (Van Eetvelde et al. 2021). The first half of Hans Van Eetvelde et. al article covers the way that they selected their sources. Their paper addresses the “currently used definition of ML as well as predominantly used MIL methods”, the “accuracy of the currently used ML methods to predict injury”, and evaluates “the used methods for sport injury prevention purposes” (Van Eetvelde et al. 2021). The inclusion criteria for this article required articles to be original in “investigating the role of machine learning for sport injury prediction” and prevention, in english, and published in a peer-reviewed journal (ibid). The exclusion criteria included articles “not being sport specific, not covering injury prevention or injury prediction, [and] meeting abstracts and proceedings” (ibid). These criteria and their narrowing process left the researchers with 11 studies. The study found that “the most promising results to predict injury risk were obtained in elite youth football players…and in professional soccer based on a pre-season screening evaluation (Van Eetvelde et al. 2021).” It also concluded that “ML methods may be used to identify athletes at high injury risk during sport participation and that it may be helpful to identify risk factors (Van Eetvelde et al. 2021).” However, “the methodological study quality was moderate to very low (ibid).” As the field is growing, the authors expect promising further developments with respect to artificial intelligence and machine learning methods.\n\n\n\n\n\nReferences\n\nMechelen, Willem van. 1997. “The Severity of Sports Injuries.” Sports Medicine 24: 176–80.\n\n\nVan Eetvelde, Hans, Luciana D Mendonça, Christophe Ley, Romain Seil, and Thomas Tischer. 2021. “Machine Learning Methods in Sport Injury Prediction and Prevention: A Systematic Review.” Journal of Experimental Orthopaedics 8: 1–15."
  },
  {
    "objectID": "tabs/data_gathering/data_gathering.html",
    "href": "tabs/data_gathering/data_gathering.html",
    "title": "General Injury Data",
    "section": "",
    "text": "Injury & Injury Prevention Data Soccer\nThis data set focuses on the factors that may have influenced injuries that occured in soccer. The data collects information about the injury that occured, any history of injuries, and factors that may have affected the injury beforehand. This data will be useful to see what factors can influence injuries, such as warming up, previous injuries, etc.\nThe raw data and dataset can be found here.\n\n\nInjury Data From Different Activites\nThis data breaks down the how many injuries per age group come from different types of activities. While this data isn’t directly related to professional sports, it helps provide a basic understanding of what sports or physical activities produce the most injuries.\nThe data source can be found here.\n\n\nNBA Data\n\nNBA Injury Data\nThe first data set that I selected to use is a NBA Injury Data set. This data set records all of the roster moves related to injuries made by each NBA team in every season betwwen the 2010 and 2020 seasons. The raw data contains 5 features, including player name, team name, date of transaction, acquired players, relinquished players, and notes about the injury. I thought this data source would be useful to help gather information about what kinds of injuries are most common and how many injuries each team had per season.\nThe link to the raw data can be found here and the link to the data source can be found here.\n\n\nNBA Data\nThese three data sets are derived from Kaggle and are all related to the NBA. One is a players data set which contains information about players in the NBA including player name, team ID, and the season. Next is the rankings data set which includes information about each team’s record in each season and well as information about their conference. Last is the teams data which includes information about each team and where they play. These datasets combined would provide me with information about the NBA records for each season.\nThe link to raw data and data source can be found here\n\n\n\nNFL Data\n\nNFL Concussion Data\nThis data sources is a list of all the concussion or head injuries in the NFL during the 2012-2014. The data set has information about the time of the injury within the season, the severity of the injury, and the aftermath of the injury. This information is helpful to provide injury context for the NFL seasons.\nThe link to the raw data and data source can be found here\n\n\nNFL Game Injury Data\nThis data set provides anonymous information about NFL injuries and the situation in which the injury occured, such as the stadium type, the field type, and the temperature. This data will be useful for seeing how the weather and game circumstances can influence injuries.\nThe link to the raw data and data source can be found here."
  },
  {
    "objectID": "tabs/eda/eda.html",
    "href": "tabs/eda/eda.html",
    "title": "Data Exploration",
    "section": "",
    "text": "In order to explore my data, I plan to use summary statistics, a variety of graph types including a word cloud, bar graphs, line graphs, pie charts, etc. More specifically, I will use Pandas, Matplotlib, and Seaborn. I hope that through this exploration I can learn more about my data and the relationships that exist within it."
  },
  {
    "objectID": "tabs/eda/eda.html#text-data",
    "href": "tabs/eda/eda.html#text-data",
    "title": "Data Exploration",
    "section": "Text Data",
    "text": "Text Data\n\nNews API\n\nimport pandas as pd \n\nfile_path = \"../../../../data/01-modified-data/cleaned.csv\"\ndf = pd.read_csv(file_path)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nsource\nauthor\npublish_date\ncombined\n\n\n\n\n0\ncnet\nadam oram\n2023-09-30T12:02:09Z\nman united vs crystal palace livestream: how t...\n\n\n1\ncnet\nadam oram\n2023-09-23T12:00:09Z\nman city vs nottingham forest livestream: how ...\n\n\n2\nhuffpost\nap\n2023-09-19T11:28:10Z\nspanish soccer star accuses federation of thre...\n\n\n3\nslate magazine\njosh levin and stefan fatsis\n2023-09-18T21:43:37Z\nis there any part of aaron rodgers first game ...\n\n\n4\ndeadspin\nsam fels\n2023-10-03T11:27:00Z\nmls is playing pretty fast and loose with lion...\n\n\n\n\n\n\n\n\nStop Word Removal\n\nall_text = \"\"\nfor row in df[\"combined\"]:\n    all_text = all_text + \" \" + row \n\nall_text = \" \".join(list(df[\"combined\"].values))\n\n\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\n\nwords = all_text.split()\n\nstop_words = set(stopwords.words('english'))\nfiltered_words = [word for word in words if word.lower() not in stop_words]\n\nfiltered_text = ' '.join(filtered_words)\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/reneedemaio/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n\n\n\n\nWord Cloud\n\ndef generate_word_cloud(my_text): \n    from wordcloud import WordCloud, STOPWORDS\n    import matplotlib.pyplot as plt\n\n    def plot_cloud(wordcloud):\n        plt.figure(figsize=(40, 30))\n        plt.imshow(wordcloud)\n        plt.axis(\"off\");\n\n    wordcloud = WordCloud(\n        width = 3000, \n        height = 2000,\n        random_state=1, \n        background_color='lavender',\n        colormap= \"rainbow\",\n        collocations=False,\n        stopwords = STOPWORDS).generate(my_text)\n    plot_cloud(wordcloud)\n    plt.show()\n\n\ngenerate_word_cloud(filtered_text)\n\n\n\n\n\n\nBar Chart\n\nfrom collections import Counter\n\nwords = filtered_text.split()\nword_freq = Counter(words)\n\n\nimport matplotlib.pyplot as plt\n\nfrequency_threshold = 10\n\ncommon_words = [(word, freq) for word, freq in word_freq.items() if freq &gt; frequency_threshold]\ncommon_words.sort(key=lambda x: x[1], reverse=True)\n\nwords, frequencies = zip(*common_words)\n\nplt.figure(figsize=(10, 6))\nplt.bar(words, frequencies)\nplt.xlabel(\"Words\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Word Frequency Bar Chart\")\nplt.xticks(rotation=45)\n\nplt.tight_layout()\nplt.show\n\n&lt;function matplotlib.pyplot.show(close=None, block=None)&gt;\n\n\n\n\n\n\nmost_common_words = word_freq.most_common(10)  \n\nwords, frequencies = zip(*most_common_words)\n\nplt.figure(figsize=(10, 6))\nplt.bar(words, frequencies)\nplt.xlabel('Words')\nplt.ylabel('Frequency')\nplt.title('Word Frequency Bar Chart')\nplt.xticks(rotation=45)  \n\nplt.tight_layout()\nplt.show()\n\nThe data exploration for this data was successful and revealed that the majority of the topics discussed in the news relating to soccer are about specific players or competitions, and less about injuries. While some information of injuries are present, based on the lack of words that could relate to discussion of injury, it appears that these mentions are surface level."
  },
  {
    "objectID": "tabs/eda/eda.html#tabular-data",
    "href": "tabs/eda/eda.html#tabular-data",
    "title": "Data Exploration",
    "section": "Tabular Data",
    "text": "Tabular Data\n\nBasketball Injury Data\n\nimport pandas as pd \nimport numpy as np\nimport gdown\n\n# the csv url is: https://drive.google.com/file/d/1tdKeSFi492daHWh8Laqb7e3_68o3kaqD/view?usp=share_link\nfile_id = \"1tdKeSFi492daHWh8Laqb7e3_68o3kaqD\"\nurl = f\"https://drive.google.com/uc?id={file_id}\"\noutput = \"basketball_injuries.csv\"\ngdown.download(url, output, quiet=False)\n\nDownloading...\nFrom: https://drive.google.com/uc?id=1tdKeSFi492daHWh8Laqb7e3_68o3kaqD\nTo: /Users/reneedemaio/Desktop/git_repo/dsan-5000-project-rennyd123/dsan-website/5000-website/tabs/eda/basketball_injuries.csv\n100%|██████████| 1.57M/1.57M [00:00&lt;00:00, 5.38MB/s]\n\n\n'basketball_injuries.csv'\n\n\n\nfile_path = \"../../../../data/01-modified-data/basketball_injury_data.csv\"\n\nbball_injury_data = pd.read_csv(file_path)\nprint(bball_injury_data.head())\n\n         Date     Team   Relinquished  \\\n0  2010-10-03    Bulls  Carlos Boozer   \n1  2010-10-06  Pistons  Jonas Jerebko   \n2  2010-10-06  Pistons  Terrico White   \n3  2010-10-08  Blazers     Jeff Ayres   \n4  2010-10-08     Nets    Troy Murphy   \n\n                                               Notes      InjuryStatus  \n0  fractured bone in right pinky finger (out inde...  out indefinitely  \n1      torn right Achilles tendon (out indefinitely)  out indefinitely  \n2  broken fifth metatarsal in right foot (out ind...  out indefinitely  \n3          torn ACL in right knee (out indefinitely)  out indefinitely  \n4             strained lower back (out indefinitely)  out indefinitely  \n\n\n\nInjury Status Bar Chart\n\nimport matplotlib.pyplot as plt\n\ninjury_counts = bball_injury_data[\"InjuryStatus\"].value_counts()\n\ntop_5_injury_counts = injury_counts.head(5)\n\nplt.figure(figsize=(8, 6))\nplt.bar(top_5_injury_counts.index, top_5_injury_counts.values, color='pink')\nplt.xlabel('Injury Status')\nplt.ylabel('Count')\nplt.title('Top 5 Injury Status Bar Chart')\n\nplt.show()\n\n\n\n\n\n\nInjuries Per Team\n\nteam_counts = bball_injury_data[\"Team\"].value_counts()\nteam_names = bball_injury_data[\"Team\"].unique()\n\nplt.figure(figsize=(8, 8))\nplt.pie(team_counts, labels=team_names, autopct='%1.1f%%', startangle=140, shadow=True)\nplt.axis(\"equal\")\nplt.title(\"Injury Count per Team\")\nplt.show()\n\n\n\n\n\n\nInjuries by Dates\n\nbball_injury_data['Date'] = pd.to_datetime(bball_injury_data['Date'])\ndate_counts_2010 = bball_injury_data[bball_injury_data['Date'].dt.year == 2010]\ndate_counts_2010 = date_counts_2010['Date'].value_counts().sort_index()\n\nplt.figure(figsize=(12, 6))\nplt.plot(date_counts_2010.index, date_counts_2010.values, marker='o', linestyle='-', color=\"green\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Injury Count\")\nplt.title(\"Injury Counts by Day in 2010\")\nplt.xticks(rotation=45)\n\nplt.grid(True)\nplt.show()\n\n\n\n\nThe first graph created for this data set reveals that the most common reason given for an athlete’s absence from an NBA game is simply that they did not play. This can be for any variety of reasons from personal reasons, issues with the coach, or injury as well. The next most common is did not dress, which could imply that the athlete is injuries, but doesn’t specifically state that. The next graph shows that most teams in the NBA had relatively similar rates of injury with most of the hovering around 3%. Finally, the last graph shows one interesting spike in injuries in late October of 2010. There is nothing notable in NBA history that day that would cause so many more athletes to be out, so this can be considered an outlier. Outside of that, the average number of injuries across all teams per day hovers between 0-15.\n\n\n\nNFL Concussion Data\n\nfile_path = \"../../../../data/01-modified-data/nfl_concussions.csv\"\n\nconcussion_data = pd.read_csv(file_path)\nprint(concussion_data.head())\n\n             Player                  Team        Date         Opposing Team  \\\n0  Aldrick Robinson   Washington Redskins  30/09/2012  Tampa Bay Buccaneers   \n1       D.J. Fluker    San Diego Chargers  22/09/2013      Tennessee Titans   \n2  Marquise Goodwin         Buffalo Bills  28/09/2014        Houston Texans   \n3       Bryan Stork  New England Patriots  12/10/2014         Buffalo Bills   \n4    Lorenzo Booker         Chicago Bears   9/09/2012    Indianapolis Colts   \n\n           Position Pre-Season Injury? Winning Team?  Week of Injury  \\\n0     Wide Receiver                 No           Yes               4   \n1  Offensive Tackle                 No            No               3   \n2     Wide Receiver                 No            No               4   \n3            Center                 No           Yes               6   \n4      Running Back                Yes           Yes               1   \n\n      Season  Weeks Injured  Games Missed Unknown Injury?  \\\n0  2012/2013              1           1.0              No   \n1  2013/2014              1           1.0              No   \n2  2014/2015              1           1.0              No   \n3  2014/2015              1           1.0              No   \n4  2012/2013              0           NaN              No   \n\n  Reported Injury Type  Total Snaps      Play Time After Injury  \\\n0                 Head            0                    14 downs   \n1           Concussion            0                    78 downs   \n2           Concussion            0                    25 downs   \n3                 Head            0                    82 downs   \n4                 Head            0  Did not return from injury   \n\n  Average Playtime Before Injury  \n0                    37.00 downs  \n1                    73.50 downs  \n2                    17.50 downs  \n3                    41.50 downs  \n4                            NaN  \n\n\n\nInjury Count by Team\n\ninjuries_by_team = concussion_data['Team'].value_counts()\n\n \nplt.figure(figsize=(15, 6))\nplt.bar(injuries_by_team.index, injuries_by_team.values, color=\"skyblue\")\nplt.xlabel(\"Team Name\")\nplt.ylabel(\"Injury Count\")\nplt.title(\"Injury Count by Team\")\nplt.xticks(rotation=60)\n\nplt.show()\n\n\n\n\n\n\nType of Injury\n\ninjury_type = concussion_data[\"Reported Injury Type\"].value_counts()\ninjury_name = injury_type.index\n\npastel_colors = ['#FF9999', '#66B2FF', '#99FF99', '#FFCC99', '#c2c2f0', '#ffb3e6', '#c2f0c2']\n\nplt.figure(figsize=(8, 8))\nplt.pie(injury_type, labels=injury_name, autopct='%1.1f%%', startangle=140, shadow=False, colors=pastel_colors)\nplt.axis(\"equal\")\nplt.title(\"Injury Count by Type of Injury\")\nplt.legend(injury_name, title=\"Injury Types\", loc=\"best\")\nplt.show()\n\n\n\n\n\n\nInjuries by Position\n\ninjuries_by_pos = concussion_data['Position'].value_counts()\n\n \nplt.figure(figsize=(15, 6))\nplt.bar(injuries_by_pos.index, injuries_by_pos.values, color=\"orange\")\nplt.xlabel(\"Position\")\nplt.ylabel(\"Injury Count\")\nplt.title(\"Injury Count by Position\")\nplt.xticks(rotation=60)\n\nplt.show()\n\n\n\n\n\n\nCorrelation Between Week of Injury and Total Snaps\n\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns \nimport matplotlib.pyplot as plt \n\n\nsubset = [\"Week of Injury\", \"Total Snaps\"]\nconcussion_data_subset = concussion_data[subset]\n\ncorr_matrix = concussion_data_subset.corr()\n\nsns.heatmap(corr_matrix, \n            annot=True, \n            cmap=\"coolwarm\")\nplt.title(\"Correlation Map between Week of Injury and Total Snaps\")\nplt.show()\n\n\n\n\nThe graphs made from the NFL concussion data reveal that the Cincinatti Bengals and the Cleveland Browns had the most concussions on their team. The majority of head injuries are concussions (81.7%), but there are also a fair share of non-concussion head injuries (18.0%). Next, the position that sees the most amount of concussion injuries is cornerback, closely followed by wide receiver and safety. Finally, there was a 12% positive correlation between the week of injury and the total number of snaps a player had.\n\n\n\nNFL Game Injury Data\n\nfile_path = \"../../../../data/01-modified-data/nfl_injuries.csv\"\n\nnfl_injuries = pd.read_csv(file_path)\nprint(nfl_injuries.head())\n\n   PlayerKey BodyPart RosterPosition       StadiumType  FieldType  \\\n0      39873     Knee     Linebacker            indoor  Synthetic   \n1      39873     Knee     Linebacker           outdoor    Natural   \n2      39873     Knee     Linebacker            indoor  Synthetic   \n3      39873     Knee     Linebacker  retractable roof  Synthetic   \n4      39873     Knee     Linebacker           outdoor    Natural   \n\n   Temperature        Weather  \n0           85  Mostly Cloudy  \n1           82          Sunny  \n2           84         Cloudy  \n3           78  Partly Cloudy  \n4           80         Cloudy  \n\n\n\nInjury Count by Stadium Type\n\nstadium_type = nfl_injuries[\"StadiumType\"].value_counts()\ncolors = plt.cm.viridis(np.linspace(0, 1, len(stadium_type)))\n\n\nplt.figure(figsize=(12,6))\nplt.bar(stadium_type.index, stadium_type.values, color=colors)\nplt.xlabel(\"Stadium Type\")\nplt.ylabel(\"Injury Count\")\nplt.title(\"Injury Count by Stadium Types\")\nplt.xticks(rotation = 60)\nplt.show()\n\n\n\n\n\n\nInjury Count by Body Part Injured\n\nimport seaborn as sns\n\ninjury_frequency = nfl_injuries[\"BodyPart\"].value_counts()\ninjury_place = nfl_injuries[\"BodyPart\"].unique()\nprint(injury_frequency)\n\npastel_colors = ['#FF9999', '#66B2FF', '#99FF99', '#FFCC99', '#c2c2f0', '#ffb3e6', '#c2f0c2']\n\nplt.figure(figsize=(8, 8))\nplt.pie(injury_frequency, labels=injury_place, autopct='%1.1f%%', startangle=140, shadow=False, colors=pastel_colors)\nplt.axis(\"equal\")\nplt.title(\"Injury Count by Type of Injury\")\nplt.show()\n\nBodyPart\nKnee     825\nAnkle    720\nToes     144\nFoot      96\nHeel      18\nName: count, dtype: int64\n\n\n\n\n\n\n\nInjury Count vs Temperature\n\nnfl_injuries[\"Temperature\"] = pd.to_numeric(nfl_injuries[\"Temperature\"], errors='coerce')\nnfl_injuries = nfl_injuries[nfl_injuries[\"Temperature\"] &gt;= 0]\nnfl_injuries = nfl_injuries.dropna(subset=[\"Temperature\"])\n\ntemperature_counts = nfl_injuries[\"Temperature\"].value_counts()\n\nplt.figure(figsize=(12, 6))\nplt.plot(temperature_counts.values, temperature_counts.index, marker='o', linestyle='-', color=\"green\")\nplt.xlabel(\"Temperature\")\nplt.ylabel(\"Injury Count\")\nplt.title(\"Injury Counts by Temperature\")\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\nThis dataset reveals that the majority of NFL injuries occur in stadiums that are outdoors. Most of the injuries are knee or ankle injuires which makese sense due to the nature of American Football. Finally, there doesn’t seem to be a huge correlation between temperature and number of injuries. Spikes in injuries occur during many points in the year.\n\n\n\nInjury Prevention Data Soccer\n\nfile_path = \"../../../../data/01-modified-data/injury_prevention_data_soccer.csv\"\n\ninjury_prevention = pd.read_csv(file_path)\nprint(injury_prevention.head())\n\n    ID  Age  Height  Mass  Team  Position  Years of Football Experience  \\\n0  146   19   173.0  67.6     1         3                             1   \n1  155   22   179.5  71.0     1         3                             1   \n2  160   22   175.5  71.8     1         3                             1   \n3  164   23   190.0  80.5     1         4                             1   \n4  145   19   173.5  68.7     1         3                             1   \n\n  Previous Injuries   Number of Injuries Ankle Injuries   ...  \\\n0                yes                   6             yes  ...   \n1                yes                   2              no  ...   \n2                yes                   7             yes  ...   \n3                yes                   1              no  ...   \n4                yes                   2             yes  ...   \n\n   Importance Injury Prevention Knowledgeability   \\\n0                             2                 1   \n1                             1                 1   \n2                             1                 1   \n3                             1                 1   \n4                             1                 2   \n\n  Prevention Measure Stretching  Prevention Measure Warm Up   \\\n0                            yes                          no   \n1                            yes                         yes   \n2                            yes                          no   \n3                            yes                         yes   \n4                            yes                         yes   \n\n   Prevention Measure Specific Strength Exercises   \\\n0                                              yes   \n1                                               no   \n2                                               no   \n3                                              yes   \n4                                               no   \n\n  Prevention Measure Bracing  Prevention Measure Taping   \\\n0                          no                         no   \n1                          no                         no   \n2                          no                         no   \n3                          no                         no   \n4                          no                        yes   \n\n  Prevention Measure Shoe Insoles   Prevention Measure Face Masks   \\\n0                               no                              no   \n1                               no                              no   \n2                              yes                              no   \n3                               no                              no   \n4                               no                              no   \n\n  Prevention Measure Medical Corset   \n0                                 no  \n1                                 no  \n2                                 no  \n3                                 no  \n4                                 no  \n\n[5 rows x 41 columns]\n\n\n\nSummary Statistics on Personal Information\n\ninjury_prevention.columns\nprint(injury_prevention.dtypes)\n\nID                                                   int64\nAge                                                  int64\nHeight                                             float64\nMass                                               float64\nTeam                                                 int64\nPosition                                             int64\nYears of Football Experience                         int64\nPrevious Injuries                                   object\nNumber of Injuries                                   int64\nAnkle Injuries                                      object\nNumber of Ankle Injuries                             int64\nSevere_Ankle_Injuries                               object\nNoncontact_Ankle_Injuries                           object\nKnee Injuries                                       object\nNumber of Knee Injuries                              int64\nSevere_Knee_Injuries                                object\nNoncontact_Knee_Injuries                            object\nThigh_Injuries                                      object\nNumber of Thigh Injuries                             int64\nSevere_Thigh_Injuries                               object\nNoncontact_Thigh_Injuries                           object\nRisk Factor Condition                               object\nRisk Factor Coordination                            object\nRisk Factor Muscle Impairments                      object\nRisk Factor Fatigue                                 object\nRisk Factor Previous Injury                         object\nRisk Factor Attentiveness                           object\nRisk Factor Other Player                            object\nRisk Factor Equipment                               object\nRisk Factor Climatic Condition                      object\nRisk Factor Diet                                    object\nImportance Injury Prevention                         int64\nKnowledgeability                                     int64\nPrevention Measure Stretching                       object\nPrevention Measure Warm Up                          object\nPrevention Measure Specific Strength Exercises      object\nPrevention Measure Bracing                          object\nPrevention Measure Taping                           object\nPrevention Measure Shoe Insoles                     object\nPrevention Measure Face Masks                       object\nPrevention Measure Medical Corset                   object\ndtype: object\n\n\n\ninjury_prevention[\"Height\"] = injury_prevention[\"Height\"].astype(int)\ninjury_prevention[\"Mass\"] = injury_prevention[\"Mass\"].astype(int)\n\n\nprint(injury_prevention.dtypes)\n\nID                                                  int64\nAge                                                 int64\nHeight                                              int64\nMass                                                int64\nTeam                                                int64\nPosition                                            int64\nYears of Football Experience                        int64\nPrevious Injuries                                  object\nNumber of Injuries                                  int64\nAnkle Injuries                                     object\nNumber of Ankle Injuries                            int64\nSevere_Ankle_Injuries                              object\nNoncontact_Ankle_Injuries                          object\nKnee Injuries                                      object\nNumber of Knee Injuries                             int64\nSevere_Knee_Injuries                               object\nNoncontact_Knee_Injuries                           object\nThigh_Injuries                                     object\nNumber of Thigh Injuries                            int64\nSevere_Thigh_Injuries                              object\nNoncontact_Thigh_Injuries                          object\nRisk Factor Condition                              object\nRisk Factor Coordination                           object\nRisk Factor Muscle Impairments                     object\nRisk Factor Fatigue                                object\nRisk Factor Previous Injury                        object\nRisk Factor Attentiveness                          object\nRisk Factor Other Player                           object\nRisk Factor Equipment                              object\nRisk Factor Climatic Condition                     object\nRisk Factor Diet                                   object\nImportance Injury Prevention                        int64\nKnowledgeability                                    int64\nPrevention Measure Stretching                      object\nPrevention Measure Warm Up                         object\nPrevention Measure Specific Strength Exercises     object\nPrevention Measure Bracing                         object\nPrevention Measure Taping                          object\nPrevention Measure Shoe Insoles                    object\nPrevention Measure Face Masks                      object\nPrevention Measure Medical Corset                  object\ndtype: object\n\n\n\nnumerical_columns = injury_prevention.select_dtypes(include='int64')\nnumerical_columns = injury_prevention.drop(columns=\"ID\")\nnumerical_columns.describe()\n\n\n\n\n\n\n\n\nAge\nHeight\nMass\nTeam\nPosition\nYears of Football Experience\nNumber of Injuries\nNumber of Ankle Injuries\nNumber of Knee Injuries\nNumber of Thigh Injuries\nImportance Injury Prevention\nKnowledgeability\n\n\n\n\ncount\n139.000000\n139.000000\n139.000000\n139.000000\n139.000000\n139.000000\n139.000000\n139.000000\n139.000000\n139.000000\n139.000000\n139.000000\n\n\nmean\n17.597122\n177.043165\n68.417266\n3.690647\n2.546763\n1.733813\n1.906475\n0.683453\n0.517986\n0.726619\n1.345324\n0.827338\n\n\nstd\n4.601070\n9.416198\n11.781156\n1.825142\n0.853153\n0.913470\n1.614661\n0.932784\n0.684746\n0.778383\n0.586221\n0.415777\n\n\nmin\n13.000000\n141.000000\n31.000000\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n14.000000\n172.500000\n61.000000\n2.000000\n2.000000\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n1.000000\n1.000000\n\n\n50%\n16.000000\n178.000000\n70.000000\n4.000000\n3.000000\n2.000000\n2.000000\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n\n\n75%\n19.000000\n184.000000\n76.000000\n5.500000\n3.000000\n2.000000\n2.000000\n1.000000\n1.000000\n1.000000\n2.000000\n1.000000\n\n\nmax\n35.000000\n196.000000\n101.000000\n6.000000\n4.000000\n4.000000\n10.000000\n4.000000\n3.000000\n3.000000\n4.000000\n2.000000\n\n\n\n\n\n\n\n\nobject_columns = injury_prevention.select_dtypes(include='object')\nfor column in object_columns:\n    plt.figure(figsize=(4, 4))\n    sns.countplot(data=injury_prevention, x=column, order=['yes', 'no'], palette=['pink', 'lightblue'])\n    plt.title(f'{column}')\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis set of graphs show counts of how many athletes used each prevention method, how many athletes had each time of injury, and what risk factors are present.\n\n\n\nSports Injury Data\n\nfile_path = \"../../../../data/01-modified-data/sports_injury_data.csv\"\n\nsports_injuries = pd.read_csv(file_path)\nprint(sports_injuries.head())\n\n     Sport, activity or equipment Injuries (1) Younger than 5  5 to 14  \\\n0    Exercise, exercise equipment      445,642          6,662   36,769   \n1        Bicycles and accessories      405,411         13,297   91,089   \n2                      Basketball      313,924          1,216  109,696   \n3                        Football      265,747            581  145,499   \n4  ATV's, mopeds, minibikes, etc.      242,347          3,688   42,069   \n\n  15 to 24 25 to 64 65 and older  \n0   91,013  229,640       81,558  \n1   50,863  195,030       55,132  \n2  143,773   57,413        1,825  \n3  100,760   18,527          381  \n4   61,065  122,941       12,584  \n\n\n\nsports_injuries_subset = sports_injuries[['Injuries (1)', 'Younger than 5',\n       '5 to 14', '15 to 24', '25 to 64', '65 and older']]\nactivity_type = sports_injuries[\"Sport, activity or equipment\"]\n\n\nsports_injuries.dtypes\n\nSport, activity or equipment    object\nInjuries (1)                    object\nYounger than 5                  object\n5 to 14                         object\n15 to 24                        object\n25 to 64                        object\n65 and older                    object\ndtype: object\n\n\n\nsports_injuries[\"Injuries (1)\"] = sports_injuries[\"Injuries (1)\"].str.replace(',', '').astype(float)\nsports_injuries[\"Younger than 5\"] = sports_injuries[\"Younger than 5\"].str.replace(',', '').astype(float)\nsports_injuries[\"5 to 14\"] = sports_injuries[\"5 to 14\"].str.replace(',', '').astype(float)\nsports_injuries[\"15 to 24\"] = sports_injuries[\"15 to 24\"].str.replace(',', '').astype(float)\nsports_injuries[\"25 to 64\"] = sports_injuries[\"25 to 64\"].str.replace(',', '').astype(float)\nsports_injuries[\"65 and older\"] = sports_injuries[\"65 and older\"].str.replace(',', '').astype(float)\n\n\nprint(sports_injuries.head())\nprint(sports_injuries.dtypes)\n\n     Sport, activity or equipment  Injuries (1)  Younger than 5   5 to 14  \\\n0    Exercise, exercise equipment      445642.0          6662.0   36769.0   \n1        Bicycles and accessories      405411.0         13297.0   91089.0   \n2                      Basketball      313924.0          1216.0  109696.0   \n3                        Football      265747.0           581.0  145499.0   \n4  ATV's, mopeds, minibikes, etc.      242347.0          3688.0   42069.0   \n\n   15 to 24  25 to 64  65 and older  \n0   91013.0  229640.0       81558.0  \n1   50863.0  195030.0       55132.0  \n2  143773.0   57413.0        1825.0  \n3  100760.0   18527.0         381.0  \n4   61065.0  122941.0       12584.0  \nSport, activity or equipment     object\nInjuries (1)                    float64\nYounger than 5                  float64\n5 to 14                         float64\n15 to 24                        float64\n25 to 64                        float64\n65 and older                    float64\ndtype: object\n\n\n\nsubsetted_sports_injuries = sports_injuries.head(5)\nmelted_df = pd.melt(subsetted_sports_injuries, id_vars=['Sport, activity or equipment'], var_name='Age group', value_name='Injuries')\nplt.figure(figsize=(12, 6))\nsns.barplot(data=melted_df, x='Sport, activity or equipment', y='Injuries', hue='Age group')\nplt.title('Injuries by Age Group for Different Activities')\nplt.xlabel('Sport, activity or equipment')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\nsubsetted_sports_injuries = sports_injuries.tail(5)\nmelted_df = pd.melt(subsetted_sports_injuries, id_vars=['Sport, activity or equipment'], var_name='Age group', value_name='Injuries')\nplt.figure(figsize=(12, 6))\nsns.barplot(data=melted_df, x='Sport, activity or equipment', y='Injuries', hue='Age group')\nplt.title('Injuries by Age Group for Different Activities')\nplt.xlabel('Sport, activity or equipment')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\nThis set of graphs shows a count of how many injuries each sport or type of active activity causes by age group.\n\n\nNBA Player and Team Data\n\nfile_path = \"../../../../data/01-modified-data/nba_data.csv\"\n\nnba = pd.read_csv(file_path)\nprint(nba.head())\n\n   Unnamed: 0       PLAYER_NAME     TEAM_ID  PLAYER_ID  SEASON  LEAGUE_ID  \\\n0           1     Royce O'Neale  1610612762    1626220    2019          0   \n1           2  Bojan Bogdanovic  1610612762     202711    2019          0   \n2           3       Rudy Gobert  1610612762     203497    2019          0   \n3           4  Donovan Mitchell  1610612762    1628378    2019          0   \n4           5       Mike Conley  1610612762     201144    2019          0   \n\n   MIN_YEAR  MAX_YEAR ABBREVIATION NICKNAME  ...    HEADCOACH  \\\n0      1974      2019          UTA     Jazz  ...  Quin Snyder   \n1      1974      2019          UTA     Jazz  ...  Quin Snyder   \n2      1974      2019          UTA     Jazz  ...  Quin Snyder   \n3      1974      2019          UTA     Jazz  ...  Quin Snyder   \n4      1974      2019          UTA     Jazz  ...  Quin Snyder   \n\n     DLEAGUEAFFILIATION SEASON_ID  STANDINGSDATE CONFERENCE  TEAM   G   W   L  \\\n0  Salt Lake City Stars     22022     2022-12-22       West  Utah  35  19  16   \n1  Salt Lake City Stars     22022     2022-12-22       West  Utah  35  19  16   \n2  Salt Lake City Stars     22022     2022-12-22       West  Utah  35  19  16   \n3  Salt Lake City Stars     22022     2022-12-22       West  Utah  35  19  16   \n4  Salt Lake City Stars     22022     2022-12-22       West  Utah  35  19  16   \n\n   W_PCT  \n0  0.543  \n1  0.543  \n2  0.543  \n3  0.543  \n4  0.543  \n\n[5 rows x 26 columns]\n\n\n\nnba.columns\n\nIndex(['Unnamed: 0', 'PLAYER_NAME', 'TEAM_ID', 'PLAYER_ID', 'SEASON',\n       'LEAGUE_ID', 'MIN_YEAR', 'MAX_YEAR', 'ABBREVIATION', 'NICKNAME',\n       'YEARFOUNDED', 'CITY', 'ARENA', 'ARENACAPACITY', 'OWNER',\n       'GENERALMANAGER', 'HEADCOACH', 'DLEAGUEAFFILIATION', 'SEASON_ID',\n       'STANDINGSDATE', 'CONFERENCE', 'TEAM', 'G', 'W', 'L', 'W_PCT'],\n      dtype='object')\n\n\n\nWin Percentage by Team\n\nteam = nba[\"TEAM\"]\nwin_pct = nba[\"W_PCT\"]\n\nplt.figure(figsize=(12, 6))\nplt.bar(team, win_pct)\nplt.title('2019 Win Percentage by Team in the NBA')\nplt.xlabel('Team')\nplt.ylabel('Win Percentage')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\nWins vs Arena Capacity\n\nheatmap_data = nba.pivot_table(index='ARENACAPACITY', values='W_PCT', aggfunc='mean')\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(heatmap_data, cmap='YlGnBu', annot=True, fmt='.2f', cbar=True)\nplt.title('Heatmap: ARENACAPACITY vs. W_PCT')\nplt.xlabel('W_PCT (Win Percentage)')\nplt.ylabel('ARENACAPACITY (Arena Capacity)')\nplt.show()\n\n\n\n\nFinally, these graphs show two different representations of win percentage in the 2019 NBA season. The first graph shows win percentage by team, and the second shows a correlation between arena capacity and win percentage. It appears that win percentage and arena capacity are not strongly positively correlated."
  },
  {
    "objectID": "tabs/eda/eda.html#hypothesis-refinement",
    "href": "tabs/eda/eda.html#hypothesis-refinement",
    "title": "Data Exploration",
    "section": "Hypothesis Refinement",
    "text": "Hypothesis Refinement\nBased off of the exploratory data analysis process, it is necessary that I refine my original hypotheses and identify next steps. Firstly, I think that it is important to continue to further explore that factors can affect an athletes susceptibility to injury to understand confounding variables in sport. That will be involved in my next steps with this project. Secondly, I would refine my hypothesis to include more sport specific hypotheses to create more specific hypotheses for soccer, American football, and basketball. In order to do so, I will need to collect more data."
  },
  {
    "objectID": "tabs/decision_tree/regression.html",
    "href": "tabs/decision_tree/regression.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "1+1"
  },
  {
    "objectID": "tabs/clustering/clustering.html",
    "href": "tabs/clustering/clustering.html",
    "title": "Introduction",
    "section": "",
    "text": "In this section, I will give a brief overview of three clustering methods: KMeans, DBSCAN, and Hierarchical Clustering.\n\n\nThe K-Means clustering method is an unsupervised machine lerning algorithm. The goal of K-Means to group points that are similar together into cluseters in an attempt to reveal any underlying patterns. K-Means works only with numerical data, and each data point must be able to be described using numerical coordinates (Burkardt 2009).\nIn order to return clusters of your selected input data, K-Means will create k numbers of clusters, based on a value of k that you select. One way that we can choose an optimal values of k is by using the elbow method, which plots the sum of the squared distances between each cluster as the value of k increases. When we look at the plot, there is a point where “increasing the size of the cluster provides minimal gain to the error function (Artley 2022).” This can be seen in the image below.\n\n\n\nArtley (2022)\n\n\nAfter we choose our value for k, K-Means will assign each data point to a cluster using the euclidian distance to each centroid. After all the data points are assinged, the centroids of each cluster will be updated by taking the mean of the data points and assigning it to be the new center of the cluster. This reassigning points to clusters and recalcuating the centroids will occur until the centroids values don’t change anymore (Artley 2022).\n\n\n\nDBSCAN, or density-based spatial clustering of applications with noise, is an unsupervised clustering algorithm based on clusters and noise. Any density based clustering method is useful when we have a dataset that has irregular or intertwined clusters or when there is a lot of noise or outliers.\nDBSCAN will first divide the dataset into n number of dimensions. Then it will group points together that are tightly packed by forming an n dimensional shape around each point in the dataset. Clusters will be formed by the points that fall within that shape (Lutins 2017).\n\n\n\nHierarchical clustering is an unsupervised clustering method. It has a nested structure that is somewhat organized like a tree, and does not assume a value of k. Hierarchical clustering can be further subdivided into two types: agglomerative (bottom up) and divisive (top down) hierarchical clustering. We can visualize this technique using a dendrogram.\n\nIn agglomerative hierarchical clustering, “each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy [how to cite the slide].” The basic algorithm of agglomerative hierarchical clustering includes computing the proximity matrix, letting each data point be its own cluster, and then repreating to merge the two closest clusters and update the proximity matrix, until only one cluster remains (Patlolla 2018).\n\nDivisive hierarchical clustering is the opposite of agglomerative hierarchical clustering. In divisive hierarchical clustering, “all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy [cite the slides].”"
  },
  {
    "objectID": "tabs/clustering/clustering.html#theory",
    "href": "tabs/clustering/clustering.html#theory",
    "title": "Introduction",
    "section": "",
    "text": "In this section, I will give a brief overview of three clustering methods: KMeans, DBSCAN, and Hierarchical Clustering.\n\n\nThe K-Means clustering method is an unsupervised machine lerning algorithm. The goal of K-Means to group points that are similar together into cluseters in an attempt to reveal any underlying patterns. K-Means works only with numerical data, and each data point must be able to be described using numerical coordinates (Burkardt 2009).\nIn order to return clusters of your selected input data, K-Means will create k numbers of clusters, based on a value of k that you select. One way that we can choose an optimal values of k is by using the elbow method, which plots the sum of the squared distances between each cluster as the value of k increases. When we look at the plot, there is a point where “increasing the size of the cluster provides minimal gain to the error function (Artley 2022).” This can be seen in the image below.\n\n\n\nArtley (2022)\n\n\nAfter we choose our value for k, K-Means will assign each data point to a cluster using the euclidian distance to each centroid. After all the data points are assinged, the centroids of each cluster will be updated by taking the mean of the data points and assigning it to be the new center of the cluster. This reassigning points to clusters and recalcuating the centroids will occur until the centroids values don’t change anymore (Artley 2022).\n\n\n\nDBSCAN, or density-based spatial clustering of applications with noise, is an unsupervised clustering algorithm based on clusters and noise. Any density based clustering method is useful when we have a dataset that has irregular or intertwined clusters or when there is a lot of noise or outliers.\nDBSCAN will first divide the dataset into n number of dimensions. Then it will group points together that are tightly packed by forming an n dimensional shape around each point in the dataset. Clusters will be formed by the points that fall within that shape (Lutins 2017).\n\n\n\nHierarchical clustering is an unsupervised clustering method. It has a nested structure that is somewhat organized like a tree, and does not assume a value of k. Hierarchical clustering can be further subdivided into two types: agglomerative (bottom up) and divisive (top down) hierarchical clustering. We can visualize this technique using a dendrogram.\n\nIn agglomerative hierarchical clustering, “each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy [how to cite the slide].” The basic algorithm of agglomerative hierarchical clustering includes computing the proximity matrix, letting each data point be its own cluster, and then repreating to merge the two closest clusters and update the proximity matrix, until only one cluster remains (Patlolla 2018).\n\nDivisive hierarchical clustering is the opposite of agglomerative hierarchical clustering. In divisive hierarchical clustering, “all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy [cite the slides].”"
  },
  {
    "objectID": "tabs/clustering/clustering.html#methods",
    "href": "tabs/clustering/clustering.html#methods",
    "title": "Introduction",
    "section": "Methods",
    "text": "Methods\n\nData Selection\nI’ve chosen to use the same data set that I used for my Naive Bayes tab but I hope to use a different target. I will not be performing feature selection as I already performed it previously and this is the optimal subset. I’ve included the subsetting process in my code below.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd \nimport matplotlib as plt\n\nfile_path = \"../../../../data/01-modified-data/injury_prevention_data_soccer.csv\"\nsoccer_injury = pd.read_csv(file_path)\n\nsoccer_injury.columns = soccer_injury.columns.str.strip()\n\nsoccer_injury = soccer_injury.replace({\"yes\": 1, \"no\": 0})\nsoccer_injury.head()\n\n\n\n\n\n\n\n\n\nID\nAge\nHeight\nMass\nTeam\nPosition\nYears of Football Experience\nPrevious Injuries\nNumber of Injuries\nAnkle Injuries\n...\nImportance Injury Prevention\nKnowledgeability\nPrevention Measure Stretching\nPrevention Measure Warm Up\nPrevention Measure Specific Strength Exercises\nPrevention Measure Bracing\nPrevention Measure Taping\nPrevention Measure Shoe Insoles\nPrevention Measure Face Masks\nPrevention Measure Medical Corset\n\n\n\n\n0\n146\n19\n173.0\n67.6\n1\n3\n1\n1\n6\n1\n...\n2\n1\n1\n0\n1\n0\n0\n0\n0\n0\n\n\n1\n155\n22\n179.5\n71.0\n1\n3\n1\n1\n2\n0\n...\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n\n\n2\n160\n22\n175.5\n71.8\n1\n3\n1\n1\n7\n1\n...\n1\n1\n1\n0\n0\n0\n0\n1\n0\n0\n\n\n3\n164\n23\n190.0\n80.5\n1\n4\n1\n1\n1\n0\n...\n1\n1\n1\n1\n1\n0\n0\n0\n0\n0\n\n\n4\n145\n19\n173.5\n68.7\n1\n3\n1\n1\n2\n1\n...\n1\n2\n1\n1\n0\n0\n1\n0\n0\n0\n\n\n\n\n5 rows × 41 columns\n\n\n\n\n\nCode\nx_vars = ['Prevention Measure Stretching', 'Prevention Measure Warm Up',\n       'Prevention Measure Specific Strength Exercises',\n       'Prevention Measure Bracing', 'Prevention Measure Taping',\n       'Prevention Measure Shoe Insoles', 'Prevention Measure Face Masks',\n       'Prevention Measure Medical Corset']\n\nsi_subset = soccer_injury[x_vars]\nsi_subset.head()\n\n\n\n\n\n\n\n\n\nPrevention Measure Stretching\nPrevention Measure Warm Up\nPrevention Measure Specific Strength Exercises\nPrevention Measure Bracing\nPrevention Measure Taping\nPrevention Measure Shoe Insoles\nPrevention Measure Face Masks\nPrevention Measure Medical Corset\n\n\n\n\n0\n1\n0\n1\n0\n0\n0\n0\n0\n\n\n1\n1\n1\n0\n0\n0\n0\n0\n0\n\n\n2\n1\n0\n0\n0\n0\n1\n0\n0\n\n\n3\n1\n1\n1\n0\n0\n0\n0\n0\n\n\n4\n1\n1\n0\n0\n1\n0\n0\n0\n\n\n\n\n\n\n\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\n\nX = si_subset.copy()\nX_norm = StandardScaler().fit_transform(X)"
  },
  {
    "objectID": "tabs/clustering/clustering.html#hyper-parameter-tuning",
    "href": "tabs/clustering/clustering.html#hyper-parameter-tuning",
    "title": "Introduction",
    "section": "Hyper-Parameter Tuning",
    "text": "Hyper-Parameter Tuning\n\nFor each of the three clustering algorithms, perform any relevant parameter tuning in an attempt to achieve the optimal clustering results\ne.g. For k-means, Use Elbow and Silhouette methods to illustrate the ideal number of clusters. Visualize your results.\nAlso, when relevant, explore different choices of distance metric for the algorithm. Which distance metric seems to works best in which cases and why?\n\n\nK-Means: Elbow Method\n\n\nCode\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.cluster import MeanShift\nfrom sklearn.cluster import Birch\nfrom sklearn.cluster import DBSCAN\nimport numpy as np \nimport matplotlib.pyplot as plt\n\nnum_clusters = range(1, 15)\nkmeans_values = []\ninertia_values = []\ndistortion_values = []\n\n\nfor k in num_clusters:\n    kmeans = KMeans(n_clusters=k, n_init=10)\n    kmeans.fit(X_norm) \n    centroids = kmeans.cluster_centers_\n    labels = kmeans.labels_\n\n    distortion = 0\n    for i in range(len(X_norm)):\n        cluster_idx = labels[i]\n        distortion += np.linalg.norm(X_norm[i] - centroids[cluster_idx])**2\n    distortion /= len(X_norm) \n    distortion_values.append(distortion)\n\n    inertia = kmeans.inertia_\n    \n    kmeans_values.append(k)\n    inertia_values.append(inertia) \n\ndf = pd.DataFrame({\"Number of Clusters\": kmeans_values, \"Inertia\": inertia_values, \"Distortion\": distortion_values})\ndf.head()\n\nsecond_derivative = np.diff(np.diff(inertia_values))\nelbow_index = np.argmax(second_derivative) + 1\nprint(f\"The 'Elbow' of the graph is: {elbow_index}\")\n\nplt.figure(figsize=(15, 6))\nplt.scatter(data=df, x=\"Number of Clusters\", y=\"Inertia\")\nplt.title(\"Number of Clusters vs Inertia\")\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"Inertia\")\n\nplt.axvline(x=5, color='r', linestyle='--', label='Elbow Point')\n\n\nThe 'Elbow' of the graph is: 5\n\n\n&lt;matplotlib.lines.Line2D at 0x167aa0a50&gt;\n\n\n\n\n\n\n\nCode\nimport seaborn as sns\nplt.figure(figsize=(4, 1.5))\nsns.lineplot(df, x=\"Number of Clusters\", y=\"Inertia\", color=\"orange\")\nplt.title(\"Inertia\")\nplt.xlabel(\"Number of Clusters\")\n\nplt.figure(figsize=(4, 1.5))\nsns.lineplot(df, x=\"Number of Clusters\", y=\"Distortion\", color=\"green\")\nplt.title(\"Distortion\")\nplt.xlabel(\"Number of Clusters\")\n\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\nText(0.5, 0, 'Number of Clusters')\n\n\n\n\n\n\n\n\nBased on the graphs above, we can determine that the optimal number of clusters to use is 5.\n\n\nK-Means Silhouette Score\n\n\nCode\nimport sklearn.cluster\n\ndef maximize_silhouette(X,algo=\"birch\",nmax=20,i_plot=False):\n\n    i_print=False\n\n    X=np.ascontiguousarray(X) \n\n    params=[]; sil_scores=[]\n    sil_max=-10\n    for param in range(2,nmax+1):\n        if(algo==\"birch\"):\n            model = sklearn.cluster.Birch(n_clusters=param).fit(X)\n            labels=model.predict(X)\n\n        if(algo==\"ag\"):\n            model = sklearn.cluster.AgglomerativeClustering(n_clusters=param).fit(X)\n            labels=model.labels_\n\n        if(algo==\"dbscan\"):\n            param=0.5*(param-1)\n            model = sklearn.cluster.DBSCAN(eps=param).fit(X)\n            labels=model.labels_\n\n        if(algo==\"kmeans\"):\n            model = sklearn.cluster.KMeans(n_clusters=param).fit(X)\n            labels=model.predict(X)\n\n        try:\n            sil_scores.append(sklearn.metrics.silhouette_score(X,labels))\n            params.append(param)\n        except:\n            continue \n\n        if(i_print): print(param,sil_scores[-1])\n        \n        if(sil_scores[-1]&gt;sil_max):\n             opt_param=param\n             sil_max=sil_scores[-1]\n             opt_labels=labels\n\n    print(\"-----------------\")\n    print(\"OPTIMAL PARAMETER =\",opt_param)\n    print(\"-----------------\")\n\n    if(i_plot):\n        fig, ax = plt.subplots()\n        ax.plot(params, sil_scores, \"-o\")  \n        ax.set(xlabel='Hyper-parameter', ylabel='Silhouette')\n        plt.show()\n\n    return opt_labels\n\n\n\n\nCode\ndef plot(X,color_vector):\n    fig, ax = plt.subplots()\n    jitter = 0.01 * np.random.randn(*X.shape)\n    X += jitter \n    ax.scatter(X.iloc[:,0], X.iloc[:,1],c=color_vector, cmap=\"viridis\")\n    ax.set(xlabel='Feature-1 (x_1)', ylabel='Feature-2 (x_2)',\n    title='Cluster data')\n    ax.grid()\n    plt.show()\n\n\n\n\nCode\nimport sklearn.cluster\nopt_labels=maximize_silhouette(X,algo=\"kmeans\",nmax=50, i_plot=True)\nplot(X,opt_labels)\n\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\n-----------------\nOPTIMAL PARAMETER = 31\n-----------------\n\n\n\n\n\n\n\n\n\n\nDBSCAN\n\n\nCode\nopt_labels=maximize_silhouette(X,algo=\"dbscan\",nmax=15, i_plot=True)\nplot(X,opt_labels)\n\n\n-----------------\nOPTIMAL PARAMETER = 1.0\n-----------------\n\n\n\n\n\n\n\n\n\n\nAgglomerative Clustering\n\n\nCode\nopt_labels=maximize_silhouette(X,algo=\"ag\",nmax=50, i_plot=True)\nplot(X,opt_labels)\n\n\n-----------------\nOPTIMAL PARAMETER = 31\n-----------------\n\n\n\n\n\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\nlinkage_matrix = linkage(X_norm, method=\"ward\")\ndendrogram(linkage_matrix)\nplt.title('Dendrogram')\n\n\nplt.show()\n\n\n\n\n\n\nFinal Results\n\nimport sklearn.cluster\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef maximize_silhouette(X, algo=\"birch\", nmax=20, i_plot=False):\n    X = np.ascontiguousarray(X)\n    opt_param = 31\n\n    if algo == \"birch\":\n        model = sklearn.cluster.Birch(n_clusters=opt_param).fit(X)\n        labels = model.predict(X)\n\n    elif algo == \"ag\":\n        model = sklearn.cluster.AgglomerativeClustering(n_clusters=opt_param).fit(X)\n        labels = model.labels_\n\n    elif algo == \"dbscan\":\n        param = 0.5 * (opt_param - 1)\n        model = sklearn.cluster.DBSCAN(eps=param).fit(X)\n        labels = model.labels_\n\n    elif algo == \"kmeans\":\n        model = sklearn.cluster.KMeans(n_clusters=opt_param).fit(X)\n        labels = model.predict(X)\n\n    try:\n        sil_score = sklearn.metrics.silhouette_score(X, labels)\n    except:\n        sil_score = None\n\n    print(\"-----------------\")\n    print(\"OPTIMAL PARAMETER =\", opt_param)\n    print(\"-----------------\")\n\n\n    return labels\n\n\n\nCode\nagg_cluster = AgglomerativeClustering(n_clusters=5)\nagg_labels = agg_cluster.fit_predict(X_norm)\nplt.scatter(X_norm[:, 0], X_norm[:, 1], c=agg_labels, cmap=\"viridis\", edgecolors=\"k\")\nplt.title(\"Agglomerative Clustering\")\n\n\nText(0.5, 1.0, 'Agglomerative Clustering')\n\n\n\n\n\n\n\nCode\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\nlinkage_matrix = linkage(X_norm, method=\"ward\")\ndendrogram(linkage_matrix)\nplt.title('Dendrogram')\n\nplt.show()\n\n\n\n\n\n\nKMeans\n\n\nCode\nk=5\nkmeans = KMeans(n_clusters=k)\nkmeans.fit(X)\n\nlabels = kmeans.labels_\ncentroids = kmeans.cluster_centers_\n\nplt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=labels, cmap='viridis', alpha=0.5)\nplt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200)\nplt.title('K-Means Clustering')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.show()\n\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\n\n\n\n\n\nDBSCAN\n\ndbscan = DBSCAN(eps=0.5, min_samples=5)\nlabels = dbscan.fit_predict(X)\n\nplt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=labels, cmap='viridis', alpha=0.5)\nplt.title('DBSCAN Clustering')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.show()"
  },
  {
    "objectID": "tabs/clustering/clustering.html#results",
    "href": "tabs/clustering/clustering.html#results",
    "title": "Introduction",
    "section": "Results",
    "text": "Results\nAgglomerative Clustering was the most successful clustering methods as it produced distinct clusters. K-Means and DBSCAN were somewhat successful but the clusters weren’t well defined and the graphs were hard to read. All methods were easy to implement but for this data, agglometative clustering is definitely the best option. I would say that based on the agglomerative clustering results, we can learn that clusters are based off of injury prevention methods that athletes took. Points within similar clusters represent athletes who took similar measures.\nThe optimal number of clusters was determined to be 5. This makes sense based on the nature of the features. There were 5 prevention measures that were very commonly used, which is illustrated in the plot below. I think we can perhaps make a connection between the optimal number of clusters and the years of football experience that an athlete has, though, as there are many labels in the dataset, it is hard to be definitive about this.\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npm_counts = si_subset.sum()\npm_counts = pm_counts.sort_values(ascending=False)\n\nsns.barplot(x=pm_counts.index, y=pm_counts.values, color='skyblue')\nplt.title('Prevention Measures Counts')\nplt.xlabel('Prevention Measures')\nplt.ylabel('Count')\nplt.xticks(rotation=45, ha='right')  \nplt.tight_layout()\n\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead"
  },
  {
    "objectID": "tabs/clustering/clustering.html#conclusions",
    "href": "tabs/clustering/clustering.html#conclusions",
    "title": "Introduction",
    "section": "Conclusions",
    "text": "Conclusions\nThe exploration of this soccer injuries data set revealed that among the clustering algorithms tested, Agglomerative Clustering emerged as the most effective method. It successfully delineated distinct clusters, offering a clear representation of athletes who implemented similar injury prevention measures.\nThe determination of an optimal number of clusters, set at 5, aligns well with the nature of the dataset. The five prevalent prevention measures identified in the clusters mirror the commonly used strategies among athletes. This finding underscores the practicality of Agglomerative Clustering in uncovering patterns related to injury prevention strategies.\nFurthermore, the potential correlation between the optimal number of clusters and an athlete’s years of football experience hints at a nuanced relationship worth exploring further. While the dataset contains numerous labels, making it challenging to definitively establish this connection, it opens avenues for future research and investigation.\nOverall, the success of Agglomerative Clustering in providing clear and meaningful clusters emphasizes its applicability in understanding patterns of injury prevention among athletes. These insights contribute not only to the field of sports science but also hold relevance for real-life scenarios, potentially informing coaching strategies, training programs, and injury mitigation efforts in sports. As I continue to delve into the intricate details of athletes’ choices in injury prevention, Agglomerative Clustering stands out as a valuable tool for bringing to light meaningful patterns that can positively impact the health of athletes."
  },
  {
    "objectID": "tabs/about/about.html",
    "href": "tabs/about/about.html",
    "title": "Introduction",
    "section": "",
    "text": "Renee DeMaio (she/her) is a fourth year undergraduate and first year graduate student at Georgetown University. Her undergraduate degree is in Chinese with a minor in mathematics. Her graduate degree is in Data Science and Analytics. She is from Hong Kong, and spent all of her adolecent life in Hong Kong and Singapore. At the age of 18, she moved to the United States for the first time to attend university. Her academic interests include how LGBTQ+ identities exist and flourish within a rigid Chinese social and familial structure, as well as artificial intelligence and machine learning. She hopes to further explore the latter two throughout the duration of her masters. Outside of school, her hobbies include dance, running, cooking, and reading.\n\nFavourite Books:\n\nCrying at H Mart by Michelle Zauner\nSidelined: Sports, Culture, and Being a Woman in America by Julie Dicaro\nKillers of a Certain Age by Deanna Raybourn\nJoan is Okay by Weike Wang\nIf He Had Been with Me by Laura Nowlin\nHappy Place by Emily Henry\n\n     \n\n\nRecent Personal Accomplishments\n\n2023: Acceptance into the DSAN Masters Programme\n2021 & 2022: Completing the National Women’s Half Marathon\n2020: Becoming a Lead Coach at Splash Foundation Hong Kong, an organization that teaches foreign domestic helpers water safety skills and how to swim."
  },
  {
    "objectID": "tabs/naive_bayes/naive_bayes.html",
    "href": "tabs/naive_bayes/naive_bayes.html",
    "title": "Introduction to Naive Bayes",
    "section": "",
    "text": "The Naive Bayes classifier is a supervised machine learning algorithm that is used for classification tasks, such as text classification. The goal of the Naive Bayes classifier is to classify data points into predefined categories, also called classes, by estimating the probability of the data point having each class using a predetermined set of features. At the core of Naive Bayes classifier is Bayes Theorem, a statistical tool that describes the probability of an event with knowledge of conditions that might influence the outcome of the event. When applied in Naive Bayes, it predicts the probability of a given object having a specific class, based on the observed features. The ‘naive’ aspect of Naive Bayes comes from the classifier’s assumption that all the data is independent.\nThere are different versions of Naive Bayes including Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes. The differences between each version is that they are all defined to accommodate different types of data and be used in different scenarios, so it is important to select the variant that best suits your data. Gaussian Naive Bayes is best suited for continuous data whose features are assumed to follow a normal distribution. This variant is best suited for data that consists of continuous, numeric features. Multinomial Naive Bayes is best suited for text or data represented as counts. It is commonly used in text classification, for example in sentiment analysis. Finally, Bernoulli Naive Bayes works best with binary data whose features are binary variables. It is often used in document classification tasks such as spam detection.\n\nPreparing Data for Naive Bayes\nIn order to carry out Naive Bayes, it is important that we preprocess our data. The data cleaning process has happened already and can be found within the data cleaning tab. Since our data is now prepared and cleaned, we need to separate it into training, validation, and testing sets. Training data is typically 80% of a data set and is the data that we give to the model so it can learn the existing releationships and methods that desired outcomes are predicted. The validation set is typically 10% of the data, and is given to the model to help fine tune the model’s hyperparameters and prevent overfitting. Finally, the remaining 10% of the data constitutes the test data, which is the data set given to the model to evaluate the model’s accuracy and performance. It is used to evaluate how well the model performs on unseen data.\n\n\nFeature Selection for Record Data\n\nData Set Up\n\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd \nimport matplotlib as plt\n\nfile_path = \"../../../../data/01-modified-data/injury_prevention_data_soccer.csv\"\nsoccer_injury = pd.read_csv(file_path)\n\nsoccer_injury.head()\n\n\n\n\n\n\n\n\nID\nAge\nHeight\nMass\nTeam\nPosition\nYears of Football Experience\nPrevious Injuries\nNumber of Injuries\nAnkle Injuries\n...\nImportance Injury Prevention\nKnowledgeability\nPrevention Measure Stretching\nPrevention Measure Warm Up\nPrevention Measure Specific Strength Exercises\nPrevention Measure Bracing\nPrevention Measure Taping\nPrevention Measure Shoe Insoles\nPrevention Measure Face Masks\nPrevention Measure Medical Corset\n\n\n\n\n0\n146\n19\n173.0\n67.6\n1\n3\n1\nyes\n6\nyes\n...\n2\n1\nyes\nno\nyes\nno\nno\nno\nno\nno\n\n\n1\n155\n22\n179.5\n71.0\n1\n3\n1\nyes\n2\nno\n...\n1\n1\nyes\nyes\nno\nno\nno\nno\nno\nno\n\n\n2\n160\n22\n175.5\n71.8\n1\n3\n1\nyes\n7\nyes\n...\n1\n1\nyes\nno\nno\nno\nno\nyes\nno\nno\n\n\n3\n164\n23\n190.0\n80.5\n1\n4\n1\nyes\n1\nno\n...\n1\n1\nyes\nyes\nyes\nno\nno\nno\nno\nno\n\n\n4\n145\n19\n173.5\n68.7\n1\n3\n1\nyes\n2\nyes\n...\n1\n2\nyes\nyes\nno\nno\nyes\nno\nno\nno\n\n\n\n\n5 rows × 41 columns\n\n\n\n\nsoccer_injury[\"Number of Injuries\"].unique()\n\narray([ 6,  2,  7,  1,  4,  3,  5,  0, 10])\n\n\n\nsoccer_injury.columns = soccer_injury.columns.str.strip()\nsoccer_injury.columns\n\nIndex(['ID', 'Age', 'Height', 'Mass', 'Team', 'Position',\n       'Years of Football Experience', 'Previous Injuries',\n       'Number of Injuries', 'Ankle Injuries', 'Number of Ankle Injuries',\n       'Severe_Ankle_Injuries', 'Noncontact_Ankle_Injuries', 'Knee Injuries',\n       'Number of Knee Injuries', 'Severe_Knee_Injuries',\n       'Noncontact_Knee_Injuries', 'Thigh_Injuries',\n       'Number of Thigh Injuries', 'Severe_Thigh_Injuries',\n       'Noncontact_Thigh_Injuries', 'Risk Factor Condition',\n       'Risk Factor Coordination', 'Risk Factor Muscle Impairments',\n       'Risk Factor Fatigue', 'Risk Factor Previous Injury',\n       'Risk Factor Attentiveness', 'Risk Factor Other Player',\n       'Risk Factor Equipment', 'Risk Factor Climatic Condition',\n       'Risk Factor Diet', 'Importance Injury Prevention', 'Knowledgeability',\n       'Prevention Measure Stretching', 'Prevention Measure Warm Up',\n       'Prevention Measure Specific Strength Exercises',\n       'Prevention Measure Bracing', 'Prevention Measure Taping',\n       'Prevention Measure Shoe Insoles', 'Prevention Measure Face Masks',\n       'Prevention Measure Medical Corset'],\n      dtype='object')\n\n\n\nsoccer_injury = soccer_injury.replace({\"yes\": 1, \"no\": 0})\nsoccer_injury.head()\n\n\n\n\n\n\n\n\nID\nAge\nHeight\nMass\nTeam\nPosition\nYears of Football Experience\nPrevious Injuries\nNumber of Injuries\nAnkle Injuries\n...\nImportance Injury Prevention\nKnowledgeability\nPrevention Measure Stretching\nPrevention Measure Warm Up\nPrevention Measure Specific Strength Exercises\nPrevention Measure Bracing\nPrevention Measure Taping\nPrevention Measure Shoe Insoles\nPrevention Measure Face Masks\nPrevention Measure Medical Corset\n\n\n\n\n0\n146\n19\n173.0\n67.6\n1\n3\n1\n1\n6\n1\n...\n2\n1\n1\n0\n1\n0\n0\n0\n0\n0\n\n\n1\n155\n22\n179.5\n71.0\n1\n3\n1\n1\n2\n0\n...\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n\n\n2\n160\n22\n175.5\n71.8\n1\n3\n1\n1\n7\n1\n...\n1\n1\n1\n0\n0\n0\n0\n1\n0\n0\n\n\n3\n164\n23\n190.0\n80.5\n1\n4\n1\n1\n1\n0\n...\n1\n1\n1\n1\n1\n0\n0\n0\n0\n0\n\n\n4\n145\n19\n173.5\n68.7\n1\n3\n1\n1\n2\n1\n...\n1\n2\n1\n1\n0\n0\n1\n0\n0\n0\n\n\n\n\n5 rows × 41 columns\n\n\n\n\n\nRandom Guessing\nI’ve decided to first use a random guessing method to predict my target variable before using Naive Bayes. This is to ensure that I have a point of comparison for when I look at my Naive Bayes accuracy results and to ensure that I can properly contextualize the results.\n\nx_vars = ['Prevention Measure Stretching', 'Prevention Measure Warm Up',\n       'Prevention Measure Specific Strength Exercises',\n       'Prevention Measure Bracing', 'Prevention Measure Taping',\n       'Prevention Measure Shoe Insoles', 'Prevention Measure Face Masks',\n       'Prevention Measure Medical Corset']\ny_var = \"Number of Injuries\"\n\n\ncolumns_keep = ['Prevention Measure Stretching', 'Prevention Measure Warm Up',\n       'Prevention Measure Specific Strength Exercises',\n       'Prevention Measure Bracing', 'Prevention Measure Taping',\n       'Prevention Measure Shoe Insoles', 'Prevention Measure Face Masks',\n       'Prevention Measure Medical Corset', \"Number of Injuries\"]\ndf_features = soccer_injury[columns_keep]\ndf_features.head()\n\n\n\n\n\n\n\n\nPrevention Measure Stretching\nPrevention Measure Warm Up\nPrevention Measure Specific Strength Exercises\nPrevention Measure Bracing\nPrevention Measure Taping\nPrevention Measure Shoe Insoles\nPrevention Measure Face Masks\nPrevention Measure Medical Corset\nNumber of Injuries\n\n\n\n\n0\n1\n0\n1\n0\n0\n0\n0\n0\n6\n\n\n1\n1\n1\n0\n0\n0\n0\n0\n0\n2\n\n\n2\n1\n0\n0\n0\n0\n1\n0\n0\n7\n\n\n3\n1\n1\n1\n0\n0\n0\n0\n0\n1\n\n\n4\n1\n1\n0\n0\n1\n0\n0\n0\n2\n\n\n\n\n\n\n\n\nimport numpy as np\nnum_obs = len(df_features)\nrng = np.random.default_rng(30)\nrandom_guesses = rng.choice(range(0,10), num_obs)\ndf_features[\"random_guesses\"] = random_guesses\ndf_features\n\n/var/folders/kw/p7j6fdpx3vvgm6c0krhwpvb80000gn/T/ipykernel_20251/1720700936.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_features[\"random_guesses\"] = random_guesses\n\n\n\n\n\n\n\n\n\nPrevention Measure Stretching\nPrevention Measure Warm Up\nPrevention Measure Specific Strength Exercises\nPrevention Measure Bracing\nPrevention Measure Taping\nPrevention Measure Shoe Insoles\nPrevention Measure Face Masks\nPrevention Measure Medical Corset\nNumber of Injuries\nrandom_guesses\n\n\n\n\n0\n1\n0\n1\n0\n0\n0\n0\n0\n6\n1\n\n\n1\n1\n1\n0\n0\n0\n0\n0\n0\n2\n2\n\n\n2\n1\n0\n0\n0\n0\n1\n0\n0\n7\n7\n\n\n3\n1\n1\n1\n0\n0\n0\n0\n0\n1\n4\n\n\n4\n1\n1\n0\n0\n1\n0\n0\n0\n2\n3\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n134\n1\n0\n0\n0\n0\n0\n0\n0\n0\n2\n\n\n135\n1\n0\n0\n1\n0\n0\n0\n0\n2\n3\n\n\n136\n1\n1\n1\n0\n1\n0\n0\n1\n2\n6\n\n\n137\n1\n0\n0\n0\n1\n1\n0\n0\n1\n5\n\n\n138\n1\n1\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n\n139 rows × 10 columns\n\n\n\n\nfrom sklearn.metrics import f1_score\n\nf1 = f1_score(df_features[\"Number of Injuries\"], df_features[\"random_guesses\"], average=\"macro\")\nprint(f\"f1 score: {f1}\")\ndf_features[\"random_guesses_correct\"] = df_features[\"Number of Injuries\"] == df_features[\"random_guesses\"]\ncorrect_random_guess_rows = df_features[df_features[\"random_guesses_correct\"] == True]\nrandom_guess_accuracy = len(correct_random_guess_rows) / len(df_features)\nprint(f\"random guess accuracy: {random_guess_accuracy}\")\n\nf1 score: 0.07252856433184302\nrandom guess accuracy: 0.10071942446043165\n\n\n/var/folders/kw/p7j6fdpx3vvgm6c0krhwpvb80000gn/T/ipykernel_20251/2051189337.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_features[\"random_guesses_correct\"] = df_features[\"Number of Injuries\"] == df_features[\"random_guesses\"]\n\n\n\n\nNaive Bayes Evaluation\nNow that I have completed random guessing and gotten an accuracy level of approximately 0.101, I will be able to have proper context for the accuracy levels given by Naive Bayes.\n\nX = soccer_injury[x_vars].values.copy()\ny = soccer_injury[y_var].values.copy()\nprint(X.shape, y.shape)\n\n(139, 8) (139,)\n\n\n\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nimport random\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5000)\n\ndef train_GNB_model(x_train,x_test,y_train,y_test,i_print=False):\n    scaler = StandardScaler()\n    x_train_scaled = scaler.fit_transform(x_train)\n    clf = BernoulliNB()\n    clf.fit(x_train_scaled, y_train)\n\n    y_train_pred = clf.predict(x_train_scaled)\n    y_test_pred = clf.predict(scaler.transform(x_test))\n\n    acc_train = accuracy_score(y_train, y_train_pred)\n    acc_test = accuracy_score(y_test, y_test_pred)\n\n    return acc_train, acc_test, clf \n\n\ntracc, teacc, clf = train_GNB_model(X_train, X_test, y_train, y_test)\n\n\nprint(\"train accuracy: \", tracc)\nprint(\"test accuracy: \", teacc)\nprint(\"classifier: \", clf)\n\ntrain accuracy:  0.34234234234234234\ntest accuracy:  0.14285714285714285\nclassifier:  BernoulliNB()\n\n\n\ny_pred = clf.predict(X_test)\nprint(y_pred)\nprint(y_test)\n\n[1 1 1 0 2 1 0 0 1 0 2 0 0 2 2 1 2 1 0 0 2 2 0 1 2 1 2 2]\n[2 7 2 2 2 2 1 0 2 1 1 1 1 3 1 3 1 1 1 1 6 2 1 3 1 4 3 0]\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_pred, y_test)\ncm\n\narray([[1, 7, 1, 0, 0, 0, 0],\n       [0, 1, 4, 2, 1, 0, 1],\n       [1, 4, 2, 2, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0]])\n\n\nNow that I have done Naive Bayes, we can see that Naive Bayes is a more effective method at predicting the number of injuries that an athlete will have based on the prevention methods that they take. The random guess accuracy rate was approximately 0.101 whereas with Naive Bayes, it is approximately 0.142. While still low, the accuracy with Naive Bayes is higher than the accuracy gained with random guessing which means that we are learning something from the data with Naive Bayes. The low accuracy rate could indicate that the number of injuries an athlete will have is a difficult thing to predict.\n\n\nNaive Bayes + Feature Selection\nIn both of the following feature selection methods, I first create an object for the feature selection method using the chi-squared test and another for the bernoulli Naive Bayes classifier as I am working with binary data. Next the model selects and transforms the training data based on the importance of each feature. The importance is determiend using the chi-squared test. The test data is then transformed using the same selected features as the training set. Finally, the Bernoulli Naive Bayes classifier fits the selected training data from X and y, and creates predictions using the trained data.\nThe following metrics are used to measure the effectiveness of the model: accuracy, precision, recall, and f1 score. The accuracy score is one that measures overall model performance. It quantifies the proprotion of correctly predicted instances, both positive and negative, out of the total number of instaces. Precision calculates the numnber of correctly predicted positives out of all of the predicted positives. Recall is the measure of the model’s ability to corrently identify positive instances. It measures the true positives out of the true positives and false negatives combined. Finally, the F1 score is considered the harmonic mean of precision and recall.\nIt is important that we are conscious of overfitting and underfitting. Overfitting occurs when a model is too complex and fits the training data too closely, including noise in the data. This model typically has low bias and high variance. Underfitting is when a model is too simplistic to capture underlying patterns in the data. Underfit models have high bias and low variance. Optimal fitting is where the model strikes a balance between bias and variance. I think that my model is fit relatively well. I think some of the ambiguity I am facing in my data stems from the target label being difficult to predict in nature.\n\nSelectPercentile\n\nfrom sklearn.feature_selection import SelectKBest, chi2, SelectPercentile\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\nselector = SelectPercentile(chi2, percentile=10)\n\nbernoilli_nb = BernoulliNB()\n\nX_train_selected = selector.fit_transform(X_train, y_train)\nX_test_selected = selector.transform(X_test)\nbernoilli_nb.fit(X_train_selected, y_train)\ny_pred = bernoilli_nb.predict(X_test_selected)\n\n\naccuracy = bernoilli_nb.score(X_test_selected, y_test)\nprint(f\"Accuracy Score: {accuracy}\")\nprecision_macro = precision_score(y_test, y_pred, average=\"macro\")\nprint(f\"Precision (Macro): {precision_macro}\")\nprecision_weighted = precision_score(y_test, y_pred, average=\"weighted\")\nprint(f\"Precision (Weighted): {precision_weighted}\")\nrecall_macro = recall_score(y_test, y_pred, average=\"macro\")\nprint(f\"Recall (Macro): {recall_macro}\")\nrecall_weighted = recall_score(y_test, y_pred, average=\"weighted\")\nprint(f\"Recall (Weighted): {recall_weighted}\")\nf1_macro = f1_score(y_test, y_pred, average=\"macro\")\nprint(f\"F1 Score (Macro): {f1_macro}\")\nf1_weighted = f1_score(y_test, y_pred, average=\"weighted\")\nprint(f\"F1 Score (Weighted): {f1_weighted}\")\n\nAccuracy Score: 0.25\nPrecision (Macro): 0.03571428571428571\nPrecision (Weighted): 0.0625\nRecall (Macro): 0.14285714285714285\nRecall (Weighted): 0.25\nF1 Score (Macro): 0.05714285714285715\nF1 Score (Weighted): 0.1\n\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\nIn my future work on this project, I will need to go back to see how SelectPercentile is subsetting its features.\n\n\nSelectKBest\n\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\nselector = SelectKBest(chi2, k=\"all\")\n\nbernoilli_nb = BernoulliNB()\n\nX_train_selected = selector.fit_transform(X_train, y_train)\nX_test_selected = selector.transform(X_test)\nbernoilli_nb.fit(X_train_selected, y_train)\ny_pred = bernoilli_nb.predict(X_test_selected)\n\n\naccuracy = bernoilli_nb.score(X_test_selected, y_test)\nprint(f\"Accuracy Score: {accuracy}\")\nprecision_macro = precision_score(y_test, y_pred, average=\"macro\")\nprint(f\"Precision (Macro): {precision_macro}\")\nprecision_weighted = precision_score(y_test, y_pred, average=\"weighted\")\nprint(f\"Precision (Weighted): {precision_weighted}\")\nrecall_macro = recall_score(y_test, y_pred, average=\"macro\")\nprint(f\"Recall (Macro): {recall_macro}\")\nrecall_weighted = recall_score(y_test, y_pred, average=\"weighted\")\nprint(f\"Recall (Weighted): {recall_weighted}\")\nf1_macro = f1_score(y_test, y_pred, average=\"macro\")\nprint(f\"F1 Score (Macro): {f1_macro}\")\nf1_weighted = f1_score(y_test, y_pred, average=\"weighted\")\nprint(f\"F1 Score (Weighted): {f1_weighted}\")\n\n\n\nAccuracy Score: 0.14285714285714285\nPrecision (Macro): 0.06031746031746032\nPrecision (Weighted): 0.10555555555555556\nRecall (Macro): 0.12414965986394558\nRecall (Weighted): 0.14285714285714285\nF1 Score (Macro): 0.07319291352904798\nF1 Score (Weighted): 0.11262686892938993\n\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\n\ny_test\n\narray([2, 7, 2, 2, 2, 2, 1, 0, 2, 1, 1, 1, 1, 3, 1, 3, 1, 1, 1, 1, 6, 2,\n       1, 3, 1, 4, 3, 0])\n\n\n\ny_pred\n\narray([1, 1, 1, 0, 2, 1, 0, 0, 1, 0, 2, 0, 0, 2, 2, 1, 2, 1, 0, 0, 2, 2,\n       0, 1, 2, 1, 2, 2])\n\n\nI tested out two feature selection methods, SelectKBest and SelectPercentile. SelectKBest returned the same accuracy that Naive Bayes did which was 0.142. The accuracy with the SelectPercentile method,however, significantly increased from the one obtained only using Naive Bayes as it was 0.25.\n\nConfusion Matrix\n\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\n\ncm = confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(8,6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\n\nText(0.5, 1.0, 'Confusion Matrix')\n\n\n\n\n\nThe confusion matrix made up of my predicted and test data is quite interesting. I recognize that there is an abudance of zeros in the right side of the data which prompts further analysis about how effective the model is on estimate relationships between the data. Based on what I see, I think that it is likely that the data got complicated with as many features as it selected, so it decided to only focus on predicting instances where athletes had 0, 1, or 2 injuries.\n\n\nActual vs Predicted Number of Injuries Plotted\n\nplt.figure(figsize=(8, 6))\nplt.plot(y_test, label='Actual')\nplt.plot(y_pred, label='Predicted')\nplt.xlabel('Sample')\nplt.ylabel('Number of Injuries')\nplt.title('Comparison between Actual and Predicted Number of Injuries')\nplt.legend()\nplt.show()\n\n\n\n\nThis line graph of the predicted data vs actual data is not very specific, however, it does show us that based on the features that the model used, the number of injuries an athlete may have is a complicated thing to predict.\n\n\n\nChi-Squared Test Scores of Selected Features\n\nfeature_scores = selector.scores_\nselected_feature_names = [col for col, mask in zip(X, selector.get_support()) if mask]\n\n\nprint(selected_feature_names)\n\n[array([1, 0, 1, 0, 0, 0, 0, 0]), array([1, 1, 0, 0, 0, 0, 0, 0]), array([1, 0, 0, 0, 0, 1, 0, 0]), array([1, 1, 1, 0, 0, 0, 0, 0]), array([1, 1, 0, 0, 1, 0, 0, 0]), array([1, 1, 1, 0, 0, 0, 0, 0]), array([1, 1, 0, 0, 1, 1, 0, 0]), array([1, 0, 1, 0, 0, 0, 0, 0])]\n\n\n\nx_labels = ['Prevention Measure Stretching', 'Prevention Measure Warm Up',\n       'Prevention Measure Specific Strength Exercises',\n       'Prevention Measure Bracing', 'Prevention Measure Taping',\n       'Prevention Measure Shoe Insoles', 'Prevention Measure Face Masks',\n       'Prevention Measure Medical Corset']\nplt.figure(figsize=(10, 6))\nplt.bar(range(len(selected_feature_names)), feature_scores)\nplt.xlabel('Features')\nplt.xticks(range(len(x_labels)), x_labels)\nplt.ylabel('Score')\nplt.title('Chi-Squared Test Scores of Selected Features')\nplt.xticks(rotation=90)\nplt.show()\n\n\n\n\nThis graph shows what features are more associated with a higher number of injuries. The graph indicates that instances where athletes braced, taped, or used shoe insoles were more associated with a higher number of injuries, whereas instances where athletes utilized stretching, warming up, and strength exercises were more associated with a lower number of instances. In other words, stretching, warming up, and performing strengthening exercises are more likely to help prevent an injury. When we contextualize the results we understand that the instances where face masks or medical corsets are less common as they are used when the injury is more severe.\n\n\n\nConclusion Record Data\nThe findings of using Naive Bayes and Feature Selection on this set of record data is that overall, the number of injuries that an athlete has is a hard thing to predict based on the prevention measures that they utilize. This data will be documented using this website, as well as on my GitHub. When formatted in presentations or reports, this data would be best represented in a tabular or graphic format for easy readability.\n\n\n\nFeature Selection for Text Data\n\nData Set Up\n\ndf_features.columns\n\nprevention_stretching = \"Athletes can help protect themselves by preparing before and after a game or practice session by warming up muscles and then stretching. Exercises can include forward lunges, side lunges, standing quad stretch, seated straddle lotus, seated side straddle, seated toe touch, and the knees to chest stretch. Hold each stretch for 20 seconds. \"\nprevention_warm_up = \"Warming up involves increasing the body's core temperature, heart rate, respiratory rate, and the body's muscle temperatures. By increasing muscle temperature, muscles become more loose and pliable, and by increasing heart rate and resipiatory rate, blood flood increases which helps to increase delivery of oxygen and nutrients to muscles. Warm up exercises include dynamic stretches, light bike riding, light jogging, jumproping, etc. \"\nprevention_specific_strength = \"Specific strength exercises can refer to a wide range of exercises, from which a few are selected based each athlete. These exercises can range from muscle group specific weight-lifting exercises, to physical therapy-like exercises. These exercises depend on each athlete and are hard to define. \"\nprevention_bracing = \"Basic braces provide general support and compression to specific areas of the body. More complex braces can do the same things, as well as promote healing, necessarily restrict movement, take weight off of an injury, etc. \"\nprevention_taping = \"Taping can be used to reduce the range of motion at a joint and decrease swelling, which in turn can alleviate pain and prevent further injury. \"\nprevention_shoe_insoles = \"Orthotics can alignment of an athlete's feet, ankles, knees, hips and back which can help prevent injuries. They can also absorb shock from impact of running to reduce stress on the athlete's joints and tissues. \"\nprevention_face_masks = \"Athletic face masks can be used to protect maxillary, nasal, zygomatic and orbital injuries. These are worn in sports where a face injury could possible occur. \"\nprevention_medical_corset = \"A medical corset is a corset that can be worn to help an athlete stablize their spine after a fracture or surgery. It will remind the athlete to not move in certain directors or to move more slowly to prevent causing further injury. \"\n\nprevention_definitions = {\n    \"Prevention Measure Stretching\": prevention_stretching,\n    \"Prevention Measure Warm Up\": prevention_warm_up,\n    'Prevention Measure Specific Strength Exercises': prevention_specific_strength,\n    'Prevention Measure Bracing': prevention_bracing, \n    'Prevention Measure Taping': prevention_taping,\n    'Prevention Measure Shoe Insoles': prevention_shoe_insoles, \n    'Prevention Measure Face Masks': prevention_face_masks,\n    'Prevention Measure Medical Corset': prevention_medical_corset\n}\n\ncols_keep = ['Number of Injuries', 'Prevention Measure Stretching', 'Prevention Measure Warm Up',\n       'Prevention Measure Specific Strength Exercises',\n       'Prevention Measure Bracing', 'Prevention Measure Taping',\n       'Prevention Measure Shoe Insoles', 'Prevention Measure Face Masks',\n       'Prevention Measure Medical Corset']\nprevention_cols = ['Prevention Measure Stretching', 'Prevention Measure Warm Up',\n       'Prevention Measure Specific Strength Exercises',\n       'Prevention Measure Bracing', 'Prevention Measure Taping',\n       'Prevention Measure Shoe Insoles', 'Prevention Measure Face Masks',\n       'Prevention Measure Medical Corset']\n\n\nsi_subset = soccer_injury[cols_keep]\nsi_subset_prevention = si_subset[prevention_cols]\nsi_subset_prevention.head()\n\n\n\n\n\n\n\n\nPrevention Measure Stretching\nPrevention Measure Warm Up\nPrevention Measure Specific Strength Exercises\nPrevention Measure Bracing\nPrevention Measure Taping\nPrevention Measure Shoe Insoles\nPrevention Measure Face Masks\nPrevention Measure Medical Corset\n\n\n\n\n0\n1\n0\n1\n0\n0\n0\n0\n0\n\n\n1\n1\n1\n0\n0\n0\n0\n0\n0\n\n\n2\n1\n0\n0\n0\n0\n1\n0\n0\n\n\n3\n1\n1\n1\n0\n0\n0\n0\n0\n\n\n4\n1\n1\n0\n0\n1\n0\n0\n0\n\n\n\n\n\n\n\n\nsi_subset[\"Prevention Measures Definitions\"] = \"\"\n\n/var/folders/kw/p7j6fdpx3vvgm6c0krhwpvb80000gn/T/ipykernel_20251/2335233446.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  si_subset[\"Prevention Measures Definitions\"] = \"\"\n\n\n\nfor col in si_subset.columns:\n    if col.startswith(\"Prevention\"):\n        si_subset[\"Prevention Measures Definitions\"] += si_subset[col].apply(\n            lambda x: prevention_definitions[col] if x == 1 else \"\"\n        )\n\n/var/folders/kw/p7j6fdpx3vvgm6c0krhwpvb80000gn/T/ipykernel_20251/236569242.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  si_subset[\"Prevention Measures Definitions\"] += si_subset[col].apply(\n\n\n\nsi_subset.head()\n\n\n\n\n\n\n\n\nNumber of Injuries\nPrevention Measure Stretching\nPrevention Measure Warm Up\nPrevention Measure Specific Strength Exercises\nPrevention Measure Bracing\nPrevention Measure Taping\nPrevention Measure Shoe Insoles\nPrevention Measure Face Masks\nPrevention Measure Medical Corset\nPrevention Measures Definitions\n\n\n\n\n0\n6\n1\n0\n1\n0\n0\n0\n0\n0\nAthletes can help protect themselves by prepar...\n\n\n1\n2\n1\n1\n0\n0\n0\n0\n0\n0\nAthletes can help protect themselves by prepar...\n\n\n2\n7\n1\n0\n0\n0\n0\n1\n0\n0\nAthletes can help protect themselves by prepar...\n\n\n3\n1\n1\n1\n1\n0\n0\n0\n0\n0\nAthletes can help protect themselves by prepar...\n\n\n4\n2\n1\n1\n0\n0\n1\n0\n0\n0\nAthletes can help protect themselves by prepar...\n\n\n\n\n\n\n\n\n\nText Pre-processing\n\nimport nltk\nnltk.download('punkt')\nnltk.download('wordnet')\n\n[nltk_data] Downloading package punkt to\n[nltk_data]     /Users/reneedemaio/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/reneedemaio/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n\n\nTrue\n\n\n\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem.wordnet import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n\n\nfrom nltk.corpus import stopwords\n\nfrom tqdm import tqdm\ntqdm.pandas()\n\nstopwords = set(stopwords.words(\"english\"))\n\n\ncustom_stopwords = [\"athlete\", \"include\", \"these\", \"further\", \"like\", 'a', 'about', 'all', 'also', 'an', 'and', 'are', 'as', 'at',\n    'be', 'both', 'but', 'by',\n    'can',\n    'do', \"etc\",\n    'for', 'from',\n    'get', 'go',\n    'had', 'have',\n    'i', 'if', 'in', 'is', 'it',\n    'me', 'more', 'my',\n    'no', 'not',\n    'of', 'on', 'one', 'or', 'out',\n    'should', \"should've\", 'so',\n    'take', 'than', 'that', 'the', 'this', 'to', 'too',\n    'up',\n    'very',\n    'want', 'was', 'we', 'were', 'what', 'where', 'which', 'with', 'would', \"would've\",\n    'you', 'your']\ncustom_stoplemmas = [\n    's'\n]\n\n\nimport string\nfrom collections import Counter\ntoken_counter = Counter()\n\ndef remove_special_chars(token):\n  return token.translate(str.maketrans('', '', string.punctuation))\n\ndef remove_digits(token):\n  return ''.join([c for c in token if not c.isdigit()])\n\n\ndef clean_def(definitions):\n  definitions_cleaned = definitions.lower()\n  definitions_applied = sent_tokenize(definitions_cleaned)\n  clean_applied = []\n  for d in definitions_applied:\n    def_tokens = word_tokenize(d)\n    df_tokens_cleaned = [t for t in def_tokens if t not in custom_stopwords]\n    df_tokens_cleaned = [remove_digits(t) for t in df_tokens_cleaned]\n    df_tokens_cleaned = [t.replace(\"-\", \" \") for t in df_tokens_cleaned]\n    df_tokens_cleaned = [remove_special_chars(t) for t in df_tokens_cleaned]\n    df_tokens_cleaned = [t for t in df_tokens_cleaned if len(t) &gt; 0]\n    df_tokens_cleaned = [lemmatizer.lemmatize(t) for t in df_tokens_cleaned]\n    df_tokens_cleaned = [t for t in df_tokens_cleaned if t not in custom_stoplemmas]\n    token_counter.update(df_tokens_cleaned)\n    clean_apply = ' '.join(df_tokens_cleaned)\n    clean_applied.append(clean_apply)\n  review_final = \". \".join(clean_applied)\n  return review_final\nsi_subset[\"Prevention Measures Definitions Cleaned\"] = si_subset[\"Prevention Measures Definitions\"].progress_apply(clean_def)\n\n  0%|          | 0/139 [00:00&lt;?, ?it/s]100%|██████████| 139/139 [00:00&lt;00:00, 177.40it/s]\n/var/folders/kw/p7j6fdpx3vvgm6c0krhwpvb80000gn/T/ipykernel_20251/888061501.py:30: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  si_subset[\"Prevention Measures Definitions Cleaned\"] = si_subset[\"Prevention Measures Definitions\"].progress_apply(clean_def)\n\n\n\nsi_subset.head()\n\n\n\n\n\n\n\n\nNumber of Injuries\nPrevention Measure Stretching\nPrevention Measure Warm Up\nPrevention Measure Specific Strength Exercises\nPrevention Measure Bracing\nPrevention Measure Taping\nPrevention Measure Shoe Insoles\nPrevention Measure Face Masks\nPrevention Measure Medical Corset\nPrevention Measures Definitions\nPrevention Measures Definitions Cleaned\n\n\n\n\n0\n6\n1\n0\n1\n0\n0\n0\n0\n0\nAthletes can help protect themselves by prepar...\nathlete help protect themselves preparing befo...\n\n\n1\n2\n1\n1\n0\n0\n0\n0\n0\n0\nAthletes can help protect themselves by prepar...\nathlete help protect themselves preparing befo...\n\n\n2\n7\n1\n0\n0\n0\n0\n1\n0\n0\nAthletes can help protect themselves by prepar...\nathlete help protect themselves preparing befo...\n\n\n3\n1\n1\n1\n1\n0\n0\n0\n0\n0\nAthletes can help protect themselves by prepar...\nathlete help protect themselves preparing befo...\n\n\n4\n2\n1\n1\n0\n0\n1\n0\n0\n0\nAthletes can help protect themselves by prepar...\nathlete help protect themselves preparing befo...\n\n\n\n\n\n\n\n\ntoken_counter.most_common(25)\n\n[('exercise', 478),\n ('muscle', 473),\n ('stretch', 456),\n ('seated', 381),\n ('rate', 300),\n ('lunge', 254),\n ('side', 254),\n ('straddle', 254),\n ('help', 241),\n ('increasing', 225),\n ('temperature', 225),\n ('each', 219),\n ('warming', 202),\n ('body', 166),\n ('knee', 165),\n ('heart', 150),\n ('increase', 150),\n ('light', 150),\n ('range', 142),\n ('after', 128),\n ('athlete', 127),\n ('protect', 127),\n ('themselves', 127),\n ('preparing', 127),\n ('before', 127)]\n\n\n\n\nVectorization\nThis text data is trained using vectorization. First the corpus is created and is fitted using CountVectorizer. This finds the frequency of the words so the most common words can then be easily found. Then, KMeans clustering will be used to assign each data point to a cluster.\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncorpus = si_subset[\"Prevention Measures Definitions Cleaned\"].values\n\n\nmax_document_freq = 0.4\nmin_document_count = 6\n\ncv = CountVectorizer(max_df=max_document_freq, min_df=min_document_count)\nX = cv.fit_transform(corpus)\nprint(X)\n\nfeature_names = cv.get_feature_names_out()\nfeature_names\n\ncv_ng = CountVectorizer(max_df=max_document_freq, min_df=min_document_count,\n                              stop_words=custom_stopwords)\nX_ng = cv_ng.fit_transform(corpus)\nX_ng.shape\n\n  (0, 39)   1\n  (0, 33)   1\n  (0, 51)   1\n  (0, 14)   1\n  (0, 37)   1\n  (0, 6)    1\n  (0, 17)   1\n  (0, 22)   1\n  (0, 30)   1\n  (0, 44)   1\n  (0, 23)   1\n  (0, 13)   1\n  (0, 18)   1\n  (0, 12)   1\n  (2, 28)   1\n  (2, 1)    1\n  (2, 15)   1\n  (2, 3)    1\n  (2, 20)   1\n  (2, 5)    1\n  (2, 45)   1\n  (2, 0)    1\n  (2, 38)   1\n  (2, 21)   1\n  (2, 35)   1\n  : :\n  (136, 42) 1\n  (136, 48) 1\n  (136, 2)  1\n  (136, 29) 1\n  (137, 28) 1\n  (137, 1)  1\n  (137, 15) 1\n  (137, 3)  1\n  (137, 20) 1\n  (137, 5)  1\n  (137, 45) 1\n  (137, 0)  1\n  (137, 38) 1\n  (137, 21) 1\n  (137, 35) 1\n  (137, 40) 1\n  (137, 47) 1\n  (137, 43) 1\n  (137, 49) 1\n  (137, 24) 1\n  (137, 11) 1\n  (137, 42) 1\n  (137, 48) 1\n  (137, 2)  1\n  (137, 29) 1\n\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ve'] not in stop_words.\n  warnings.warn(\n\n\n(139, 51)\n\n\n\n\nKMeans Clustering\n\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(init=\"random\", n_clusters=2, n_init=4, random_state=5000)\nkmeans.fit(X)\ny_pred = kmeans.predict(X)\ny_pred\n\narray([0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,\n       1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,\n       0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n       0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n       1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 0, 1, 1], dtype=int32)\n\n\n\nlen(y_pred[y_pred==0])\n\n46\n\n\n\nlen(y_pred[y_pred==1])\n\n93\n\n\n\nfrom sklearn.metrics import pairwise_distances_argmin_min\n\ncentroids = kmeans.cluster_centers_\ncentroids.shape\n\n(2, 52)\n\n\n\nargmins, mins = pairwise_distances_argmin_min(centroids, X)\nargmins, mins\n\n(array([0, 1]), array([1.63260725, 1.44662624]))\n\n\n\nargmins_0 = argmins[0]\ny_pred[argmins_0]\n\n0\n\n\n\nsi_subset.iloc[argmins_0][[\"Prevention Measures Definitions Cleaned\"]]\n\nPrevention Measures Definitions Cleaned    athlete help protect themselves preparing befo...\nName: 0, dtype: object\n\n\n\nargmins_1 = argmins[1]\ny_pred[argmins_1]\n\n1\n\n\n\nsi_subset.iloc[argmins_1][\"Prevention Measures Definitions Cleaned\"]\n\n'athlete help protect themselves preparing before after game practice session warming muscle then stretching. exercise forward lunge side lunge standing quad stretch seated straddle lotus seated side straddle seated toe touch knee chest stretch. hold each stretch second. warming involves increasing body core temperature heart rate respiratory rate body muscle temperature. increasing muscle temperature muscle become loose pliable increasing heart rate resipiatory rate blood flood increase help increase delivery oxygen nutrient muscle. warm exercise dynamic stretch light bike riding light jogging jumproping'\n\n\n\nsi_subset[\"y_pred\"] = y_pred\nsi_subset.head()\n\n/var/folders/kw/p7j6fdpx3vvgm6c0krhwpvb80000gn/T/ipykernel_20251/419468994.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  si_subset[\"y_pred\"] = y_pred\n\n\n\n\n\n\n\n\n\nNumber of Injuries\nPrevention Measure Stretching\nPrevention Measure Warm Up\nPrevention Measure Specific Strength Exercises\nPrevention Measure Bracing\nPrevention Measure Taping\nPrevention Measure Shoe Insoles\nPrevention Measure Face Masks\nPrevention Measure Medical Corset\nPrevention Measures Definitions\nPrevention Measures Definitions Cleaned\ny_pred\n\n\n\n\n0\n6\n1\n0\n1\n0\n0\n0\n0\n0\nAthletes can help protect themselves by prepar...\nathlete help protect themselves preparing befo...\n0\n\n\n1\n2\n1\n1\n0\n0\n0\n0\n0\n0\nAthletes can help protect themselves by prepar...\nathlete help protect themselves preparing befo...\n1\n\n\n2\n7\n1\n0\n0\n0\n0\n1\n0\n0\nAthletes can help protect themselves by prepar...\nathlete help protect themselves preparing befo...\n1\n\n\n3\n1\n1\n1\n1\n0\n0\n0\n0\n0\nAthletes can help protect themselves by prepar...\nathlete help protect themselves preparing befo...\n0\n\n\n4\n2\n1\n1\n0\n0\n1\n0\n0\n0\nAthletes can help protect themselves by prepar...\nathlete help protect themselves preparing befo...\n1\n\n\n\n\n\n\n\n\npd.crosstab(si_subset['Number of Injuries'], si_subset['y_pred'])\n\n\n\n\n\n\n\ny_pred\n0\n1\n\n\nNumber of Injuries\n\n\n\n\n\n\n0\n6\n16\n\n\n1\n11\n30\n\n\n2\n15\n27\n\n\n3\n5\n10\n\n\n4\n5\n6\n\n\n5\n2\n1\n\n\n6\n2\n0\n\n\n7\n0\n2\n\n\n10\n0\n1\n\n\n\n\n\n\n\n\npd.crosstab(si_subset['Number of Injuries'], si_subset['y_pred'], normalize='all', margins=True)\n\n\n\n\n\n\n\ny_pred\n0\n1\nAll\n\n\nNumber of Injuries\n\n\n\n\n\n\n\n0\n0.043165\n0.115108\n0.158273\n\n\n1\n0.079137\n0.215827\n0.294964\n\n\n2\n0.107914\n0.194245\n0.302158\n\n\n3\n0.035971\n0.071942\n0.107914\n\n\n4\n0.035971\n0.043165\n0.079137\n\n\n5\n0.014388\n0.007194\n0.021583\n\n\n6\n0.014388\n0.000000\n0.014388\n\n\n7\n0.000000\n0.014388\n0.014388\n\n\n10\n0.000000\n0.007194\n0.007194\n\n\nAll\n0.330935\n0.669065\n1.000000\n\n\n\n\n\n\n\n\npd.crosstab(si_subset['Number of Injuries'], si_subset['y_pred'], normalize='index')\n\n\n\n\n\n\n\ny_pred\n0\n1\n\n\nNumber of Injuries\n\n\n\n\n\n\n0\n0.272727\n0.727273\n\n\n1\n0.268293\n0.731707\n\n\n2\n0.357143\n0.642857\n\n\n3\n0.333333\n0.666667\n\n\n4\n0.454545\n0.545455\n\n\n5\n0.666667\n0.333333\n\n\n6\n1.000000\n0.000000\n\n\n7\n0.000000\n1.000000\n\n\n10\n0.000000\n1.000000\n\n\n\n\n\n\n\nThis data indicates that if an athlete has 4 or fewer injuries, they are much more likely to be doing a prevention measure than not. If an athlete has 5 or 6 injuries, it is more likely than they are not doing any prevention measures. Athletes with 7 or 10 injuries can be considered outliers.\n\n\nVisualizing Clusters\n\nfrom sklearn.manifold import TSNE\n\ndef plot_top_words(model, feature_names, n_top_words, title):\n    fig, axes = plt.subplots(2, 3, figsize=(30, 15)) #, sharex=True)\n    axes = axes.flatten()\n    for topic_idx, topic in enumerate(model.components_):\n        top_features_ind = topic.argsort()[-n_top_words:]\n        top_features = feature_names[top_features_ind]\n        weights = topic[top_features_ind]\n\n        ax = axes[topic_idx]\n        ax.barh(top_features, weights, height=0.7)\n        ax.set_title(f\"Topic {topic_idx + 1}\", fontdict={\"fontsize\": 30})\n        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n        for i in \"top right left\".split():\n            ax.spines[i].set_visible(False)\n        fig.suptitle(title, fontsize=40)\n\n    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n    plt.show()\n\n\nn_top_words = 20\nnum_topics = 6\n\n\nfrom sklearn.decomposition import LatentDirichletAllocation, NMF\nnmf = NMF(\n    n_components=num_topics,\n    random_state=5000,\n    beta_loss=\"frobenius\",\n)\nnmf.fit(X)\n\nNMF(n_components=6, random_state=5000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.NMFNMF(n_components=6, random_state=5000)\n\n\n\nplot_top_words(nmf, feature_names, n_top_words, \"Topics in NMF model\")\n\n\n\n\nThis set of graphs groups together language that tends to be correlated. For example, topic 3 is likely to be grouped around words that relate to bracing as it includes “brace”, “movement”, “compression”, “restrict”, etc.\n\nfrom sklearn.manifold import TSNE\n\nnmf_ng = NMF(\n    n_components=num_topics,\n    random_state=5000,\n    beta_loss=\"frobenius\",\n    max_iter=1000\n)\nnmf_ng.fit(X_ng)\n\nnmf_ng_components = nmf_ng.components_\nnmf_ng_components.shape\n\n(6, 51)\n\n\n\nW = nmf_ng.transform(X_ng)\nW.shape\n\n(139, 6)\n\n\n\nW[:5,:]\n\narray([[5.95204182e-10, 5.90725194e-01, 0.00000000e+00, 0.00000000e+00,\n        0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n        0.00000000e+00, 0.00000000e+00],\n       [4.34564308e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n        7.35380448e-01, 0.00000000e+00],\n       [5.95204182e-10, 5.90725194e-01, 0.00000000e+00, 0.00000000e+00,\n        0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 2.12556806e-19, 4.61762617e-01,\n        0.00000000e+00, 0.00000000e+00]])\n\n\n\nW_df = pd.DataFrame(W)\nW_df[\"cluster\"] = W_df.idxmax(axis=1)\n\n\nW_df[\"cluster\"].value_counts\n\n&lt;bound method IndexOpsMixin.value_counts of 0      1\n1      0\n2      4\n3      1\n4      3\n      ..\n134    0\n135    2\n136    1\n137    4\n138    0\nName: cluster, Length: 139, dtype: int64&gt;\n\n\n\nW_df.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\ncluster\n\n\n\n\n0\n5.952042e-10\n0.590725\n0.000000e+00\n0.000000\n0.00000\n0.0\n1\n\n\n1\n0.000000e+00\n0.000000\n0.000000e+00\n0.000000\n0.00000\n0.0\n0\n\n\n2\n4.345643e-04\n0.000000\n0.000000e+00\n0.000000\n0.73538\n0.0\n4\n\n\n3\n5.952042e-10\n0.590725\n0.000000e+00\n0.000000\n0.00000\n0.0\n1\n\n\n4\n0.000000e+00\n0.000000\n2.125568e-19\n0.461763\n0.00000\n0.0\n3\n\n\n\n\n\n\n\n\ntsne_model = TSNE(\n    init='random',\n    random_state=5000\n)\ntsne_embedding = tsne_model.fit_transform(W)\ntsne_df = pd.DataFrame(tsne_embedding, columns=['x','y'])\ntsne_df.shape\n\n(139, 2)\n\n\n\ntsne_df.head()\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n0.755610\n-11.911394\n\n\n1\n-1.636704\n-0.801197\n\n\n2\n5.845468\n3.555564\n\n\n3\n0.525336\n-11.894978\n\n\n4\n3.503934\n-3.436377\n\n\n\n\n\n\n\n\ntsne_df['cluster'] = W_df['cluster']\ntsne_df['Definitions'] = si_subset[\"Prevention Measures Definitions\"]\n\n\ntsne_df.head()\n\n\n\n\n\n\n\n\nx\ny\ncluster\nDefinitions\n\n\n\n\n0\n0.755610\n-11.911394\n1\nAthletes can help protect themselves by prepar...\n\n\n1\n-1.636704\n-0.801197\n0\nAthletes can help protect themselves by prepar...\n\n\n2\n5.845468\n3.555564\n4\nAthletes can help protect themselves by prepar...\n\n\n3\n0.525336\n-11.894978\n1\nAthletes can help protect themselves by prepar...\n\n\n4\n3.503934\n-3.436377\n3\nAthletes can help protect themselves by prepar...\n\n\n\n\n\n\n\n\ntsne_sample_size = 100\ntsne_sample_df = tsne_df.sample(tsne_sample_size, random_state=5000)\n\n\nsns.scatterplot(tsne_sample_df, x='x', y='y', hue='cluster')\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n\n\n&lt;Axes: xlabel='x', ylabel='y'&gt;\n\n\n\n\n\nThis is a general graph of the clusters that are present within the data. These clusters represent different prevention measures that are used. However, this graph isn’t very information on what each cluster represents.\n\nimport textwrap\nmywrap = lambda x: textwrap.wrap(x, width=60)\n\n\ntsne_sample_df.shape\n\n(100, 4)\n\n\n\ntsne_sample_df['Definitions'] = tsne_sample_df['Definitions'].apply(lambda x: x if type(x) == str else '')\ntsne_sample_df['Definitions_wrap'] = tsne_sample_df['Definitions'].apply(lambda x: '&lt;br&gt;'.join(mywrap(x)))\n\n\nimport plotly\nimport plotly.express as px\nreview_fig = px.scatter(tsne_sample_df, x='x', y='y', color='cluster',\n                        hover_data=['Definitions_wrap'], template='simple_white')\nreview_fig.update(layout_coloraxis_showscale=False)\nreview_fig.update_traces(marker=dict(size=12),selector=dict(mode='markers'))\n\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\n\nplotly.offline.plot(review_fig, filename='interactive_plot.html')\n\nfrom IPython.display import IFrame\nIFrame(src='interactive_plot.html', width=800, height=600)\n\n\n        \n        \n\n\nThis clustering graph is much more specific as its interactive nature allows us to see what factors are grouping each cluster together. For example, the yellow cluster is likely clustered around orthotics as both of these points include orthotics as they main point of commonality. The green cluster is likely using taping as its point of commonality. It is important to note the prevention measures such as stretching and warming up are quite universal, so they don’t hold as much influence in terms of determining their own cluster."
  },
  {
    "objectID": "tabs/conclusion/conclusion.html",
    "href": "tabs/conclusion/conclusion.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "1+1"
  },
  {
    "objectID": "tabs/data/data.html",
    "href": "tabs/data/data.html",
    "title": "Basketball Injury Data from 2010-2018",
    "section": "",
    "text": "This data comes from this website\n\nimport requests\nimport pandas as pd\nimport requests\n\ncsv_url = \"https://raw.githubusercontent.com/anly501/dsan-5000-project-rennyd123/main/dsan-website/5000-website/data/raw/injuries_2010-2020.csv\"\n\n\nbball_injury_data = pd.read_csv(csv_url)\nprint(bball_injury_data.head())\n\n         Date     Team Acquired   Relinquished  \\\n0  2010-10-03    Bulls      NaN  Carlos Boozer   \n1  2010-10-06  Pistons      NaN  Jonas Jerebko   \n2  2010-10-06  Pistons      NaN  Terrico White   \n3  2010-10-08  Blazers      NaN     Jeff Ayres   \n4  2010-10-08     Nets      NaN    Troy Murphy   \n\n                                               Notes  \n0  fractured bone in right pinky finger (out inde...  \n1      torn right Achilles tendon (out indefinitely)  \n2  broken fifth metatarsal in right foot (out ind...  \n3          torn ACL in right knee (out indefinitely)  \n4             strained lower back (out indefinitely)"
  },
  {
    "objectID": "tabs/decision_tree/classification.html#class-distribution",
    "href": "tabs/decision_tree/classification.html#class-distribution",
    "title": "Methods",
    "section": "Class distribution",
    "text": "Class distribution\n\n\nCode\nimport pandas as pd\nimport numpy as np \nimport sklearn.tree\nimport sklearn.model_selection\nfile_path = \"../../../../data/01-modified-data/injury_prevention_data_soccer.csv\"\nsoccer_injuries = pd.read_csv(file_path)\n\nlabel_dist = soccer_injuries[\"Number of Injuries\"].value_counts()\nlabel_dist = label_dist.sort_index()\nprint(\"DISTRIBUTION OF LABELS:\")\nprint(label_dist)\n\n\nDISTRIBUTION OF LABELS:\nNumber of Injuries\n0     22\n1     41\n2     42\n3     15\n4     11\n5      3\n6      2\n7      2\n10     1\nName: count, dtype: int64\n\n\n\n\nCode\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n\nsns.barplot(x= label_dist.index, y=label_dist.values, color=\"lavender\")\nplt.plot(label_dist.index, label_dist.values, color='purple', marker='o', linestyle='-')\nplt.ylabel(\"Count\")\nplt.title(\"Count per Number of Injuries\")\n\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n\n\nText(0.5, 1.0, 'Count per Number of Injuries')\n\n\n\n\n\n\n\nCode\nplt.pie(label_dist.values, labels=[f\"{count} injuries\" for count in label_dist.index], autopct='%.0f%%')\n\n\n([&lt;matplotlib.patches.Wedge at 0x175da9310&gt;,\n  &lt;matplotlib.patches.Wedge at 0x175dab310&gt;,\n  &lt;matplotlib.patches.Wedge at 0x175db8c50&gt;,\n  &lt;matplotlib.patches.Wedge at 0x175dba350&gt;,\n  &lt;matplotlib.patches.Wedge at 0x175dbb950&gt;,\n  &lt;matplotlib.patches.Wedge at 0x175dc90d0&gt;,\n  &lt;matplotlib.patches.Wedge at 0x175dca810&gt;,\n  &lt;matplotlib.patches.Wedge at 0x175dcbe90&gt;,\n  &lt;matplotlib.patches.Wedge at 0x175dbba50&gt;],\n [Text(0.966797659815723, 0.5246925623399303, '0 injuries'),\n  Text(-0.3775197345008592, 1.0331886807657162, '1 injuries'),\n  Text(-0.8720577330038374, -0.6704590295522971, '2 injuries'),\n  Text(0.40077272406551884, -1.0243931001549667, '3 injuries'),\n  Text(0.9014631001688704, -0.6303683677294802, '4 injuries'),\n  Text(1.0528595955870075, -0.3185697286001347, '5 injuries'),\n  Text(1.0820679519520409, -0.19781038233195886, '6 injuries'),\n  Text(1.0955078094302033, -0.09931082255946361, '7 injuries'),\n  Text(1.0997190565076482, -0.024859540501146027, '10 injuries')],\n [Text(0.5273441780813034, 0.2861959430945074, '16%'),\n  Text(-0.2059198551822868, 0.5635574622358451, '29%'),\n  Text(-0.47566785436572945, -0.36570492521034387, '30%'),\n  Text(0.21860330403573755, -0.5587598728118, '11%'),\n  Text(0.49170714554665657, -0.3438372914888074, '8%'),\n  Text(0.5742870521383676, -0.17376530650916436, '2%'),\n  Text(0.5902188828829313, -0.10789657218106845, '1%'),\n  Text(0.5975497142346563, -0.05416953957788923, '1%'),\n  Text(0.5998467580950808, -0.01355974936426147, '1%')])\n\n\n\n\n\nAs we can see based on the graphs above, the distribution of number of injuries is skewed to the right as it is much more common for an athlete to have 1 or 2 injuries than any other amount of injuries. This can have a few affects on my classification algorithm results.\n\nModel Bias\nBecause the 1 injury, 2 injuries, and 0 injuries categories make up 75% of the data, the model may do really well at correctly classifying points in those categories, but less well at correctly categorizing points in the 3, 4, 5, 6, 7, and 10 injuries categories.\nAccuracy Scores\nThe accuracy score of the model may not be particularly reliable. Because most of the data points belong to the 0, 1, or 2 injuries classes, the model may predict those classes for all data points which would make the accuracy high but the score wouldn’t mean much.\nFeature Importance\nAn imbalanced dataset may make the model focus more on features that help distinguish the majority classes, rather the features that help distinguish the minority classes. It is important to make sure that all features are being considered during the classification process."
  },
  {
    "objectID": "tabs/decision_tree/classification.html#baseline-model-for-comparison",
    "href": "tabs/decision_tree/classification.html#baseline-model-for-comparison",
    "title": "Methods",
    "section": "Baseline model for comparison",
    "text": "Baseline model for comparison\n\n\nCode\nsoccer_injuries.columns = soccer_injuries.columns.str.strip()\ncolumns_keep = ['Prevention Measure Stretching', 'Prevention Measure Warm Up',\n       'Prevention Measure Specific Strength Exercises',\n       'Prevention Measure Bracing', 'Prevention Measure Taping',\n       'Prevention Measure Shoe Insoles', 'Prevention Measure Face Masks',\n       'Prevention Measure Medical Corset', \"Number of Injuries\"]\ndf_features = soccer_injuries[columns_keep]\n\ncolumns_features = ['Prevention Measure Stretching', 'Prevention Measure Warm Up',\n       'Prevention Measure Specific Strength Exercises',\n       'Prevention Measure Bracing', 'Prevention Measure Taping',\n       'Prevention Measure Shoe Insoles', 'Prevention Measure Face Masks',\n       'Prevention Measure Medical Corset']\nfeatures = soccer_injuries[columns_features]\n\nfeatures.columns = features.columns.str.strip()\n\nimport numpy as np\nnum_obs = len(df_features)\nrng = np.random.default_rng(30)\nrandom_guesses = np.random.choice([i for i in range(8)] + [10], num_obs)\ndf_features[\"random_guesses\"] = random_guesses\ndf_features\n\n\n/var/folders/kw/p7j6fdpx3vvgm6c0krhwpvb80000gn/T/ipykernel_64435/2656229809.py:23: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_features[\"random_guesses\"] = random_guesses\n\n\n\n\n\n\n\n\n\nPrevention Measure Stretching\nPrevention Measure Warm Up\nPrevention Measure Specific Strength Exercises\nPrevention Measure Bracing\nPrevention Measure Taping\nPrevention Measure Shoe Insoles\nPrevention Measure Face Masks\nPrevention Measure Medical Corset\nNumber of Injuries\nrandom_guesses\n\n\n\n\n0\nyes\nno\nyes\nno\nno\nno\nno\nno\n6\n1\n\n\n1\nyes\nyes\nno\nno\nno\nno\nno\nno\n2\n5\n\n\n2\nyes\nno\nno\nno\nno\nyes\nno\nno\n7\n6\n\n\n3\nyes\nyes\nyes\nno\nno\nno\nno\nno\n1\n7\n\n\n4\nyes\nyes\nno\nno\nyes\nno\nno\nno\n2\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n134\nyes\nno\nno\nno\nno\nno\nno\nno\n0\n6\n\n\n135\nyes\nno\nno\nyes\nno\nno\nno\nno\n2\n3\n\n\n136\nyes\nyes\nyes\nno\nyes\nno\nno\nyes\n2\n6\n\n\n137\nyes\nno\nno\nno\nyes\nyes\nno\nno\n1\n2\n\n\n138\nyes\nyes\nno\nno\nno\nno\nno\nno\n0\n2\n\n\n\n\n139 rows × 10 columns\n\n\n\n\n\nCode\nfrom sklearn.metrics import f1_score\n\nf1 = f1_score(df_features[\"Number of Injuries\"], df_features[\"random_guesses\"], average=\"macro\")\nprint(f\"f1 score: {f1}\")\nprint(\"-------------\")\n#df_features[\"random_guesses_correct\"] = df_features[\"Number of Injuries\"] == df_features[\"random_guesses\"]\n#correct_random_guess_rows = df_features[df_features[\"random_guesses_correct\"] == True]\nrandom_guess_accuracy = len(correct_random_guess_rows) / len(df_features)\nprint(f\"Random guess accuracy: {random_guess_accuracy}\")\nprint(\"-------------\")\n\n\nf1 score: 0.1098411921329338\n-------------\nRandom guess accuracy: 0.11510791366906475\n-------------\n\n\nThe basic, unweighted random classifier has an accuracy of 11.51% which is not high and that is to be expected. Because there is a right skew in the data and points in the 0, 1, and 2 injuries classes make up 75% of the data, a classifier that has a 12.5% chance of assigning each number will not be able to accuracy represent this skew. Let’s try again with a random classifier that is weighted the same as the distribution of the labels.\n\n\nCode\nimport numpy as np\nnum_obs = len(df_features)\nrng = np.random.default_rng(30)\nchoices = [i for i in range(8)] + [10]\nweights = [0.16, 0.29, 0.3, 0.11, 0.08, 0.02, 0.01, 0.01, 0.01]\nweights /= np.sum(weights)\nweights = weights[:len(choices)]\nweights_dict = dict(zip(choices, weights))\nprint(weights_dict)\nrandom_guesses_weighted = rng.choice(choices, num_obs, p=weights)\ndf_features[\"random_guesses_weighted\"] = random_guesses_weighted\ndf_features\n\n\n{0: 0.16161616161616163, 1: 0.29292929292929293, 2: 0.30303030303030304, 3: 0.11111111111111112, 4: 0.08080808080808081, 5: 0.020202020202020204, 6: 0.010101010101010102, 7: 0.010101010101010102, 10: 0.010101010101010102}\n\n\n/var/folders/kw/p7j6fdpx3vvgm6c0krhwpvb80000gn/T/ipykernel_64435/1441920233.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_features[\"random_guesses_weighted\"] = random_guesses_weighted\n\n\n\n\n\n\n\n\n\nPrevention Measure Stretching\nPrevention Measure Warm Up\nPrevention Measure Specific Strength Exercises\nPrevention Measure Bracing\nPrevention Measure Taping\nPrevention Measure Shoe Insoles\nPrevention Measure Face Masks\nPrevention Measure Medical Corset\nNumber of Injuries\nrandom_guesses\nrandom_guesses_weighted\n\n\n\n\n0\nyes\nno\nyes\nno\nno\nno\nno\nno\n6\n0\n1\n\n\n1\nyes\nyes\nno\nno\nno\nno\nno\nno\n2\n10\n1\n\n\n2\nyes\nno\nno\nno\nno\nyes\nno\nno\n7\n7\n0\n\n\n3\nyes\nyes\nyes\nno\nno\nno\nno\nno\n1\n5\n2\n\n\n4\nyes\nyes\nno\nno\nyes\nno\nno\nno\n2\n4\n3\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n134\nyes\nno\nno\nno\nno\nno\nno\nno\n0\n5\n0\n\n\n135\nyes\nno\nno\nyes\nno\nno\nno\nno\n2\n10\n2\n\n\n136\nyes\nyes\nyes\nno\nyes\nno\nno\nyes\n2\n7\n1\n\n\n137\nyes\nno\nno\nno\nyes\nyes\nno\nno\n1\n2\n1\n\n\n138\nyes\nyes\nno\nno\nno\nno\nno\nno\n0\n1\n2\n\n\n\n\n139 rows × 11 columns\n\n\n\n\n\nCode\nf1 = f1_score(df_features[\"Number of Injuries\"], df_features[\"random_guesses_weighted\"], average=\"macro\")\nprint(f\"f1 score: {f1}\")\nprint(\"-------------\")\ndf_features[\"random_guesses_weighted_correct\"] = df_features[\"Number of Injuries\"] == df_features[\"random_guesses_weighted\"]\ncorrect_random_guess_weighted_rows = df_features[df_features[\"random_guesses_weighted_correct\"] == True]\nrandom_guess_weighted_accuracy = len(correct_random_guess_weighted_rows) / len(df_features)\nprint(f\"Weighted random guess accuracy: {random_guess_weighted_accuracy}\")\nprint(\"-------------\")\n\n\nf1 score: 0.11044140294140295\n-------------\nWeighted random guess accuracy: 0.2517985611510791\n-------------\n\n\n/var/folders/kw/p7j6fdpx3vvgm6c0krhwpvb80000gn/T/ipykernel_64435/3744477820.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_features[\"random_guesses_weighted_correct\"] = df_features[\"Number of Injuries\"] == df_features[\"random_guesses_weighted\"]\n\n\nWhen we weight the random choices to match the distribution of our data, it is significantly more accurate with an accuracy rate of 25%. That is a pretty good accuracy rate for random guessing. It is clear that weighting our random guesses to match the distribution helps."
  },
  {
    "objectID": "tabs/decision_tree/classification.html#feature-selection-optional",
    "href": "tabs/decision_tree/classification.html#feature-selection-optional",
    "title": "Methods",
    "section": "Feature selection (optional)",
    "text": "Feature selection (optional)\nYou can repeat this with your tree-algorithm, but if you already have an optimal feature set from a previous assignment, then you can stick with that one. There are many ways to “pre-process” data to make it more suitable for analysis. Make sure to document what you are doing and why you did what you did. Model tuning: Carry out, document, and visualize a hyper-parameter tuning protocol. Attempt to find the set of hyper parameters that result int the optimal model (i.e. lowest validation error without overfitting, validation and training error should be similar)\n\n\nCode\nparam_grid = {\n    'max_depth': [2,3,4,5,6,7,8],\n    'min_samples_leaf': [1, 2, 4, 8, 16],\n    'min_impurity_decrease': [0.01, 0.02, 0.03, 0.04, 0.05]\n}\n\ndtc = sklearn.tree.DecisionTreeClassifier(random_state=5000)\n\ngrid_search = GridSearchCV(dtc, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n\ngrid_search.fit(X_train_num, y_train)\n\nbest_params = grid_search.best_params_\nprint(\"Best Hyperparameters:\", best_params)\n\nresults = grid_search.cv_results_\nparam_names = list(param_grid.keys())\nparam_values = [param_grid[name] for name in param_names]\n\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n  warnings.warn(\n\n\nBest Hyperparameters: {'max_depth': 5, 'min_impurity_decrease': 0.01, 'min_samples_leaf': 1}\n\n\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\n\nthreshold = 0.05\nfeature_importances = clf.feature_importances_\nselected_features = X_train.columns[feature_importances &gt; threshold]\n\nX_train_selected = X_train[selected_features]\nX_test_selected = X_test[selected_features]\n\nprint(\"Selected Features:\", selected_features)\nprint(\"Feature Importances:\", feature_importances)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_selected)\nX_test_scaled = scaler.transform(X_test_selected)\n\n\nSelected Features: Index(['Prevention Measure Stretching', 'Prevention Measure Warm Up',\n       'Prevention Measure Specific Strength Exercises',\n       'Prevention Measure Bracing', 'Prevention Measure Taping',\n       'Prevention Measure Shoe Insoles'],\n      dtype='object')\nFeature Importances: [0.14759918 0.22554741 0.23204718 0.10404068 0.19730943 0.08347534\n 0.         0.00998078]\n\n\n\n\nCode\nimport sklearn.ensemble\n\ndef accuracy_over_range(est_range, depth_range, long_format=True):\n  result_data = []\n  for cur_estimators in est_range:\n    for cur_depth in depth_range:\n      cur_rfc = sklearn.ensemble.RandomForestClassifier(\n        n_estimators = cur_estimators,\n        max_depth = cur_depth,\n        random_state = 5000\n      )\n      cur_rfc.fit(X_train_num, y_train)\n      y_train_pred_rfc = cur_rfc.predict(X_train_num)\n      y_test_pred_rfc = cur_rfc.predict(X_test_num)\n      y_train_correct = y_train_pred_rfc == Xy_train[\"Number of Injuries\"]\n      y_test_correct = y_test_pred_rfc == Xy_test[\"Number of Injuries\"]\n      train_accuracy = sum(y_train_correct) / len(y_train_correct)\n      test_accuracy = sum(y_test_correct) / len(y_test_correct)\n      cur_result = {\n          'n_estimators': cur_estimators,\n          'max_depth': cur_depth,\n          'train_accuracy': train_accuracy,\n          'test_accuracy': test_accuracy\n      }\n      result_data.append(cur_result)\n  rfc_result_df = pd.DataFrame(result_data)\n  if long_format:\n    rfc_long_df = pd.melt(rfc_result_df, id_vars=['n_estimators','max_depth'])\n    return rfc_long_df\n  return rfc_result_df\n\nestimators_range = np.arange(20, 220, 20)\ndepth_range = [2,3,4,5,6,7]\nrfc_result_df = accuracy_over_range(estimators_range, depth_range)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_hyperparam_grid(result_df, wrap=3):\n  g = sns.FacetGrid(result_df, col=\"max_depth\", col_wrap=wrap)\n  g.map_dataframe(sns.lineplot, x=\"n_estimators\", y=\"value\", hue='variable', marker='o')\n  plt.show()\nplot_hyperparam_grid(rfc_result_df)\n\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/axisgrid.py:118: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)\n\n\n\n\n\nWhen we look at the training and test accuracy for each of our different values for maximum death, we can see that the training accuracy and test accuracy are the highest for max_depth = 5. Therefore we choose that as our value. The other best parameters are calculated as well, with minimum purity decreasing being 0.1 and minimum sample leafs being 1.\nWhen we perform feature selection we see that the important features are Prevention Measure Stretching, Prevention Measure Warm Up, Prevention Measure Specific Strength Exercises, Prevention Measure Bracing, Prevention Measure Taping, and Prevention Measure Shoe Insoles. Two features were deamed not important, which were medical corsets and face masks, which makes sense as they had very small sample sizes.\n\n\nCode\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier, export_text, export_graphviz\nimport sklearn.tree\nimport sklearn.model_selection\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image\nfrom sklearn.tree import plot_tree\nimport graphviz\nimport plotly.graph_objects as go\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,  confusion_matrix\n\ndtc = sklearn.tree.DecisionTreeClassifier(\n    max_depth = 6,\n    random_state=5000,\n    min_samples_leaf=1,\n    min_impurity_decrease=0.01\n)\n\ndtc.fit(X_train, y_train)\n\nimportant_features = ['Prevention Measure Stretching', 'Prevention Measure Warm Up',\n       'Prevention Measure Specific Strength Exercises',\n       'Prevention Measure Bracing', 'Prevention Measure Taping',\n       'Prevention Measure Shoe Insoles']\n\nX_df = soccer_injuries[important_features].copy()\nX_df = X_df.replace({\"yes\": 1, \"no\": 0})\ny = soccer_injuries[\"Number of Injuries\"].copy()\n\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n    X_df, y, test_size=0.2, random_state=5000\n)\n\ny_train_pred = dtc.predict(X_train)\ny_test_pred = dtc.predict(X_test)\n\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\n\ntrain_precision = precision_score(y_train, y_train_pred, average='macro')\ntest_precision = precision_score(y_test, y_test_pred, average='macro')\n\ntrain_recall = recall_score(y_train, y_train_pred, average='macro')\ntest_recall = recall_score(y_test, y_test_pred, average='macro')\n\ntrain_f1 = f1_score(y_train, y_train_pred, average='macro')\ntest_f1 = f1_score(y_test, y_test_pred, average='macro')\n\nprint(\"Training Accuracy:\", train_accuracy)\nprint(\"Testing Accuracy:\", test_accuracy)\n\nprint(\"Training Precision:\", train_precision)\nprint(\"Testing Precision:\", test_precision)\n\nprint(\"Training Recall:\", train_recall)\nprint(\"Testing Recall:\", test_recall)\n\nprint(\"Training F1 Score:\", train_f1)\nprint(\"Testing F1 Score:\", test_f1)\n\nconf_matrix_train = confusion_matrix(y_train, y_train_pred)\nconf_matrix_test = confusion_matrix(y_test, y_test_pred)\n\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nsns.heatmap(conf_matrix_train, annot=True, fmt='d', cmap='Blues', cbar=False, vmin=1, vmax=10)\nplt.title('Confusion Matrix - Training Set')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\n\nplt.subplot(1, 2, 2)\nsns.heatmap(conf_matrix_test, annot=True, fmt='d', cmap='Blues', cbar=False, vmin=1, vmax=10)\nplt.title('Confusion Matrix - Testing Set')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\n\nplt.tight_layout()\nplt.show()\n\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning:\n\nPrecision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning:\n\nPrecision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n\n\n\nTraining Accuracy: 0.36936936936936937\nTesting Accuracy: 0.17857142857142858\nTraining Precision: 0.14187276977974653\nTesting Precision: 0.05952380952380952\nTraining Recall: 0.1776135741652983\nTesting Recall: 0.15306122448979592\nTraining F1 Score: 0.13615600393520802\nTesting F1 Score: 0.08055853920515575\n\n\n\n\n\n\n\nCode\nXy_train = pd.concat([X_train, y_train], axis=1)\nXy_train['prediction'] = y_train_pred\nXy_test = pd.concat([X_test, y_test], axis=1)\nXy_test['prediction'] = y_test_pred\n\n\nclf = DecisionTreeClassifier(max_depth=5, random_state=5000)\nclf.fit(X_train, y_train)\n\ndot_data = export_graphviz(\n    clf, out_file=None,\n    feature_names=important_features,\n    class_names=[str(i) for i in clf.classes_],\n    filled=True, rounded=True\n)\n\ngraph = graphviz.Source(dot_data, format=\"png\")\ngraph.render(\"decision_tree\", cleanup=True)\ngraph.view(\"decision_tree\")"
  },
  {
    "objectID": "tabs/decision_tree/classification.html#final-results",
    "href": "tabs/decision_tree/classification.html#final-results",
    "title": "Methods",
    "section": "Final results",
    "text": "Final results\nThe final results of my decision tree classifier were not very successful. The accuracy of predicting the test data was merely 17.86%, which was less than the accuracy of the weighted random guessing method. The recall, or the number of correctly classified positives out of the total number of correct classifications (both positive and negative), was only 15% on the testing data, and the precision, or the number of correctly classified positives out of the total number of positives (both true and false positives), was only 5.92% on the testing data. These values were higher on the training data, but not by a large amount. In the confusion matricies, we can see that the model did a fair job at predicting points in the 0, 1, and 2 injuries categories but was unable to predict any points correctly for athletes with any amount of injuries over 3. However, that success was not reflected in the test data as there are very few correct predictions.\nThe fit isn’t particularly good, but I think that may be due to the heavily right skewed distribution of the label. With so much of the data falling within the 0, 1, and 2 injuries categories, it makes sense that observations will be incorrectly classified into these categories. Perhaps in the future we can look to taking a smaller subset of the data and focus only on the most used common three or four features to have a more accurate classification."
  },
  {
    "objectID": "tabs/decision_tree/classification.html#conclusions",
    "href": "tabs/decision_tree/classification.html#conclusions",
    "title": "Methods",
    "section": "Conclusions:",
    "text": "Conclusions:\nIn my attempt to predict the number of injuries an athlete may have based on what injury prevention methods they use using a decision tree classifier, the outcome unfortunately was not as successful as I originally hoped it would be. The model’s accuracy in predicting test data was a mere 17.86%, even lower than a method that randomly guesses based on the distribution of the data. Because the majority of the athletes in the data set have only had 0, 1, or 2 injuries in their careers so far (this accounts for 75% of the athletes), the classifier may have had a hard time correctly predicting athletes that have had more than 2 injuries. Additionally, because there are so many possible injury prevention methods that they could take, there are many confounding variables and possible combinations of injury prevention methods that may have made accurate classification more difficult. The overall performance leaves much room for improvement.\nFor future directions, I think focusing in on the most common injury prevention methods, i.e. stretching, warming up, and strengthening exercises, could lead to more successful classification. Additionally, we could likely also ignore data points where athletes have had 5, 6, 7, or 10 injuries because there are only a few of each of these points. This smaller and more refined data set could help find more accurate results and provide meaningful information about the correlation between injury prevention methods and number of injuries, which would guide athletes in how to properly protect themselves."
  },
  {
    "objectID": "tabs/decision_tree/classification.html#feature-selection",
    "href": "tabs/decision_tree/classification.html#feature-selection",
    "title": "Methods",
    "section": "Feature selection",
    "text": "Feature selection\n\n\nCode\nparam_grid = {\n    'max_depth': [2,3,4,5,6,7,8],\n    'min_samples_leaf': [1, 2, 4, 8, 16],\n    'min_impurity_decrease': [0.01, 0.02, 0.03, 0.04, 0.05]\n}\n\ndtc = sklearn.tree.DecisionTreeClassifier(random_state=5000)\n\ngrid_search = GridSearchCV(dtc, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n\ngrid_search.fit(X_train_num, y_train)\n\nbest_params = grid_search.best_params_\nprint(\"Best Hyperparameters:\", best_params)\n\nresults = grid_search.cv_results_\nparam_names = list(param_grid.keys())\nparam_values = [param_grid[name] for name in param_names]\n\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n  warnings.warn(\n\n\nBest Hyperparameters: {'max_depth': 5, 'min_impurity_decrease': 0.01, 'min_samples_leaf': 1}\n\n\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\n\nthreshold = 0.05\nfeature_importances = clf.feature_importances_\nselected_features = X_train.columns[feature_importances &gt; threshold]\n\nX_train_selected = X_train[selected_features]\nX_test_selected = X_test[selected_features]\n\nprint(\"Selected Features:\", selected_features)\nprint(\"Feature Importances:\", feature_importances)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_selected)\nX_test_scaled = scaler.transform(X_test_selected)\n\n\nSelected Features: Index(['Prevention Measure Stretching', 'Prevention Measure Warm Up',\n       'Prevention Measure Specific Strength Exercises',\n       'Prevention Measure Bracing', 'Prevention Measure Taping',\n       'Prevention Measure Shoe Insoles'],\n      dtype='object')\nFeature Importances: [0.14759918 0.22554741 0.23204718 0.10404068 0.19730943 0.08347534\n 0.         0.00998078]\n\n\n\n\nCode\nimport sklearn.ensemble\n\ndef accuracy_over_range(est_range, depth_range, long_format=True):\n  result_data = []\n  for cur_estimators in est_range:\n    for cur_depth in depth_range:\n      cur_rfc = sklearn.ensemble.RandomForestClassifier(\n        n_estimators = cur_estimators,\n        max_depth = cur_depth,\n        random_state = 5000\n      )\n      cur_rfc.fit(X_train_num, y_train)\n      y_train_pred_rfc = cur_rfc.predict(X_train_num)\n      y_test_pred_rfc = cur_rfc.predict(X_test_num)\n      y_train_correct = y_train_pred_rfc == Xy_train[\"Number of Injuries\"]\n      y_test_correct = y_test_pred_rfc == Xy_test[\"Number of Injuries\"]\n      train_accuracy = sum(y_train_correct) / len(y_train_correct)\n      test_accuracy = sum(y_test_correct) / len(y_test_correct)\n      cur_result = {\n          'n_estimators': cur_estimators,\n          'max_depth': cur_depth,\n          'train_accuracy': train_accuracy,\n          'test_accuracy': test_accuracy\n      }\n      result_data.append(cur_result)\n  rfc_result_df = pd.DataFrame(result_data)\n  if long_format:\n    rfc_long_df = pd.melt(rfc_result_df, id_vars=['n_estimators','max_depth'])\n    return rfc_long_df\n  return rfc_result_df\n\nestimators_range = np.arange(20, 220, 20)\ndepth_range = [2,3,4,5,6,7]\nrfc_result_df = accuracy_over_range(estimators_range, depth_range)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_hyperparam_grid(result_df, wrap=3):\n  g = sns.FacetGrid(result_df, col=\"max_depth\", col_wrap=wrap)\n  g.map_dataframe(sns.lineplot, x=\"n_estimators\", y=\"value\", hue='variable', marker='o')\n  plt.show()\nplot_hyperparam_grid(rfc_result_df)\n\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/axisgrid.py:118: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)\n\n\n\n\n\nWhen we look at the training and test accuracy for each of our different values for maximum death, we can see that the training accuracy and test accuracy are the highest for max_depth = 5. Therefore we choose that as our value. The other best parameters are calculated as well, with minimum purity decreasing being 0.1 and minimum sample leafs being 1.\nWhen we perform feature selection we see that the important features are Prevention Measure Stretching, Prevention Measure Warm Up, Prevention Measure Specific Strength Exercises, Prevention Measure Bracing, Prevention Measure Taping, and Prevention Measure Shoe Insoles. Two features were deamed not important, which were medical corsets and face masks, which makes sense as they had very small sample sizes.\n\n\nCode\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier, export_text, export_graphviz\nimport sklearn.tree\nimport sklearn.model_selection\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image\nfrom sklearn.tree import plot_tree\nimport graphviz\nimport plotly.graph_objects as go\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,  confusion_matrix\n\ndtc = sklearn.tree.DecisionTreeClassifier(\n    max_depth = 6,\n    random_state=5000,\n    min_samples_leaf=1,\n    min_impurity_decrease=0.01\n)\n\ndtc.fit(X_train, y_train)\n\nimportant_features = ['Prevention Measure Stretching', 'Prevention Measure Warm Up',\n       'Prevention Measure Specific Strength Exercises',\n       'Prevention Measure Bracing', 'Prevention Measure Taping',\n       'Prevention Measure Shoe Insoles']\n\nX_df = soccer_injuries[important_features].copy()\nX_df = X_df.replace({\"yes\": 1, \"no\": 0})\ny = soccer_injuries[\"Number of Injuries\"].copy()\n\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n    X_df, y, test_size=0.2, random_state=5000\n)\n\ny_train_pred = dtc.predict(X_train)\ny_test_pred = dtc.predict(X_test)\n\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\n\ntrain_precision = precision_score(y_train, y_train_pred, average='macro')\ntest_precision = precision_score(y_test, y_test_pred, average='macro')\n\ntrain_recall = recall_score(y_train, y_train_pred, average='macro')\ntest_recall = recall_score(y_test, y_test_pred, average='macro')\n\ntrain_f1 = f1_score(y_train, y_train_pred, average='macro')\ntest_f1 = f1_score(y_test, y_test_pred, average='macro')\n\nprint(\"Training Accuracy:\", train_accuracy)\nprint(\"Testing Accuracy:\", test_accuracy)\n\nprint(\"Training Precision:\", train_precision)\nprint(\"Testing Precision:\", test_precision)\n\nprint(\"Training Recall:\", train_recall)\nprint(\"Testing Recall:\", test_recall)\n\nprint(\"Training F1 Score:\", train_f1)\nprint(\"Testing F1 Score:\", test_f1)\n\nconf_matrix_train = confusion_matrix(y_train, y_train_pred)\nconf_matrix_test = confusion_matrix(y_test, y_test_pred)\n\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nsns.heatmap(conf_matrix_train, annot=True, fmt='d', cmap='Blues', cbar=False, vmin=1, vmax=10)\nplt.title('Confusion Matrix - Training Set')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\n\nplt.subplot(1, 2, 2)\nsns.heatmap(conf_matrix_test, annot=True, fmt='d', cmap='Blues', cbar=False, vmin=1, vmax=10)\nplt.title('Confusion Matrix - Testing Set')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\n\nplt.tight_layout()\nplt.show()\n\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning:\n\nPrecision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning:\n\nPrecision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n\n\n\nTraining Accuracy: 0.36936936936936937\nTesting Accuracy: 0.17857142857142858\nTraining Precision: 0.14187276977974653\nTesting Precision: 0.05952380952380952\nTraining Recall: 0.1776135741652983\nTesting Recall: 0.15306122448979592\nTraining F1 Score: 0.13615600393520802\nTesting F1 Score: 0.08055853920515575\n\n\n\n\n\n\n\nCode\nXy_train = pd.concat([X_train, y_train], axis=1)\nXy_train['prediction'] = y_train_pred\nXy_test = pd.concat([X_test, y_test], axis=1)\nXy_test['prediction'] = y_test_pred\n\n\nclf = DecisionTreeClassifier(max_depth=5, random_state=5000)\nclf.fit(X_train, y_train)\n\ndot_data = export_graphviz(\n    clf, out_file=None,\n    feature_names=important_features,\n    class_names=[str(i) for i in clf.classes_],\n    filled=True, rounded=True\n)\n\ngraph = graphviz.Source(dot_data, format=\"png\")\ngraph.render(\"decision_tree\", cleanup=True)\ngraph.view(\"decision_tree\")"
  }
]