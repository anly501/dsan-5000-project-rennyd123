[
  {
    "objectID": "tabs/conclusion/conclusion.html",
    "href": "tabs/conclusion/conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "Summary\nWhen I started on this project, my goal was to understand more about how athletes sustain injuries and what we can do to keep athletes safe. Taking a data approach to a topic that is heavily influenced by a person’s nature can make analysis difficult. However, through the different machine learning methods I tried out, I was able to have some success.\nI first started this project by gathering and cleaning data. I found that it was hard to find data sets only about injury prevention methods so I felt very lucky to find one within the injury prevention methods data set. I decided to collect the other data sets to paint the bigger picture of injuries in athletes before diving deep into how we can prevent them. I used a mix of record data and text data, but due to the numerical nature of sports data, record data was more common. I found six record data sets and one text data set. Through these data sets, I was able to explore topics such as the number of injuries per activity by age group, National Basketball Association (NBA) data, National Football League (NFL) concussion data, as well as data on what the conditions were when the NFL injuries were experienced. My text data focused on the news, which was a little less relevant due to the nature of the news cycle and recent soccer-related injuries.\nI then went on to explore many different machine learning methods, including decision trees, Naive Bayes, clustering, and dimensionality reduction. I found that decision trees weren’t the most successful method for predicting how many injuries each athlete had. The accuracy rate was a mere 17.86%, which was just above a random classifier, and just below a random classifier that had been weighted according to the distribution of the number of injuries each athlete had. Naive Bayes, on the other hand, was a relatively successful method for predicting how many injuries an athlete has had. When combined with feature selection and using SelectPercentile, the accuracy rate was 25%! The model was very good at predicting the number of injuries for athletes who had 0, 1, or 2 previous injuries, but less so if the athlete had any more. We were also able to discover that stretching, warming up, and specific strength exercises were the most successful at preventing injuries.\nIn the clustering tab, I learned that agglomerative clustering was the most successful of the clustering methods as it was able to make distinct clusters. K-Means clustering and DBSCAN were successful but didn’t make as well-defined clusters so they didn’t live up to the standards that agglomerative clustering set. The optimal number of clusters was found to be 5 which makes sense since 5 prevention methods were commonly used: stretching, warming up, taping, strength exercises, and shoe insoles.\nFinally, the dimensionality reduction process helped me determine that the optimal number of principal components to retain is 5. PCA helped to reveal a few outliers that were quite evident from both the 2D and 3D plots. PCA’s visualizations were quite accurate and were conducive to an interactive nature.\n\n\nReflection\nWhen it comes to the research questions that I posed at the beginning of the project, I think I can answer quite a few of them. The questions I posed were:\n\nWhat information about injuries can be discovered with machine learning?\nHow can we use machine learning and artificial intelligence to predict and prevent sports injuries?\nIs there a difference in the data between predicting injuries in male athletes versus predicting injuries in female athletes?\nHow can we make the results of many of these studies more useful to clinicians?\nHow can we more efficiently use machine learning to predict and prevent sports injuries?\nHow can we more efficiently use artificial intelligence to predict and prevent sports injuries?\nWhat does it look like on the day-to-day sports level when advice from machine learning findings are incorporated?\nWhat does using machine learning and artificial intelligence mean for the future of sports?\nWhat effect has machine learning and artificial intelligence had on predicting and preventing sports injuries so far?\nHow can we use machine learning to achieve the goals of the injury prevention field?\n\nAs shown throughout this project, we can learn a lot about injuries that athletes sustain as well as what measures athletes should be taking to prevent injuries. The types of injuries that athletes will sustain vary from sport to sport but the prevention methods are relatively standard across the board. As I learnt, machine learning can help to evaluate which injury prevention methods are the most effective, and what injury prevention athletes with fewer injuries take. Were the findings of this study to be implemented, it would take the form of more athletes partaking in stretching and warming up before activities, and regularly completing specific strength exercises.\n\n\nFuture Considerations\nHowever, because there hasn’t been much exploration into this topic as it is very specific, there is a lot that I was limited by and a lot that I would still like to explore. First, I would love to explore more into how female athletes versus male athletes tend to be injured and what prevention methods are best for athletes who compete in different ways. I’d also like to look more at other injury prevention methods such as physical therapy or sports medicine treatments, but I know that that data would be hard to come by because of how niche it is. Additionally, I would like to spend more time focusing on artificial intelligence and learning how it can be beneficial to athletes, but due to the scope of the project, I was not able to focus on it this time.\nInjuries in athletes are not only really common but also can be very severe. Because sports are such a big part of many countries’ cultures and many people look up to these athletes, athletes must take care of themselves. This project not only taught me a lot about injury prevention but also a lot about data science and I am excited to do more like this in the future."
  },
  {
    "objectID": "tabs/dimensionality_reduction/dimensionality_reduction.html",
    "href": "tabs/dimensionality_reduction/dimensionality_reduction.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "Dimensionality reduction is a technique that aims to transform data in a high-dimensional space into a low-dimensional space. Meaning, if a data set has a lot of features, say 40, dimensionality reduction would reduce the number of features to say 8, while retaining as much information about the features and the relationships within features in the process. The reason why people use dimensionality reduction is to improve the performance of a machine learning model by giving it less information to process, and make it easier to visualize the data.\nThere are many dimensionality reduction techniques, including Principal Component Analysis (PCA), T-distributed Stochastic Neighbour Embedding (t-SNE), Linear Discriminant Analysis (LDA), Independent Component Analysis (ICA), and more. In my project, I utilize PCA and t-SNE, which are introduced below.\nOnce again, I will be using the injury prevention factors data set. I will start with the full 8 injury prevention measures, and then will use PCA to determine the optimal number of components. Unlike in Naive Bayes, my target variable is the athlete’s position, rather than the number of injuries that they have had.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd \nimport matplotlib as plt\nfrom sklearn.preprocessing import StandardScaler\n\nfile_path = \"../../../../data/01-modified-data/injury_prevention_data_soccer.csv\"\nsoccer_injury = pd.read_csv(file_path)\n\nsoccer_injury.columns = soccer_injury.columns.str.strip()\n\nsoccer_injury = soccer_injury.replace({\"yes\": 1, \"no\": 0})\nsoccer_injury.head()\n\nx_vars = ['Prevention Measure Stretching', 'Prevention Measure Warm Up',\n       'Prevention Measure Specific Strength Exercises',\n       'Prevention Measure Bracing', 'Prevention Measure Taping',\n       'Prevention Measure Shoe Insoles', 'Prevention Measure Face Masks',\n       'Prevention Measure Medical Corset']\ntarget_var = \"Position\"\n\nsi_subset = soccer_injury[x_vars]\n\nX = si_subset.copy()\ny = soccer_injury[target_var].values.copy()\nX = StandardScaler().fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5000)"
  },
  {
    "objectID": "tabs/dimensionality_reduction/dimensionality_reduction.html#introduction",
    "href": "tabs/dimensionality_reduction/dimensionality_reduction.html#introduction",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "Dimensionality reduction is a technique that aims to transform data in a high-dimensional space into a low-dimensional space. Meaning, if a data set has a lot of features, say 40, dimensionality reduction would reduce the number of features to say 8, while retaining as much information about the features and the relationships within features in the process. The reason why people use dimensionality reduction is to improve the performance of a machine learning model by giving it less information to process, and make it easier to visualize the data.\nThere are many dimensionality reduction techniques, including Principal Component Analysis (PCA), T-distributed Stochastic Neighbour Embedding (t-SNE), Linear Discriminant Analysis (LDA), Independent Component Analysis (ICA), and more. In my project, I utilize PCA and t-SNE, which are introduced below.\nOnce again, I will be using the injury prevention factors data set. I will start with the full 8 injury prevention measures, and then will use PCA to determine the optimal number of components. Unlike in Naive Bayes, my target variable is the athlete’s position, rather than the number of injuries that they have had.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd \nimport matplotlib as plt\nfrom sklearn.preprocessing import StandardScaler\n\nfile_path = \"../../../../data/01-modified-data/injury_prevention_data_soccer.csv\"\nsoccer_injury = pd.read_csv(file_path)\n\nsoccer_injury.columns = soccer_injury.columns.str.strip()\n\nsoccer_injury = soccer_injury.replace({\"yes\": 1, \"no\": 0})\nsoccer_injury.head()\n\nx_vars = ['Prevention Measure Stretching', 'Prevention Measure Warm Up',\n       'Prevention Measure Specific Strength Exercises',\n       'Prevention Measure Bracing', 'Prevention Measure Taping',\n       'Prevention Measure Shoe Insoles', 'Prevention Measure Face Masks',\n       'Prevention Measure Medical Corset']\ntarget_var = \"Position\"\n\nsi_subset = soccer_injury[x_vars]\n\nX = si_subset.copy()\ny = soccer_injury[target_var].values.copy()\nX = StandardScaler().fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5000)"
  },
  {
    "objectID": "tabs/dimensionality_reduction/dimensionality_reduction.html#principal-component-analysis-pca",
    "href": "tabs/dimensionality_reduction/dimensionality_reduction.html#principal-component-analysis-pca",
    "title": "Dimensionality Reduction",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\nPrincipal Component Analysis, or PCA, is a statistical dimensionality reduction technique that determines the number of relevant features to retain. It linearly transforms the data set into a new coordinate system with fewer dimensions, where the almost all of the variation in data can be explained with fewer dimensions than the initial data.\nPCA is a good technique to use if you want to visualize your data as it allows you to visualize the first two principal components in a 2D graph to visually identity the clsuters of closely related data points.\nIn the following section, I will apply PCA to my dataset, determine the optimal number of principle components to retain, and create several visualizations of my results. My analysis of the findings will be included in the project report section.\n\nDetermining Optimal Number of Principal Components to Retain\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nclf = RandomForestClassifier(random_state=5000)\n\nnum_features_list = []\ntrain_accuracy_list = []\ntest_accuracy_list = []\n\nfor num_features in range(1, X.shape[1] + 1):\n    X_train_subset = X_train[:, :num_features]\n    X_test_subset = X_test[:, :num_features]\n\n    clf.fit(X_train_subset, y_train)\n\n    y_train_pred = clf.predict(X_train_subset)\n    y_test_pred = clf.predict(X_test_subset)\n\n    train_accuracy = accuracy_score(y_train, y_train_pred)\n    test_accuracy = accuracy_score(y_test, y_test_pred)\n\n    num_features_list.append(num_features)\n    train_accuracy_list.append(train_accuracy)\n    test_accuracy_list.append(test_accuracy)\n\nmax_test_accuracy_index = np.argmax(test_accuracy_list)\noptimal_num_features = num_features_list[max_test_accuracy_index]\nmax_test_accuracy = test_accuracy_list[max_test_accuracy_index]\n\nprint(\"--------------------\")\nprint(f\"The optimal numbers of features is: {optimal_num_features}\")\nprint(\"--------------------\")\n\nplt.figure(figsize=(10, 6))\nplt.plot(num_features_list, train_accuracy_list, label='Training Accuracy', marker='o')\nplt.plot(num_features_list, test_accuracy_list, label='Test Accuracy', marker='o')\n\nplt.axhline(y=max_test_accuracy, color='gray', linestyle='--', label=f'Max Test Accuracy: {max_test_accuracy:.3f}')\n\nplt.xlabel('Number of Features')\nplt.ylabel('Accuracy')\nplt.title('Training and Test Accuracy vs Number of Features')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n--------------------\nThe optimal numbers of features is: 5\n--------------------\n\n\n\n\n\nWhen visualizing the relationship between the number of features and the training/test accuracies, we can see that the ideal number of components to preserve is 5. At this point, both training and test accuracies peak. Further inclusion of features beyond this juncture results in diminishing returns, therefore opting for 5 principal components is ideal to achieve the best possible model performance.\n\n\nVisualizing Components in 2D and 3D\n\n\nCode\nfrom sklearn.decomposition import PCA\nimport numpy as np\nfrom numpy import linalg as LA\nimport matplotlib.pyplot as plt\nimport plotly.io as pio\n\nprint(\"--------------------\")\nprint('\\nNUMERIC MEAN:',np.mean(X,axis=0))\nprint(\"X SHAPE\",X.shape)\nprint(\"NUMERIC COV:\")\nprint(np.cov(X.T))\nprint(\"--------------------\")\n\nw, v1 = LA.eig(np.cov(X.T))\nprint(\"--------------------\")\nprint(\"\\nCOV EIGENVALUES:\",w)\nprint(\"COV EIGENVECTORS (across rows):\")\nprint(v1.T)\nprint(\"--------------------\")\n\npca = PCA(n_components=5) \npca.fit(X)\nprint(\"--------------------\")\nprint('\\nPCA')\nprint(pca.components_)\nv2=pca.components_\nprint(\"--------------------\")\n\n#print(v1/v2)\n\nprint(\"--------------------\")\nprint(\"2D Visualization\")\nprint(\"--------------------\")\n\nX_2d = np.dot(X, v2[:2, :].T)\nplt.figure(figsize=(8, 6))\nplt.scatter(X_2d[:, 0], X_2d[:, 1], marker=\".\", cmap=\"viridis\")\n\nplt.quiver(0, 0, v2[0, 0], v2[0, 1], color='r', scale=3, label='PCA 1')\nplt.quiver(0, 0, v2[1, 0], v2[1, 1], color='g', scale=3, label='PCA 2')\n\nplt.xlabel('PCA 1')\nplt.ylabel('PCA 2')\nplt.title('2D Projection of Data with PCA Components')\nplt.legend()\nplt.show()\n\nprint(\"--------------------\")\nprint(\"3D Visualization\")\nprint(\"--------------------\")\n\njitter = 0.01 * np.random.randn(*X.shape)\nX += jitter\n\nfig = plt.figure()\nax = fig.add_subplot(projection='3d')\nax.scatter(X[:, 0], X[:, 1], X[:, 2], marker=\".\")\nv1=v1*1000\nv2=v2*1000\n\nax.quiver(0,0,0,v1[0,0],v1[1,0],v1[2,0])\nax.quiver(0,0,0,v1[0,1],v1[1,1],v1[2,1])\nax.quiver(0,0,0,v1[0,2],v1[1,2],v1[2,2])\n\nax.quiver(0,0,0,v2[0,0],v2[1,0],v2[2,0])\nax.quiver(0,0,0,v2[0,1],v2[1,1],v2[2,1])\nax.quiver(0,0,0,v2[0,2],v2[1,2],v2[2,2])\nplt.show()\n\n\n--------------------\n\nNUMERIC MEAN: [ 9.26517057e-17  2.55590912e-17  7.66772736e-17 -3.19488640e-17\n  6.38977280e-17 -2.55590912e-17  0.00000000e+00  1.27795456e-17]\nX SHAPE (139, 8)\nNUMERIC COV:\n[[ 1.00724638  0.17987456 -0.05641036  0.03082525  0.01701829 -0.04164767\n   0.          0.02635633]\n [ 0.17987456  1.00724638  0.06734996  0.0167137   0.09153441  0.04881193\n   0.          0.07920553]\n [-0.05641036  0.06734996  1.00724638 -0.01423268  0.04663228  0.04921846\n   0.          0.12191545]\n [ 0.03082525  0.0167137  -0.01423268  1.00724638  0.24812509  0.03188526\n   0.         -0.03092457]\n [ 0.01701829  0.09153441  0.04663228  0.24812509  1.00724638  0.11284549\n   0.          0.11439477]\n [-0.04164767  0.04881193  0.04921846  0.03188526  0.11284549  1.00724638\n   0.         -0.05259292]\n [ 0.          0.          0.          0.          0.          0.\n   0.          0.        ]\n [ 0.02635633  0.07920553  0.12191545 -0.03092457  0.11439477 -0.05259292\n   0.          1.00724638]]\n--------------------\n--------------------\n\nCOV EIGENVALUES: [1.37521753 0.69732822 0.78409436 1.17481729 1.13367753 1.0204704\n 0.86511932 0.        ]\nCOV EIGENVECTORS (across rows):\n[[-0.23026559 -0.42450414 -0.2358463  -0.43530927 -0.60858415 -0.2427097\n   0.         -0.30393041]\n [-0.1089195   0.1265258  -0.09895721  0.5576344  -0.68197739  0.23367751\n   0.          0.36287844]\n [-0.63353687  0.66459623 -0.24656874 -0.03490773  0.06964111 -0.27023382\n   0.         -0.13058329]\n [ 0.4502687   0.44357956  0.17877813 -0.45564124 -0.28954    -0.33841371\n   0.          0.40319655]\n [-0.56514131 -0.17987249  0.63380655 -0.2561114   0.01847516  0.14993358\n   0.          0.39766316]\n [-0.08142011 -0.35020975 -0.12790662  0.31920708  0.1894602  -0.74252766\n   0.          0.40648265]\n [-0.07063644 -0.10166363 -0.65096497 -0.35104989  0.19902856  0.35370788\n   0.          0.52245761]\n [ 0.          0.          0.          0.          0.          0.\n   1.          0.        ]]\n--------------------\n--------------------\n\nPCA\n[[ 0.23026559  0.42450414  0.2358463   0.43530927  0.60858415  0.2427097\n  -0.          0.30393041]\n [ 0.4502687   0.44357956  0.17877813 -0.45564124 -0.28954    -0.33841371\n   0.          0.40319655]\n [-0.56514131 -0.17987249  0.63380655 -0.2561114   0.01847516  0.14993358\n   0.          0.39766316]\n [-0.08142011 -0.35020975 -0.12790662  0.31920708  0.1894602  -0.74252766\n  -0.          0.40648265]\n [-0.07063644 -0.10166363 -0.65096497 -0.35104989  0.19902856  0.35370788\n   0.          0.52245761]]\n--------------------\n--------------------\n2D Visualization\n--------------------\n--------------------\n3D Visualization\n--------------------\n\n\n\n\n\n\n\n\n\n\nInteractive Versions of 2D and 3D Graphs\n\n\nCode\nimport numpy as np\nimport plotly.graph_objs as go\nfrom sklearn.decomposition import PCA\nimport plotly\nimport plotly.io as pio\npio.renderers.default = \"notebook\"\n\n\npca = PCA(n_components=5)\npca.fit(X)\n\nv2 = pca.components_\n\nX_2d = np.dot(X, v2[:2, :].T)\n\nscatter_2d = go.Scatter(\n    x=X_2d[:, 0],\n    y=X_2d[:, 1],\n    mode='markers',\n    marker=dict(\n        size=8,\n        color=np.arange(len(X)),  \n        colorscale='Viridis',\n        opacity=0.8\n    ),\n    text=['Point {}'.format(i) for i in range(len(X))],  \n)\n\nlayout_2d = go.Layout(\n    title='2D Projection of Data with PCA Components',\n    xaxis=dict(title='PCA 1'),\n    yaxis=dict(title='PCA 2'),\n    showlegend=False,\n)\n\nfig_2d = go.Figure(data=[scatter_2d], layout=layout_2d)\n#fig_2d.show()\n\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\nplotly.offline.plot(fig_2d, filename='PCA_2d_interactive.html')\n\nIFrame(src='PCA_2d_interactive.html', width=800, height=600)\n\n\n\n        \n        \n\n\n\n\nCode\njitter = 0.01 * np.random.randn(*X.shape)\nX += jitter\n\nscatter_3d = go.Scatter3d(\n    x=X[:, 0],\n    y=X[:, 1],\n    z=X[:, 2],\n    mode='markers',\n    marker=dict(\n        size=6,\n        color=np.arange(len(X)),  \n        colorscale='Viridis',\n        opacity=0.8\n    ),\n    text=['Point {}'.format(i) for i in range(len(X))],  \n)\n\nlayout_3d = go.Layout(\n    title='3D Projection of Data with PCA Components',\n    scene=dict(\n        xaxis=dict(title='PCA 1'),\n        yaxis=dict(title='PCA 2'),\n        zaxis=dict(title='PCA 3'),\n    ),\n)\n\nfig_3d = go.Figure(data=[scatter_3d], layout=layout_3d)\n\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\nplotly.offline.plot(fig_3d, filename='PCA_3d_interactive.html')\n\nIFrame(src='PCA_3d_interactive.html', width=800, height=600)\n\n\n\n        \n        \n\n\n\n\nMost Frequently Used Prevention Measures\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npm_counts = si_subset.sum()\npm_counts = pm_counts.sort_values(ascending=False)\n\nsns.barplot(x=pm_counts.index, y=pm_counts.values, color='skyblue')\nplt.title('Prevention Measures Counts')\nplt.xlabel('Prevention Measures')\nplt.ylabel('Count')\nplt.xticks(rotation=45, ha='right')  \nplt.tight_layout()"
  },
  {
    "objectID": "tabs/dimensionality_reduction/dimensionality_reduction.html#dimensionality-reduction-with-t-sne",
    "href": "tabs/dimensionality_reduction/dimensionality_reduction.html#dimensionality-reduction-with-t-sne",
    "title": "Dimensionality Reduction",
    "section": "Dimensionality Reduction with t-SNE",
    "text": "Dimensionality Reduction with t-SNE\n\n\n\nt-Distributed Stochastic Neighbour Embedding is an unsupervised non-linear dimensionality reduction technique for both data exploration and visualizing high-dimensional data. It reduced the dimensions but giving each data point in a two or three dimensional map. It examines the similarity between each pair of points, constructing a map that preserves their relationships and reveals any patterns. Subsequently, it compresses these points onto a 2D map, where the distances between them mirror their similarities.\nPerplexity is a parameter in the t-SNE algorithm, shaping how the algorithm manages the trade-off between maintaining local and global structures in the data. Put simply, perplexity is a gauge of the practical number of neighbors that each data point takes into account when undergoing the process of dimensionality reduction.\nIn order to find the optimal parameter for t-SNE, we can test out multiple values of perplexity. In the following code, I test the following values of perplexity: 5, 10, 30, and 50. In order to explore what effect different values of perplexity have, I, again, utilize interactive graphs.\n\nPerplexity = 5\n\n\nCode\nfrom sklearn.manifold import TSNE\nimport plotly.express as px\n\nX_embedded = TSNE(n_components=2, learning_rate='auto',init='random', perplexity=5).fit_transform(X)\n\ndata = {\n    'x': X_embedded[:, 0],\n    'y': X_embedded[:, 1],\n    'Stretching': si_subset[\"Prevention Measure Stretching\"],\n    'Warm Up': si_subset[\"Prevention Measure Warm Up\"],\n    'Strength Exercises': si_subset[\"Prevention Measure Specific Strength Exercises\"],\n    'Bracing': si_subset[\"Prevention Measure Bracing\"],\n    'Taping': si_subset[\"Prevention Measure Taping\"],\n    'Shoe Insoles': si_subset[\"Prevention Measure Shoe Insoles\"],\n    'Face Masks': si_subset[\"Prevention Measure Face Masks\"],\n    'Medical Corset': si_subset[\"Prevention Measure Medical Corset\"]\n}\n\ndf = pd.DataFrame(data)\n\nprint(\"RESULTS\") \nprint(\"shape : \",X_embedded.shape)\nprint(\"First few points : \\n\",X_embedded[0:2,:])\n\nscatter_fig_5 = px.scatter(df, x='x', y='y', \n                         hover_data=['Stretching', 'Warm Up', \"Strength Exercises\", \"Bracing\", \"Taping\", \"Shoe Insoles\", \"Face Masks\", \"Medical Corset\"], \n                         template='simple_white',\n                         title='Perplexity = 5')\n\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\nplotly.offline.plot(scatter_fig_5, filename='per5_interactive.html')\n\nIFrame(src='per5_interactive.html', width=800, height=600)\n\n\nRESULTS\nshape :  (139, 2)\nFirst few points : \n [[-10.508396  53.720203]\n [-26.302603 -39.069176]]\n\n\n\n        \n        \n\n\n\n\nPerplexity = 10\n\n\nCode\nX_embedded = TSNE(n_components=2, learning_rate='auto',init='random', perplexity=10).fit_transform(X)\n\ndata = {\n    'x': X_embedded[:, 0],\n    'y': X_embedded[:, 1],\n    'Stretching': si_subset[\"Prevention Measure Stretching\"],\n    'Warm Up': si_subset[\"Prevention Measure Warm Up\"],\n    'Strength Exercises': si_subset[\"Prevention Measure Specific Strength Exercises\"],\n    'Bracing': si_subset[\"Prevention Measure Bracing\"],\n    'Taping': si_subset[\"Prevention Measure Taping\"],\n    'Shoe Insoles': si_subset[\"Prevention Measure Shoe Insoles\"],\n    'Face Masks': si_subset[\"Prevention Measure Face Masks\"],\n    'Medical Corset': si_subset[\"Prevention Measure Medical Corset\"]\n}\n\ndf = pd.DataFrame(data)\n\nprint(\"RESULTS\") \nprint(\"shape : \",X_embedded.shape)\nprint(\"First few points : \\n\",X_embedded[0:2,:])\n\nscatter_fig_10 = px.scatter(df, x='x', y='y', \n                         hover_data=['Stretching', 'Warm Up', \"Strength Exercises\", \"Bracing\", \"Taping\", \"Shoe Insoles\", \"Face Masks\", \"Medical Corset\"], \n                         template='simple_white',\n                         title='Perplexity = 10')\n\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\nplotly.offline.plot(scatter_fig_10, filename='per10_interactive.html')\n\nIFrame(src='per10_interactive.html', width=800, height=600)\n\n\nRESULTS\nshape :  (139, 2)\nFirst few points : \n [[-28.280321    -0.55170196]\n [  6.367788    25.649345  ]]\n\n\n\n        \n        \n\n\n\n\nPerplexity = 30\n\n\nCode\nX_embedded = TSNE(n_components=2, learning_rate='auto',init='random', perplexity=30).fit_transform(X)\n\ndata = {\n    'x': X_embedded[:, 0],\n    'y': X_embedded[:, 1],\n    'Stretching': si_subset[\"Prevention Measure Stretching\"],\n    'Warm Up': si_subset[\"Prevention Measure Warm Up\"],\n    'Strength Exercises': si_subset[\"Prevention Measure Specific Strength Exercises\"],\n    'Bracing': si_subset[\"Prevention Measure Bracing\"],\n    'Taping': si_subset[\"Prevention Measure Taping\"],\n    'Shoe Insoles': si_subset[\"Prevention Measure Shoe Insoles\"],\n    'Face Masks': si_subset[\"Prevention Measure Face Masks\"],\n    'Medical Corset': si_subset[\"Prevention Measure Medical Corset\"]\n}\n\ndf = pd.DataFrame(data)\n\nprint(\"RESULTS\") \nprint(\"shape : \",X_embedded.shape)\nprint(\"First few points : \\n\",X_embedded[0:2,:])\n\nscatter_fig_30 = px.scatter(df, x='x', y='y', \n                         hover_data=['Stretching', 'Warm Up', \"Strength Exercises\", \"Bracing\", \"Taping\", \"Shoe Insoles\", \"Face Masks\", \"Medical Corset\"], \n                         template='simple_white',\n                         title='Perplexity = 30')\n\n\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\nplotly.offline.plot(scatter_fig_30, filename='per30_interactive.html')\n\nIFrame(src='per30_interactive.html', width=800, height=600)\n\n\nRESULTS\nshape :  (139, 2)\nFirst few points : \n [[ 1.1499851 -4.915577 ]\n [-3.8866339 -3.473285 ]]\n\n\n\n        \n        \n\n\n\n\nPerplexity = 50\n\n\nCode\nX_embedded = TSNE(n_components=2, learning_rate='auto',init='random', perplexity=50).fit_transform(X)\n\ndata = {\n    'x': X_embedded[:, 0],\n    'y': X_embedded[:, 1],\n    'Stretching': si_subset[\"Prevention Measure Stretching\"],\n    'Warm Up': si_subset[\"Prevention Measure Warm Up\"],\n    'Strength Exercises': si_subset[\"Prevention Measure Specific Strength Exercises\"],\n    'Bracing': si_subset[\"Prevention Measure Bracing\"],\n    'Taping': si_subset[\"Prevention Measure Taping\"],\n    'Shoe Insoles': si_subset[\"Prevention Measure Shoe Insoles\"],\n    'Face Masks': si_subset[\"Prevention Measure Face Masks\"],\n    'Medical Corset': si_subset[\"Prevention Measure Medical Corset\"]\n}\n\ndf = pd.DataFrame(data)\n\nprint(\"RESULTS\") \nprint(\"shape : \",X_embedded.shape)\nprint(\"First few points : \\n\",X_embedded[0:2,:])\n\nscatter_fig_50 = px.scatter(df, x='x', y='y', \n                         hover_data=['Stretching', 'Warm Up', \"Strength Exercises\", \"Bracing\", \"Taping\", \"Shoe Insoles\", \"Face Masks\", \"Medical Corset\"], \n                         template='simple_white',\n                         title='Perplexity = 50')\n\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\nplotly.offline.plot(scatter_fig_50, filename='per50_interactive.html')\n\nIFrame(src='per50_interactive.html', width=800, height=600)\n\n\nRESULTS\nshape :  (139, 2)\nFirst few points : \n [[-0.2558082   0.60688174]\n [-1.4327123  -2.2652016 ]]"
  },
  {
    "objectID": "tabs/dimensionality_reduction/dimensionality_reduction.html#findings",
    "href": "tabs/dimensionality_reduction/dimensionality_reduction.html#findings",
    "title": "Dimensionality Reduction",
    "section": "Findings",
    "text": "Findings\nIn this dimensionality reduction section of my project, I first applied PCA to my dataset and determined the optimal number of prinicpal components to retain which was 5. I also was able to create a number of visualizations to display the structure and PCA’s groupings of my data. Next, I applied t-SNE to my dataset and created multiple visualizations displaying the impact of the different values of perplexity, all of which are interactive plots. My analyses of the results of my application of PCA and t-SNE can be found below.\n\nAnalysis of PCA Results\nUsing PCA, we can determine that the optimal number of principal components to retain is 5. Based on the 3D visualization, we can see that PCA is grouping the data into the clusters, likely based on what injury prevention measures that an athlete took. The interactive version of the plot confirms that the clusters are created using prevention measures as each colour represents a different prevention measure that was used to group the points. While it is common for athletes to do more than one prevention measure, PCA is grouping them based off of one common one.\nThere seem to be one or two evident outliers based on the non-interactive 3D plot. The first is most easily seen in the 3D plot, and it sits in the position (1.5, -3.5, 0.25). This point is not around many other points, which could be due to the athlete using an uncommon injury prevention measure. The other notable outlier has the approximate position (1.25, -4, -1.5). The reasons for this outlier are likely the same as the first.\nThe 2D visualization is a little less obvious, as it fails the capture the dimensionality of the data in the same way that the 3D plot does. However, we do see one obvious outlier in the top right corner of the plot, with the approximate position of (3.25, 8.25).\n\n\nAnalysis of t-SNE Results\nThe different values of perplexity seem to effect how tightly the points are clustered together. When values of perplexity are lower, points are very tightly grouped, whereas when perplexity is higher, points are much more spread out. When perplexity is 10 and 30, we see relatively tightly grouped points, but not too tight. I believe that a perplexity value of 10 gives a good indication of points that are similar to each other as, compared to other perplexity values, more tightly groups the points that are obviously in groups (such as the ones around (0, -7)). When we hover over the points, we can see that clusters are formed by what prevention measures an athlete took. The more obvious clusters are formed of points in which athletes took all of the same prevention measures, whereas the ones that don’t obviously belong to clusters may have one or two differing prevention measures.\nPCA and t-SNE have relatively similar results. Both of the dimensionality reduction techniques form groups based on clusters, indicating that they both recognize that the target may be closely related to what prevention measures an athlete took.\n\n\nEvaluation and Comparison\nIn my specific case, PCA and t-SNE were both very effective as retaining the data structure and information. Both methods were able to effectively group the data together into meaningful clusters. Visually, I found t-SNE more useful for visualizing how clusters were formed and what common variables clusters have. However, I found PCA more useful for visualizing the clusters and how similar or different each of the clusters were. The nature of the 3D plot made it quite easy to see how similar or dissimilar the clusters were since the distance between the clusters can easily be seen.\nPCA is more effective for linear data, whereas t-SNE can be more effective for non-linear data. If your objective is to preserve the overall structure of the data, because PCA focuses on overall variance, PCA may do a better job that t-SNE, as t-SNE focuses on smaller-scale relationships between points. However, because PCA focuses on variance, it is very sensitive to outliers, so if a dataset has a lot of outliers, PCA may have a hard time with it. In that case, t-SNE would be a better choice as it focuses on local relationships rather than variance. Both methods create opportunity for various visualizations. Due to PCA’s focus on overall structure and variance and t-SNE’s focus on preserving smaller-scale relationships within the data, each method’s visualizations will have its benefits and caveats. PCA’s visualizations will give a more comprehensive picture of the overarching structure of the data but may be more affected by outliers, whereas t-SNE will be more robust when it comes to outliers but may be less effective in showing overall data structure."
  },
  {
    "objectID": "tabs/code/code.html",
    "href": "tabs/code/code.html",
    "title": "Renee DeMaio",
    "section": "",
    "text": "You can access my github repo here."
  },
  {
    "objectID": "tabs/decision_tree/classification.html",
    "href": "tabs/decision_tree/classification.html",
    "title": "Decision Trees",
    "section": "",
    "text": "Decision trees are a non-parametric supervised learning model that can take two forms: classification and regression trees. Which one you select depends on your type of data. Classification is more suited towards categorical data, whereas regression is better for discrete data. Because I am working with categorical data, I will be using a classification decision tree.\n\n\n\nClassification decision trees are a great tool for helping you figure out information about a topic. Let’s use the game 20 questions as an example. Say I ask you to guess what food I am thinking of. The food I’ve selected is gummy worms. We can use a series of yes or no questions to figure out what I’ve selected. You could ask if it is a healthy food, to which my answer would be no. You could ask if it was protein, I’d say no. You could ask if it was a carb, I’d say yes. You could ask if it was sweet, I’d say yes. You could ask if there is dairy in it, I’d say no. You can ask if it was a candy, I’d say yes. And so on until you finally ask if I selected gummy worms, to which I’d say yes.\nThe appeal of decision trees is their ability to make the identification process very simple. Each question you ask helps to narrow down the number of possibilities. We can think of it as a visual flowchart that categorizes items based on their features. Going back to our previous example, we can classify foods based on their flavour profile or nutritional category. As we ask more questions, we will eventually get to our gummy worms.\nMaking an effective decision tree involves initially posing questions that will reveal the most amount of information about the object’s group. We want to figure out distinctive information as fast as possible, so we can distinguish between the groups. Decision trees have applications in many different fields, ranging from sorting out spam emails to diagnosing medical conditions. They are a tool that streamlines intricate decision-making by breaking it down into a sequence of straightforward choices."
  },
  {
    "objectID": "tabs/decision_tree/classification.html#introduction",
    "href": "tabs/decision_tree/classification.html#introduction",
    "title": "Decision Trees",
    "section": "",
    "text": "Decision trees are a non-parametric supervised learning model that can take two forms: classification and regression trees. Which one you select depends on your type of data. Classification is more suited towards categorical data, whereas regression is better for discrete data. Because I am working with categorical data, I will be using a classification decision tree.\n\n\n\nClassification decision trees are a great tool for helping you figure out information about a topic. Let’s use the game 20 questions as an example. Say I ask you to guess what food I am thinking of. The food I’ve selected is gummy worms. We can use a series of yes or no questions to figure out what I’ve selected. You could ask if it is a healthy food, to which my answer would be no. You could ask if it was protein, I’d say no. You could ask if it was a carb, I’d say yes. You could ask if it was sweet, I’d say yes. You could ask if there is dairy in it, I’d say no. You can ask if it was a candy, I’d say yes. And so on until you finally ask if I selected gummy worms, to which I’d say yes.\nThe appeal of decision trees is their ability to make the identification process very simple. Each question you ask helps to narrow down the number of possibilities. We can think of it as a visual flowchart that categorizes items based on their features. Going back to our previous example, we can classify foods based on their flavour profile or nutritional category. As we ask more questions, we will eventually get to our gummy worms.\nMaking an effective decision tree involves initially posing questions that will reveal the most amount of information about the object’s group. We want to figure out distinctive information as fast as possible, so we can distinguish between the groups. Decision trees have applications in many different fields, ranging from sorting out spam emails to diagnosing medical conditions. They are a tool that streamlines intricate decision-making by breaking it down into a sequence of straightforward choices."
  },
  {
    "objectID": "tabs/decision_tree/classification.html#class-distribution",
    "href": "tabs/decision_tree/classification.html#class-distribution",
    "title": "Decision Trees",
    "section": "Class distribution",
    "text": "Class distribution\nBefore we go any further, I’d like to look at the distribution of the label. If the distribution is skewed, it may affect the model’s performance.\n\n\nCode\nimport pandas as pd\nimport numpy as np \nimport sklearn.tree\nimport sklearn.model_selection\nfile_path = \"../../../../data/01-modified-data/injury_prevention_data_soccer.csv\"\nsoccer_injuries = pd.read_csv(file_path)\n\nlabel_dist = soccer_injuries[\"Number of Injuries\"].value_counts()\nlabel_dist = label_dist.sort_index()\nprint(\"DISTRIBUTION OF LABELS:\")\nprint(label_dist)\n\n\nDISTRIBUTION OF LABELS:\nNumber of Injuries\n0     22\n1     41\n2     42\n3     15\n4     11\n5      3\n6      2\n7      2\n10     1\nName: count, dtype: int64\n\n\n\n\nCode\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n\nsns.barplot(x= label_dist.index, y=label_dist.values, color=\"lavender\")\nplt.plot(label_dist.index, label_dist.values, color='purple', marker='o', linestyle='-')\nplt.ylabel(\"Count\")\nplt.title(\"Count per Number of Injuries\")\n\n\nText(0.5, 1.0, 'Count per Number of Injuries')\n\n\n\n\n\n\n\nCode\nplt.pie(label_dist.values, labels=[f\"{count} injuries\" for count in label_dist.index], autopct='%.0f%%')\n\n\n([&lt;matplotlib.patches.Wedge at 0x175da9310&gt;,\n  &lt;matplotlib.patches.Wedge at 0x175dab310&gt;,\n  &lt;matplotlib.patches.Wedge at 0x175db8c50&gt;,\n  &lt;matplotlib.patches.Wedge at 0x175dba350&gt;,\n  &lt;matplotlib.patches.Wedge at 0x175dbb950&gt;,\n  &lt;matplotlib.patches.Wedge at 0x175dc90d0&gt;,\n  &lt;matplotlib.patches.Wedge at 0x175dca810&gt;,\n  &lt;matplotlib.patches.Wedge at 0x175dcbe90&gt;,\n  &lt;matplotlib.patches.Wedge at 0x175dbba50&gt;],\n [Text(0.966797659815723, 0.5246925623399303, '0 injuries'),\n  Text(-0.3775197345008592, 1.0331886807657162, '1 injuries'),\n  Text(-0.8720577330038374, -0.6704590295522971, '2 injuries'),\n  Text(0.40077272406551884, -1.0243931001549667, '3 injuries'),\n  Text(0.9014631001688704, -0.6303683677294802, '4 injuries'),\n  Text(1.0528595955870075, -0.3185697286001347, '5 injuries'),\n  Text(1.0820679519520409, -0.19781038233195886, '6 injuries'),\n  Text(1.0955078094302033, -0.09931082255946361, '7 injuries'),\n  Text(1.0997190565076482, -0.024859540501146027, '10 injuries')],\n [Text(0.5273441780813034, 0.2861959430945074, '16%'),\n  Text(-0.2059198551822868, 0.5635574622358451, '29%'),\n  Text(-0.47566785436572945, -0.36570492521034387, '30%'),\n  Text(0.21860330403573755, -0.5587598728118, '11%'),\n  Text(0.49170714554665657, -0.3438372914888074, '8%'),\n  Text(0.5742870521383676, -0.17376530650916436, '2%'),\n  Text(0.5902188828829313, -0.10789657218106845, '1%'),\n  Text(0.5975497142346563, -0.05416953957788923, '1%'),\n  Text(0.5998467580950808, -0.01355974936426147, '1%')])\n\n\n\n\n\nAs we can see based on the graphs above, the distribution of number of injuries is skewed to the right as it is much more common for an athlete to have 1 or 2 injuries than any other amount of injuries. This can have a few affects on my classification algorithm results.\n\nModel Bias\nBecause the 1 injury, 2 injuries, and 0 injuries categories make up 75% of the data, the model may do really well at correctly classifying points in those categories, but less well at correctly categorizing points in the 3, 4, 5, 6, 7, and 10 injuries categories.\nAccuracy Scores\nThe accuracy score of the model may not be particularly reliable. Because most of the data points belong to the 0, 1, or 2 injuries classes, the model may predict those classes for all data points which would make the accuracy high but the score wouldn’t mean much.\nFeature Importance\nAn imbalanced dataset may make the model focus more on features that help distinguish the majority classes, rather the features that help distinguish the minority classes. It is important to make sure that all features are being considered during the classification process."
  },
  {
    "objectID": "tabs/decision_tree/classification.html#baseline-model-for-comparison",
    "href": "tabs/decision_tree/classification.html#baseline-model-for-comparison",
    "title": "Decision Trees",
    "section": "Baseline model for comparison",
    "text": "Baseline model for comparison\nBefore I get into the classification decision tree, I want to first see how well a random guessing model does at correctly predicting the number of injuries an athlete may have. This will give us a baseline for measuring how effective the decision tree is.\n\n\nCode\nsoccer_injuries.columns = soccer_injuries.columns.str.strip()\ncolumns_keep = ['Prevention Measure Stretching', 'Prevention Measure Warm Up',\n       'Prevention Measure Specific Strength Exercises',\n       'Prevention Measure Bracing', 'Prevention Measure Taping',\n       'Prevention Measure Shoe Insoles', 'Prevention Measure Face Masks',\n       'Prevention Measure Medical Corset', \"Number of Injuries\"]\ndf_features = soccer_injuries[columns_keep]\n\ncolumns_features = ['Prevention Measure Stretching', 'Prevention Measure Warm Up',\n       'Prevention Measure Specific Strength Exercises',\n       'Prevention Measure Bracing', 'Prevention Measure Taping',\n       'Prevention Measure Shoe Insoles', 'Prevention Measure Face Masks',\n       'Prevention Measure Medical Corset']\nfeatures = soccer_injuries[columns_features]\n\nfeatures.columns = features.columns.str.strip()\n\nimport numpy as np\nnum_obs = len(df_features)\nrng = np.random.default_rng(30)\nrandom_guesses = np.random.choice([i for i in range(8)] + [10], num_obs)\ndf_features[\"random_guesses\"] = random_guesses\ndf_features\n\n\n\n\n\n\n\n\n\nPrevention Measure Stretching\nPrevention Measure Warm Up\nPrevention Measure Specific Strength Exercises\nPrevention Measure Bracing\nPrevention Measure Taping\nPrevention Measure Shoe Insoles\nPrevention Measure Face Masks\nPrevention Measure Medical Corset\nNumber of Injuries\nrandom_guesses\n\n\n\n\n0\nyes\nno\nyes\nno\nno\nno\nno\nno\n6\n4\n\n\n1\nyes\nyes\nno\nno\nno\nno\nno\nno\n2\n5\n\n\n2\nyes\nno\nno\nno\nno\nyes\nno\nno\n7\n3\n\n\n3\nyes\nyes\nyes\nno\nno\nno\nno\nno\n1\n6\n\n\n4\nyes\nyes\nno\nno\nyes\nno\nno\nno\n2\n10\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n134\nyes\nno\nno\nno\nno\nno\nno\nno\n0\n4\n\n\n135\nyes\nno\nno\nyes\nno\nno\nno\nno\n2\n1\n\n\n136\nyes\nyes\nyes\nno\nyes\nno\nno\nyes\n2\n2\n\n\n137\nyes\nno\nno\nno\nyes\nyes\nno\nno\n1\n7\n\n\n138\nyes\nyes\nno\nno\nno\nno\nno\nno\n0\n4\n\n\n\n\n139 rows × 10 columns\n\n\n\n\n\nCode\nfrom sklearn.metrics import f1_score\n\nf1 = f1_score(df_features[\"Number of Injuries\"], df_features[\"random_guesses\"], average=\"macro\")\nprint(f\"f1 score: {f1}\")\nprint(\"-------------\")\n#df_features[\"random_guesses_correct\"] = df_features[\"Number of Injuries\"] == df_features[\"random_guesses\"]\n#correct_random_guess_rows = df_features[df_features[\"random_guesses_correct\"] == True]\nrandom_guess_accuracy = len(correct_random_guess_rows) / len(df_features)\nprint(f\"Random guess accuracy: {random_guess_accuracy}\")\nprint(\"-------------\")\n\n\nf1 score: 0.1098411921329338\n-------------\nRandom guess accuracy: 0.11510791366906475\n-------------\n\n\nThe basic, unweighted random classifier has an accuracy of 11.51% which is not high and that is to be expected. Because there is a right skew in the data and points in the 0, 1, and 2 injuries classes make up 75% of the data, a classifier that has a 12.5% chance of assigning each number will not be able to accuracy represent this skew. Let’s try again with a random classifier that is weighted the same as the distribution of the labels.\n\n\nCode\nimport numpy as np\nnum_obs = len(df_features)\nrng = np.random.default_rng(30)\nchoices = [i for i in range(8)] + [10]\nweights = [0.16, 0.29, 0.3, 0.11, 0.08, 0.02, 0.01, 0.01, 0.01]\nweights /= np.sum(weights)\nweights = weights[:len(choices)]\nweights_dict = dict(zip(choices, weights))\nprint(weights_dict)\nrandom_guesses_weighted = rng.choice(choices, num_obs, p=weights)\ndf_features[\"random_guesses_weighted\"] = random_guesses_weighted\ndf_features\n\n\n{0: 0.16161616161616163, 1: 0.29292929292929293, 2: 0.30303030303030304, 3: 0.11111111111111112, 4: 0.08080808080808081, 5: 0.020202020202020204, 6: 0.010101010101010102, 7: 0.010101010101010102, 10: 0.010101010101010102}\n\n\n\n\n\n\n\n\n\nPrevention Measure Stretching\nPrevention Measure Warm Up\nPrevention Measure Specific Strength Exercises\nPrevention Measure Bracing\nPrevention Measure Taping\nPrevention Measure Shoe Insoles\nPrevention Measure Face Masks\nPrevention Measure Medical Corset\nNumber of Injuries\nrandom_guesses\nrandom_guesses_weighted\n\n\n\n\n0\nyes\nno\nyes\nno\nno\nno\nno\nno\n6\n4\n1\n\n\n1\nyes\nyes\nno\nno\nno\nno\nno\nno\n2\n5\n1\n\n\n2\nyes\nno\nno\nno\nno\nyes\nno\nno\n7\n3\n0\n\n\n3\nyes\nyes\nyes\nno\nno\nno\nno\nno\n1\n6\n2\n\n\n4\nyes\nyes\nno\nno\nyes\nno\nno\nno\n2\n10\n3\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n134\nyes\nno\nno\nno\nno\nno\nno\nno\n0\n4\n0\n\n\n135\nyes\nno\nno\nyes\nno\nno\nno\nno\n2\n1\n2\n\n\n136\nyes\nyes\nyes\nno\nyes\nno\nno\nyes\n2\n2\n1\n\n\n137\nyes\nno\nno\nno\nyes\nyes\nno\nno\n1\n7\n1\n\n\n138\nyes\nyes\nno\nno\nno\nno\nno\nno\n0\n4\n2\n\n\n\n\n139 rows × 11 columns\n\n\n\n\n\nCode\nf1 = f1_score(df_features[\"Number of Injuries\"], df_features[\"random_guesses_weighted\"], average=\"macro\")\nprint(f\"f1 score: {f1}\")\nprint(\"-------------\")\ndf_features[\"random_guesses_weighted_correct\"] = df_features[\"Number of Injuries\"] == df_features[\"random_guesses_weighted\"]\ncorrect_random_guess_weighted_rows = df_features[df_features[\"random_guesses_weighted_correct\"] == True]\nrandom_guess_weighted_accuracy = len(correct_random_guess_weighted_rows) / len(df_features)\nprint(f\"Weighted random guess accuracy: {random_guess_weighted_accuracy}\")\nprint(\"-------------\")\n\n\nf1 score: 0.11044140294140295\n-------------\nWeighted random guess accuracy: 0.2517985611510791\n-------------\n\n\nWhen we weight the random choices to match the distribution of our data, it is significantly more accurate with an accuracy rate of 25%. That is a pretty good accuracy rate for random guessing. It is clear that weighting our random guesses to match the distribution helps."
  },
  {
    "objectID": "tabs/decision_tree/classification.html#feature-selection",
    "href": "tabs/decision_tree/classification.html#feature-selection",
    "title": "Decision Trees",
    "section": "Feature selection",
    "text": "Feature selection\n\n\nCode\nparam_grid = {\n    'max_depth': [2,3,4,5,6,7,8],\n    'min_samples_leaf': [1, 2, 4, 8, 16],\n    'min_impurity_decrease': [0.01, 0.02, 0.03, 0.04, 0.05]\n}\n\ndtc = sklearn.tree.DecisionTreeClassifier(random_state=5000)\n\ngrid_search = GridSearchCV(dtc, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n\ngrid_search.fit(X_train_num, y_train)\n\nbest_params = grid_search.best_params_\nprint(\"Best Hyperparameters:\", best_params)\n\nresults = grid_search.cv_results_\nparam_names = list(param_grid.keys())\nparam_values = [param_grid[name] for name in param_names]\n\n\nBest Hyperparameters: {'max_depth': 5, 'min_impurity_decrease': 0.01, 'min_samples_leaf': 1}\n\n\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\n\nthreshold = 0.05\nfeature_importances = clf.feature_importances_\nselected_features = X_train.columns[feature_importances &gt; threshold]\n\nX_train_selected = X_train[selected_features]\nX_test_selected = X_test[selected_features]\n\nprint(\"Selected Features:\", selected_features)\nprint(\"Feature Importances:\", feature_importances)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_selected)\nX_test_scaled = scaler.transform(X_test_selected)\n\n\nSelected Features: Index(['Prevention Measure Stretching', 'Prevention Measure Warm Up',\n       'Prevention Measure Specific Strength Exercises',\n       'Prevention Measure Bracing', 'Prevention Measure Taping',\n       'Prevention Measure Shoe Insoles'],\n      dtype='object')\nFeature Importances: [0.14759918 0.22554741 0.23204718 0.10404068 0.19730943 0.08347534\n 0.         0.00998078]\n\n\n\n\nCode\nimport sklearn.ensemble\n\ndef accuracy_over_range(est_range, depth_range, long_format=True):\n  result_data = []\n  for cur_estimators in est_range:\n    for cur_depth in depth_range:\n      cur_rfc = sklearn.ensemble.RandomForestClassifier(\n        n_estimators = cur_estimators,\n        max_depth = cur_depth,\n        random_state = 5000\n      )\n      cur_rfc.fit(X_train_num, y_train)\n      y_train_pred_rfc = cur_rfc.predict(X_train_num)\n      y_test_pred_rfc = cur_rfc.predict(X_test_num)\n      y_train_correct = y_train_pred_rfc == Xy_train[\"Number of Injuries\"]\n      y_test_correct = y_test_pred_rfc == Xy_test[\"Number of Injuries\"]\n      train_accuracy = sum(y_train_correct) / len(y_train_correct)\n      test_accuracy = sum(y_test_correct) / len(y_test_correct)\n      cur_result = {\n          'n_estimators': cur_estimators,\n          'max_depth': cur_depth,\n          'train_accuracy': train_accuracy,\n          'test_accuracy': test_accuracy\n      }\n      result_data.append(cur_result)\n  rfc_result_df = pd.DataFrame(result_data)\n  if long_format:\n    rfc_long_df = pd.melt(rfc_result_df, id_vars=['n_estimators','max_depth'])\n    return rfc_long_df\n  return rfc_result_df\n\nestimators_range = np.arange(20, 220, 20)\ndepth_range = [2,3,4,5,6,7]\nrfc_result_df = accuracy_over_range(estimators_range, depth_range)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_hyperparam_grid(result_df, wrap=3):\n  g = sns.FacetGrid(result_df, col=\"max_depth\", col_wrap=wrap)\n  g.map_dataframe(sns.lineplot, x=\"n_estimators\", y=\"value\", hue='variable', marker='o')\n  plt.show()\nplot_hyperparam_grid(rfc_result_df)\n\n\n\n\n\nWhen we look at the training and test accuracy for each of our different values for maximum death, we can see that the training accuracy and test accuracy are the highest for max_depth = 5. Therefore we choose that as our value. The other best parameters are calculated as well, with minimum purity decreasing being 0.1 and minimum sample leafs being 1.\nWhen we perform feature selection we see that the important features are Prevention Measure Stretching, Prevention Measure Warm Up, Prevention Measure Specific Strength Exercises, Prevention Measure Bracing, Prevention Measure Taping, and Prevention Measure Shoe Insoles. Two features were deamed not important, which were medical corsets and face masks, which makes sense as they had very small sample sizes.\n\n\nCode\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier, export_text, export_graphviz\nimport sklearn.tree\nimport sklearn.model_selection\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image\nfrom sklearn.tree import plot_tree\nimport graphviz\nimport plotly.graph_objects as go\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,  confusion_matrix\n\ndtc = sklearn.tree.DecisionTreeClassifier(\n    max_depth = 6,\n    random_state=5000,\n    min_samples_leaf=1,\n    min_impurity_decrease=0.01\n)\n\ndtc.fit(X_train, y_train)\n\nimportant_features = ['Prevention Measure Stretching', 'Prevention Measure Warm Up',\n       'Prevention Measure Specific Strength Exercises',\n       'Prevention Measure Bracing', 'Prevention Measure Taping',\n       'Prevention Measure Shoe Insoles']\n\nX_df = soccer_injuries[important_features].copy()\nX_df = X_df.replace({\"yes\": 1, \"no\": 0})\ny = soccer_injuries[\"Number of Injuries\"].copy()\n\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n    X_df, y, test_size=0.2, random_state=5000\n)\n\ny_train_pred = dtc.predict(X_train)\ny_test_pred = dtc.predict(X_test)\n\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\n\ntrain_precision = precision_score(y_train, y_train_pred, average='macro')\ntest_precision = precision_score(y_test, y_test_pred, average='macro')\n\ntrain_recall = recall_score(y_train, y_train_pred, average='macro')\ntest_recall = recall_score(y_test, y_test_pred, average='macro')\n\ntrain_f1 = f1_score(y_train, y_train_pred, average='macro')\ntest_f1 = f1_score(y_test, y_test_pred, average='macro')\n\nprint(\"Training Accuracy:\", train_accuracy)\nprint(\"Testing Accuracy:\", test_accuracy)\n\nprint(\"Training Precision:\", train_precision)\nprint(\"Testing Precision:\", test_precision)\n\nprint(\"Training Recall:\", train_recall)\nprint(\"Testing Recall:\", test_recall)\n\nprint(\"Training F1 Score:\", train_f1)\nprint(\"Testing F1 Score:\", test_f1)\n\nconf_matrix_train = confusion_matrix(y_train, y_train_pred)\nconf_matrix_test = confusion_matrix(y_test, y_test_pred)\n\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nsns.heatmap(conf_matrix_train, annot=True, fmt='d', cmap='Blues', cbar=False, vmin=1, vmax=10)\nplt.title('Confusion Matrix - Training Set')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\n\nplt.subplot(1, 2, 2)\nsns.heatmap(conf_matrix_test, annot=True, fmt='d', cmap='Blues', cbar=False, vmin=1, vmax=10)\nplt.title('Confusion Matrix - Testing Set')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\n\nplt.tight_layout()\nplt.show()\n\n\nTraining Accuracy: 0.36936936936936937\nTesting Accuracy: 0.17857142857142858\nTraining Precision: 0.14187276977974653\nTesting Precision: 0.05952380952380952\nTraining Recall: 0.1776135741652983\nTesting Recall: 0.15306122448979592\nTraining F1 Score: 0.13615600393520802\nTesting F1 Score: 0.08055853920515575\n\n\n\n\n\n\n\nCode\nXy_train = pd.concat([X_train, y_train], axis=1)\nXy_train['prediction'] = y_train_pred\nXy_test = pd.concat([X_test, y_test], axis=1)\nXy_test['prediction'] = y_test_pred\n\n\nclf = DecisionTreeClassifier(max_depth=5, random_state=5000)\nclf.fit(X_train, y_train)\n\ndot_data = export_graphviz(\n    clf, out_file=None,\n    feature_names=important_features,\n    class_names=[str(i) for i in clf.classes_],\n    filled=True, rounded=True\n)\n\ngraph = graphviz.Source(dot_data, format=\"png\")\ngraph.render(\"decision_tree\", cleanup=True)\ngraph.view(\"decision_tree\")"
  },
  {
    "objectID": "tabs/decision_tree/classification.html#final-results",
    "href": "tabs/decision_tree/classification.html#final-results",
    "title": "Decision Trees",
    "section": "Final results",
    "text": "Final results\nThe final results of my decision tree classifier were not very successful. The accuracy of predicting the test data was merely 17.86%, which was less than the accuracy of the weighted random guessing method. The recall, or the number of correctly classified positives out of the total number of correct classifications (both positive and negative), was only 15% on the testing data, and the precision, or the number of correctly classified positives out of the total number of positives (both true and false positives), was only 5.92% on the testing data. These values were higher on the training data, but not by a large amount. In the confusion matricies, we can see that the model did a fair job at predicting points in the 0, 1, and 2 injuries categories but was unable to predict any points correctly for athletes with any amount of injuries over 3. However, that success was not reflected in the test data as there are very few correct predictions.\nThe fit isn’t particularly good, but I think that may be due to the heavily right skewed distribution of the label. With so much of the data falling within the 0, 1, and 2 injuries categories, it makes sense that observations will be incorrectly classified into these categories. Perhaps in the future we can look to taking a smaller subset of the data and focus only on the most used common three or four features to have a more accurate classification."
  },
  {
    "objectID": "tabs/decision_tree/classification.html#conclusions",
    "href": "tabs/decision_tree/classification.html#conclusions",
    "title": "Decision Trees",
    "section": "Conclusions:",
    "text": "Conclusions:\nIn my attempt to predict the number of injuries an athlete may have based on what injury prevention methods they use using a decision tree classifier, the outcome unfortunately was not as successful as I originally hoped it would be. The model’s accuracy in predicting test data was a mere 17.86%, even lower than a method that randomly guesses based on the distribution of the data. Because the majority of the athletes in the data set have only had 0, 1, or 2 injuries in their careers so far (this accounts for 75% of the athletes), the classifier may have had a hard time correctly predicting athletes that have had more than 2 injuries. Additionally, because there are so many possible injury prevention methods that they could take, there are many confounding variables and possible combinations of injury prevention methods that may have made accurate classification more difficult. The overall performance leaves much room for improvement.\nFor future directions, I think focusing in on the most common injury prevention methods, i.e. stretching, warming up, and strengthening exercises, could lead to more successful classification. Additionally, we could likely also ignore data points where athletes have had 5, 6, 7, or 10 injuries because there are only a few of each of these points. This smaller and more refined data set could help find more accurate results and provide meaningful information about the correlation between injury prevention methods and number of injuries, which would guide athletes in how to properly protect themselves."
  },
  {
    "objectID": "tabs/eda/eda.html",
    "href": "tabs/eda/eda.html",
    "title": "Data Exploration",
    "section": "",
    "text": "After we collect our data and before we start our analysis, it is important that we understand what is in our data sets. The process of exploratory data analysis helps us do that. Exploratory data analysis, or often referred to as EDA, aims to uncover any trends in the data, discover outliers, and reveal relationships between variables. Its goal is to get a look at the underlying structure of the data set, screen the data to identify any obvious errors or outliers, understand patterns within the data, detect outliers, and prompt us to ask questions and check and validate our assumptions. All of these smaller goals can be summed up by the overarching goal of enhacing our understanding of our data and giving us insights into what exists within our data sets.\n\n\n\nThere are many different technqiues for data exploration, but the main two categories that these methods fall into are graphical and non-graphical. Graphical EDA includes producing visualizations to visually represent what is in the data set. Graphs such as box plots, histograms, scatterplots, stem-and-leaf plots, parallel coordinate plots, etc, are common and very useful for graphical EDA. Non-graphical EDA often includes clustering and dimensionality reduction techniques, as well as summary statistics. I will get into clustering and dimension reduction later in this website so I will not address that now, but summary statistics are very useful for figuring out what the important numerical variables are within the data.\nWhen we dig a little further into EDA, we also can further subdivide into univariate and multivariate EDA. Univariate EDA is quite simple, as you are only focusing on one variable. Non-graphical univariate EDA includes describing that one variable and finding any patterns within it. Graphical univariate EDA helps to provide all the context that the non-graphical methods lack. This can be done through graphs mentioned above.\nMultivariate EDA is a little more complex but provides us with more information. Non-graphical multivariate EDA methods can show relationships within the data through cross-correlation statistics. Graphical multivariate EDA methods use similar graphs to univariate EDA, but also includes multivariate charts such as heatmaps, bubble charts, and other multivariate charts.\nFinally, the last data exploration method worth touching on is Confirmatory Data Analysis, or CDA. CDA is the opposite of EDA as it is guided by a specific hypothesis or theories using collected data. It takes a deductive approach and is driven by analysis and a pre-defined theory or model. Researchers will often design their experiences or data collection processes with these pre-defined theories in mind. CDA relies heavily on statistical methods to assess the validity of the hypothesis. There is a greater emphasis on validity and reliability in CDA than in EDA as CDA aims to provide evidence that supports or contradicts the proposed hypothesis.\nI chose to use EDA methods instead of CDA methods because of how many unknowns come with sports data and the options for moving forward that EDA would give me. Using EDA instead of CDA allowed me to simply learn about the different relationships within the data and choose the path that will provide the most valuable insights into what will keep athletes safe."
  },
  {
    "objectID": "tabs/eda/eda.html#introduction",
    "href": "tabs/eda/eda.html#introduction",
    "title": "Data Exploration",
    "section": "",
    "text": "After we collect our data and before we start our analysis, it is important that we understand what is in our data sets. The process of exploratory data analysis helps us do that. Exploratory data analysis, or often referred to as EDA, aims to uncover any trends in the data, discover outliers, and reveal relationships between variables. Its goal is to get a look at the underlying structure of the data set, screen the data to identify any obvious errors or outliers, understand patterns within the data, detect outliers, and prompt us to ask questions and check and validate our assumptions. All of these smaller goals can be summed up by the overarching goal of enhacing our understanding of our data and giving us insights into what exists within our data sets.\n\n\n\nThere are many different technqiues for data exploration, but the main two categories that these methods fall into are graphical and non-graphical. Graphical EDA includes producing visualizations to visually represent what is in the data set. Graphs such as box plots, histograms, scatterplots, stem-and-leaf plots, parallel coordinate plots, etc, are common and very useful for graphical EDA. Non-graphical EDA often includes clustering and dimensionality reduction techniques, as well as summary statistics. I will get into clustering and dimension reduction later in this website so I will not address that now, but summary statistics are very useful for figuring out what the important numerical variables are within the data.\nWhen we dig a little further into EDA, we also can further subdivide into univariate and multivariate EDA. Univariate EDA is quite simple, as you are only focusing on one variable. Non-graphical univariate EDA includes describing that one variable and finding any patterns within it. Graphical univariate EDA helps to provide all the context that the non-graphical methods lack. This can be done through graphs mentioned above.\nMultivariate EDA is a little more complex but provides us with more information. Non-graphical multivariate EDA methods can show relationships within the data through cross-correlation statistics. Graphical multivariate EDA methods use similar graphs to univariate EDA, but also includes multivariate charts such as heatmaps, bubble charts, and other multivariate charts.\nFinally, the last data exploration method worth touching on is Confirmatory Data Analysis, or CDA. CDA is the opposite of EDA as it is guided by a specific hypothesis or theories using collected data. It takes a deductive approach and is driven by analysis and a pre-defined theory or model. Researchers will often design their experiences or data collection processes with these pre-defined theories in mind. CDA relies heavily on statistical methods to assess the validity of the hypothesis. There is a greater emphasis on validity and reliability in CDA than in EDA as CDA aims to provide evidence that supports or contradicts the proposed hypothesis.\nI chose to use EDA methods instead of CDA methods because of how many unknowns come with sports data and the options for moving forward that EDA would give me. Using EDA instead of CDA allowed me to simply learn about the different relationships within the data and choose the path that will provide the most valuable insights into what will keep athletes safe."
  },
  {
    "objectID": "tabs/eda/eda.html#record-data",
    "href": "tabs/eda/eda.html#record-data",
    "title": "Data Exploration",
    "section": "Record Data",
    "text": "Record Data\nThe main methods that I use for doing exploratory data analysis of my record data are graphical methods. I utilize bar charts, pie charts, scatterplots, confusion matricies, and more. I use the packages Pandas, Matplotlib, Numpy, etc, to so do.\n\nInjury Prevention Factors\nWhen it came to the injury prevention factors data, I wanted to explore more into each athlete’s personal statistics (height, weight, age), as well as further into the injury prevention and injury risk factors. I first started by taking all the numerical columns and finding summary statistics. Then, for all the binary labels, I wanted to see what athletes had previous injuries, risk factors, and what prevention measures they were taking, so I made a series of bar charts to further explore those variables.\n\n\nCode\nimport pandas as pd\nfile_path = \"../../../../data/01-modified-data/injury_prevention_data_soccer.csv\"\n\ninjury_prevention = pd.read_csv(file_path)\ninjury_prevention[\"Height\"] = injury_prevention[\"Height\"].astype(int)\ninjury_prevention[\"Mass\"] = injury_prevention[\"Mass\"].astype(int)\n\nnumerical_columns = injury_prevention.select_dtypes(include='int64')\nnumerical_columns = injury_prevention.drop(columns=\"ID\")\nnumerical_columns.describe()\n\n\n\n\n\n\n\n\n\nAge\nHeight\nMass\nTeam\nPosition\nYears of Football Experience\nNumber of Injuries\nNumber of Ankle Injuries\nNumber of Knee Injuries\nNumber of Thigh Injuries\nImportance Injury Prevention\nKnowledgeability\n\n\n\n\ncount\n139.000000\n139.000000\n139.000000\n139.000000\n139.000000\n139.000000\n139.000000\n139.000000\n139.000000\n139.000000\n139.000000\n139.000000\n\n\nmean\n17.597122\n177.043165\n68.417266\n3.690647\n2.546763\n1.733813\n1.906475\n0.683453\n0.517986\n0.726619\n1.345324\n0.827338\n\n\nstd\n4.601070\n9.416198\n11.781156\n1.825142\n0.853153\n0.913470\n1.614661\n0.932784\n0.684746\n0.778383\n0.586221\n0.415777\n\n\nmin\n13.000000\n141.000000\n31.000000\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n14.000000\n172.500000\n61.000000\n2.000000\n2.000000\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n1.000000\n1.000000\n\n\n50%\n16.000000\n178.000000\n70.000000\n4.000000\n3.000000\n2.000000\n2.000000\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n\n\n75%\n19.000000\n184.000000\n76.000000\n5.500000\n3.000000\n2.000000\n2.000000\n1.000000\n1.000000\n1.000000\n2.000000\n1.000000\n\n\nmax\n35.000000\n196.000000\n101.000000\n6.000000\n4.000000\n4.000000\n10.000000\n4.000000\n3.000000\n3.000000\n4.000000\n2.000000\n\n\n\n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nobject_columns = injury_prevention.select_dtypes(include='object')\nobject_col_names = object_columns.columns\n#print(object_col_names)\nnum_rows = (len(object_col_names)) // 4\n#print(num_rows)\nfig, axes = plt.subplots(num_rows, 4, figsize=(16, num_rows * 4))\naxes = axes.flatten()\n\nfor i, column in enumerate(object_columns):\n    ax = axes[i]\n    sns.countplot(data=injury_prevention, x=column, order=['yes', 'no'], palette=['pink', 'lightblue'], ax=ax)\n    ax.set_title(f'{column}')\n\nfor j in range(len(object_columns), len(axes)):\n    axes[j].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nInjuries from Different Activities\nThis data set compares injuries by age group in different activities. I wanted to see how different age groups compare between activities so I made a bar chart comparing all age groups for each activity to see which one prompted the most injuries. Some things to note about this graph and data set are (1) that the activites that have the least amount of injuries are activites that aren’t necessarily accessible for everyone, such as skiing and water skiing. Weather or condition depending sports (such as sports requiring water or snow) are only accessible to people who live in places with those resources. And (2) that the age groups aren’t particularly even. The 24-65 age group is disproportionally bigger than the other age groups which can lead to unclear or unrealiable analysis.\n\n\nCode\nfile_path = \"../../../../data/01-modified-data/sports_injury_data.csv\"\nsports_injuries = pd.read_csv(file_path)\nic_subset = sports_injuries[['Injuries (1)', 'Younger than 5',\n       '5 to 14', '15 to 24', '25 to 64', '65 and older']]\nactivity_type = sports_injuries[\"Sport, activity or equipment\"]\n\nsports_injuries[\"Injuries (1)\"] = sports_injuries[\"Injuries (1)\"].str.replace(',', '').astype(float)\nsports_injuries[\"Younger than 5\"] = sports_injuries[\"Younger than 5\"].str.replace(',', '').astype(float)\nsports_injuries[\"5 to 14\"] = sports_injuries[\"5 to 14\"].str.replace(',', '').astype(float)\nsports_injuries[\"15 to 24\"] = sports_injuries[\"15 to 24\"].str.replace(',', '').astype(float)\nsports_injuries[\"25 to 64\"] = sports_injuries[\"25 to 64\"].str.replace(',', '').astype(float)\nsports_injuries[\"65 and older\"] = sports_injuries[\"65 and older\"].str.replace(',', '').astype(float)\n\nmelted_df = pd.melt(sports_injuries, id_vars=['Sport, activity or equipment'], var_name='Age group', value_name='Injuries')\nplt.figure(figsize=(12, 6))\nsns.barplot(data=melted_df, x='Sport, activity or equipment', y='Injuries', hue='Age group')\nplt.title('Injuries by Age Group for Different Activities')\nplt.xlabel('Sport, activity or equipment')\nplt.xticks(rotation=45, size=8, ha=\"right\")\n\nplt.show()\n\n\n\n\n\n\n\nNBA Injury Data\nThe NBA injury data is interesting as it includes information about not only the type of injury but also the team, which allows us to explore any correlations between the two teams.\n\n\nCode\nimport pandas as pd \nimport numpy as np\nimport gdown\n\n# the csv url is: https://drive.google.com/file/d/1tdKeSFi492daHWh8Laqb7e3_68o3kaqD/view?usp=share_link\nfile_id = \"1tdKeSFi492daHWh8Laqb7e3_68o3kaqD\"\nurl = f\"https://drive.google.com/uc?id={file_id}\"\noutput = \"basketball_injuries.csv\"\n#gdown.download(url, output, quiet=False)\n\nfile_path = \"../../../../data/01-modified-data/basketball_injury_data.csv\"\n\nbball_injury_data = pd.read_csv(file_path)\n\n\n\nInjury Status Bar Chart\n\n\nCode\nimport matplotlib.pyplot as plt\n\ninjury_counts = bball_injury_data[\"InjuryStatus\"].value_counts()\n\ntop_5_injury_counts = injury_counts.head(5)\n\nplt.figure(figsize=(8, 6))\nplt.bar(top_5_injury_counts.index, top_5_injury_counts.values, color='pink')\nplt.xlabel('Injury Status')\nplt.ylabel('Count')\nplt.title('Top 5 Injury Status Bar Chart')\n\nplt.show()\n\n\n\n\n\n\n\nInjuries Per Team\n\n\nCode\nteam_counts = bball_injury_data[\"Team\"].value_counts()\nteam_names = bball_injury_data[\"Team\"].unique()\n\nplt.figure(figsize=(8, 8))\nplt.pie(team_counts, labels=team_names, autopct='%1.1f%%', startangle=140, shadow=True)\nplt.axis(\"equal\")\nplt.title(\"Injury Count per Team\")\nplt.show()\n\n\n\n\n\n\n\nInjuries by Dates\n\n\nCode\nbball_injury_data['Date'] = pd.to_datetime(bball_injury_data['Date'])\ndate_counts_2010 = bball_injury_data[bball_injury_data['Date'].dt.year == 2010]\ndate_counts_2010 = date_counts_2010['Date'].value_counts().sort_index()\n\nplt.figure(figsize=(12, 6))\nplt.plot(date_counts_2010.index, date_counts_2010.values, marker='o', linestyle='-', color=\"green\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Injury Count\")\nplt.title(\"Injury Counts by Day in 2010\")\nplt.xticks(rotation=45)\n\nplt.grid(True)\nplt.show()\n\n\n\n\n\nThe first graph created for this data set reveals that the most common reason given for an athlete’s absence from an NBA game is simply that they did not play. This can be for any variety of reasons from personal reasons, issues with the coach, or injury as well. The next most common is did not dress, which could imply that the athlete is injuries, but doesn’t specifically state that. The next graph shows that most teams in the NBA had relatively similar rates of injury with most of the hovering around 3%. Finally, the last graph shows one interesting spike in injuries in late October of 2010. There is nothing notable in NBA history that day that would cause so many more athletes to be out, so this can be considered an outlier. Outside of that, the average number of injuries across all teams per day hovers between 0-15.\n\n\n\nNFL Concussion Data\nThis NFL concussion data set has a lot of information about specific instances of head injuries as it provides the game the injury occured in, what kind of head injury it was, the playing time before and after the injury, how many weeks were missed, etc. This prompted me to wonder about which teams experience the most head injuries, as well if there is any correlation between the week of the injury and playing time after the injury.\n\nInjury Count by Team\n\n\nCode\nfile_path = \"../../../../data/01-modified-data/nfl_concussions.csv\"\n\nconcussion_data = pd.read_csv(file_path)\ninjuries_by_team = concussion_data['Team'].value_counts()\n \nplt.figure(figsize=(15, 6))\nplt.bar(injuries_by_team.index, injuries_by_team.values, color=\"skyblue\")\nplt.xlabel(\"Team Name\")\nplt.ylabel(\"Injury Count\")\nplt.title(\"Injury Count by Team\")\nplt.xticks(rotation=60)\n\nplt.show()\n\n\n\n\n\n\n\nType of Injury\n\n\nCode\ninjury_type = concussion_data[\"Reported Injury Type\"].value_counts()\ninjury_name = injury_type.index\n\npastel_colors = ['#FF9999', '#66B2FF', '#99FF99', '#FFCC99', '#c2c2f0', '#ffb3e6', '#c2f0c2']\n\nplt.figure(figsize=(8, 8))\nplt.pie(injury_type, labels=injury_name, autopct='%1.1f%%', startangle=140, shadow=False, colors=pastel_colors)\nplt.axis(\"equal\")\nplt.title(\"Injury Count by Type of Injury\")\nplt.legend(injury_name, title=\"Injury Types\", loc=\"best\")\nplt.show()\n\n\n\n\n\n\n\nInjuries by Position\n\n\nCode\ninjuries_by_pos = concussion_data['Position'].value_counts()\n \nplt.figure(figsize=(15, 6))\nplt.bar(injuries_by_pos.index, injuries_by_pos.values, color=\"orange\")\nplt.xlabel(\"Position\")\nplt.ylabel(\"Injury Count\")\nplt.title(\"Injury Count by Position\")\nplt.xticks(rotation=60)\n\nplt.show()\n\n\n\n\n\n\n\nCorrelation Between Week of Injury and Total Snaps\n\n\nCode\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nconcussion_data[[\"Play Time After Injury (Number)\", \"Play Time After Injury\"]] = \\\n    concussion_data[\"Play Time After Injury\"].str.split(\" \", expand=True)\n\nsubset = [\"Week of Injury\", \"Play Time After Injury (Number)\"]\nconcussion_data_subset = concussion_data[subset]\n\nconcussion_data_subset[\"Play Time After Injury (Number)\"] = pd.to_numeric(\n    concussion_data_subset[\"Play Time After Injury (Number)\"], errors=\"coerce\"\n)\n\ncorr_matrix = concussion_data_subset.corr()\n\nsns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\")\nplt.title(\"Correlation Map between Week of Injury and Play Time After Injury\")\nplt.show()\n\n\n\n\n\nThe graphs made from the NFL concussion data reveal that the Cincinatti Bengals and the Cleveland Browns had the most concussions on their team. The majority of head injuries are concussions (81.7%), but there are also a fair share of non-concussion head injuries (18.0%). It appears that the illness category (0.3%) can be taken as an outlier and can be disgarded because the data set is only intended to focus on head injuries. Next, the position that sees the most amount of concussion injuries is cornerback, closely followed by wide receiver and safety. Finally, there was a small negative correlation between week of injury and play time after injury, meaning that if a player got injured at a later week, they tended to not play very much afterwards, implying that they either got injured too late in the season or that their injury was severe.\n\n\n\nNFL Game Injury Data\nThe NFL game injury data set includes more information about the conditions in which the injury occured in, such as what the weather was like, what the field conditions and set up were, and of course information about the injury as well, such as what type of injury it was.\n\nInjury Count by Stadium Type\n\n\nCode\nfile_path = \"../../../../data/01-modified-data/nfl_injuries.csv\"\nnfl_injuries = pd.read_csv(file_path)\n\nstadium_type = nfl_injuries[\"StadiumType\"].value_counts()\ncolors = plt.cm.viridis(np.linspace(0, 1, len(stadium_type)))\n\n\nplt.figure(figsize=(12,6))\nplt.bar(stadium_type.index, stadium_type.values, color=colors)\nplt.xlabel(\"Stadium Type\")\nplt.ylabel(\"Injury Count\")\nplt.title(\"Injury Count by Stadium Types\")\nplt.xticks(rotation = 60)\nplt.show()\n\n\n\n\n\n\n\nInjury Count by Body Part Injured\n\n\nCode\nimport seaborn as sns\n\ninjury_frequency = nfl_injuries[\"BodyPart\"].value_counts()\ninjury_place = nfl_injuries[\"BodyPart\"].unique()\nprint(injury_frequency)\n\npastel_colors = ['#FF9999', '#66B2FF', '#99FF99', '#FFCC99', '#c2c2f0', '#ffb3e6', '#c2f0c2']\n\nplt.figure(figsize=(8, 8))\nplt.pie(injury_frequency, labels=injury_place, autopct='%1.1f%%', startangle=140, shadow=False, colors=pastel_colors)\nplt.axis(\"equal\")\nplt.title(\"Injury Count by Type of Injury\")\nplt.show()\n\n\nBodyPart\nKnee     825\nAnkle    720\nToes     144\nFoot      96\nHeel      18\nName: count, dtype: int64\n\n\n\n\n\n\n\nInjury Count vs Temperature\n\n\nCode\nnfl_injuries[\"Temperature\"] = pd.to_numeric(nfl_injuries[\"Temperature\"], errors='coerce')\nnfl_injuries = nfl_injuries[nfl_injuries[\"Temperature\"] &gt;= 0]\nnfl_injuries = nfl_injuries.dropna(subset=[\"Temperature\"])\n\ntemperature_counts = nfl_injuries[\"Temperature\"].value_counts()\n\nplt.figure(figsize=(12, 6))\nplt.plot(temperature_counts.values, temperature_counts.index, marker='o', linestyle='-', color=\"green\")\nplt.xlabel(\"Temperature\")\nplt.ylabel(\"Injury Count\")\nplt.title(\"Injury Counts by Temperature\")\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\nThis dataset reveals that the majority of NFL injuries occur in stadiums that are outdoors. Most of the injuries are knee or ankle injuires which makese sense due to the nature of American Football as there is much running involved with many sudden changes in directions. Players often are tackled by other players as well which often happens at the knee level. Finally, there doesn’t seem to be a huge correlation between temperature and number of injuries which is interesting. I would think that there would be more injuries in colder weather as muscles are colder and it is easier for athletes to get out of their warmed up state, but in reality, spikes in injuries occur during many points in the year."
  },
  {
    "objectID": "tabs/eda/eda.html#text-data",
    "href": "tabs/eda/eda.html#text-data",
    "title": "Data Exploration",
    "section": "Text Data",
    "text": "Text Data\n\nNews API\n\nWord Cloud\n\n\nCode\nimport pandas as pd \n\nfile_path = \"../../../../data/01-modified-data/cleaned_news_data.csv\"\ndf = pd.read_csv(file_path)\n\nall_text = \"\"\nfor row in df[\"combined_t&d\"]:\n    all_text = all_text + \" \" + row \n\nall_text = \" \".join(list(df[\"combined_t&d\"].values))\n\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\n\nwords = all_text.split()\n\nstop_words = set(stopwords.words('english'))\nfiltered_words = [word for word in words if word.lower() not in stop_words]\n\nfiltered_text = ' '.join(filtered_words)\n\ndef generate_word_cloud(my_text): \n    from wordcloud import WordCloud, STOPWORDS\n    import matplotlib.pyplot as plt\n\n    def plot_cloud(wordcloud):\n        plt.figure(figsize=(40, 30))\n        plt.imshow(wordcloud)\n        plt.axis(\"off\");\n\n    wordcloud = WordCloud(\n        width = 3000, \n        height = 2000,\n        random_state=1, \n        background_color='black',\n        colormap= \"BuGn_r\",\n        collocations=False,\n        stopwords = STOPWORDS).generate(my_text)\n    plot_cloud(wordcloud)\n    plt.show()\n\n    \n\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/reneedemaio/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\n\n\nCode\ngenerate_word_cloud(filtered_text)\n\n\n\n\n\n\n\nBar Chart\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\nwords = filtered_text.split()\nword_freq = Counter(words)\n\nfrequency_threshold = 10\n\ncommon_words = [(word, freq) for word, freq in word_freq.items() if freq &gt; frequency_threshold]\ncommon_words.sort(key=lambda x: x[1], reverse=True)\n\nwords, frequencies = zip(*common_words)\n\nmost_common_words = word_freq.most_common(10)  \n\nwords, frequencies = zip(*most_common_words)\n\nplt.figure(figsize=(10, 6))\nplt.bar(words, frequencies)\nplt.xlabel('Words')\nplt.ylabel('Frequency')\nplt.title('Word Frequency Bar Chart')\nplt.xticks(rotation=45)  \n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nThe data exploration for this data was successful and revealed that the majority of the topics discussed in the news relating to soccer are about specific players or competitions, and less about injuries. While some information of injuries are present, based on the lack of words that could relate to discussion of injury, other than the word ‘injury’, it appears that these mentions are surface level. Additionally was pleased to see heavy mention of women’s sports! The frequency bar chart showed that the most common word was soccer, followed by injury. Again, the mention is very surface level. It is important to consider the news cycle when we look at this specific set of data. I think the data being focused on Megan Rapinoe could be due to the fact that Megan Rapinoe just tore her achilles in her last game of her career a few weeks ago. While news data is interesting and can be revealing, its reliability and results are very dependent on what API you have access to and what is going on in the world."
  },
  {
    "objectID": "tabs/eda/eda.html#hypothesis-refinement",
    "href": "tabs/eda/eda.html#hypothesis-refinement",
    "title": "Data Exploration",
    "section": "Hypothesis Refinement",
    "text": "Hypothesis Refinement\nBased off of the exploratory data analysis process, it became necessary for me to refine my original hypotheses and identify proper next steps. The analysis of all of these different sports was insightful for us to understand what types of injuries are common for athletes, how these injuries commonly occur, and in what conditions they occur in. However, I think a more specific and therefore meaningful way to proceed would be to focus on injury prevention factors and take a deeper look into what methods can help prevent some of the injuries athletes experience. I want to produce an analysis that is going to be able to contribute to a specific converation, and that will only be possible by focusing on one of these data sets, while keeping what I learnt from the other ones in the back of my mind."
  },
  {
    "objectID": "tabs/data_gathering/data_gathering.html",
    "href": "tabs/data_gathering/data_gathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "In this project, I used a variety of data sets regarding injuries and injury prevention in sports. I used both record and text data. The data sets came from sources such as academic articles, Kaggle from users who had already created sports related data sets from respective sports websites such as the NFL and NBA or from competitions hosted by professional sports leagues, statistical institutes, APIs, and data.world.\nWhile I have a lot of data sets, I didn’t use all of them in my main analysis. Because injuries in sports don’t gain a ton of media attention if they aren’t big, career ending types of injuries, I wanted to provide a little more context for the injuries that do occur before diving into what we can do to prevent them. My data sets are focused on three main sports: football (or soccer, but I will use football throughout the course of this project), basketball, and American football. I decided to use three sports for my exploratory data analysis to see how the nature of injuries differs between sports.\nThe data set that is used for my main analysis is about injury prevention factors. More about this data set can be found below, and throughout the other data related tabs on my website, but I thought this data set would be a valuable one to focus on and would be the one to answer the questions in my introduction that are guiding the course of this project."
  },
  {
    "objectID": "tabs/data_gathering/data_gathering.html#introduction",
    "href": "tabs/data_gathering/data_gathering.html#introduction",
    "title": "Data Gathering",
    "section": "",
    "text": "In this project, I used a variety of data sets regarding injuries and injury prevention in sports. I used both record and text data. The data sets came from sources such as academic articles, Kaggle from users who had already created sports related data sets from respective sports websites such as the NFL and NBA or from competitions hosted by professional sports leagues, statistical institutes, APIs, and data.world.\nWhile I have a lot of data sets, I didn’t use all of them in my main analysis. Because injuries in sports don’t gain a ton of media attention if they aren’t big, career ending types of injuries, I wanted to provide a little more context for the injuries that do occur before diving into what we can do to prevent them. My data sets are focused on three main sports: football (or soccer, but I will use football throughout the course of this project), basketball, and American football. I decided to use three sports for my exploratory data analysis to see how the nature of injuries differs between sports.\nThe data set that is used for my main analysis is about injury prevention factors. More about this data set can be found below, and throughout the other data related tabs on my website, but I thought this data set would be a valuable one to focus on and would be the one to answer the questions in my introduction that are guiding the course of this project."
  },
  {
    "objectID": "tabs/introduction/introduction.html",
    "href": "tabs/introduction/introduction.html",
    "title": "Introduction to Sports Injuries and Injury Prevention",
    "section": "",
    "text": "Injury prevention in sports is an increasingly important topic in sports as athletes continue to increase their ability to perform well and competition gets more fierce. As sport remains an important part of our cultures and societies, keeping athletes healthy as they compete at a high level becomes more important. So far, the sports injury prevention field has not had a lot of research conducted that regards data. The existing studies mainly focus on how we can use machine learning and artificial intelligence to predict and prevent sports injuries. There are a few different voices in this realm including one discussing the machine learning and artificial intelligence side of things and how we can make those tools more efficient, as well as voices coming from the medical point of view striving to reduce the frequency of injuries. I would like to explore both the machine learning and artificial intelligence perspective and how we can apply those findings to athletes’ daily lives. I also would like to explore the effectiveness of injury prevention methods to see what methods are most effective in keeping athletes healthy.\nBelow are summaries of two of articles that represent the research that has been conducted in this field so far."
  },
  {
    "objectID": "tabs/introduction/introduction.html#questions",
    "href": "tabs/introduction/introduction.html#questions",
    "title": "Introduction to Sports Injuries and Injury Prevention",
    "section": "10 Questions",
    "text": "10 Questions\n\n\n\nBelow are 10 questions that I would like to explore throughout the course of this project.\n\nWhat information about injuries can be discovered with machine learning?\nHow can we use machine learning and artificial intelligence to predict and prevent sports injuries?\nIs there a difference in the data between predicting injuries in male athletes versus predicting injuries in female athletes?\nHow can we make the results of many of these studies more useful to clinicians?\nHow can we more efficiently use machine learning to predict and prevent sports injuries?\nHow can we more efficiently use artificial intelligence to predict and prevent sports injuries?\nWhat does it look like on the day to day sports level when advice from machine learning findings are incorporated?\nWhat does using machine learning and artificial intelligence mean for the future of sports?\nWhat effect has machine learning and artificial intelligence had on predicting and preventing sports injuries so far?\nHow can we use machine learning to achieve the goals of the injury prevention field?\n\nWhile I may not be able to answer all of these questions, at minimum I would like to discover how we can use data to make sports a safer place for athletes. As sports continue to develop and become more competitive, and as athletes continue to push themselves to the limits, injury prevention methods need to adapt. Without adaptable injury prevention methods, sports will only become more dangerous."
  },
  {
    "objectID": "tabs/clustering/clustering.html",
    "href": "tabs/clustering/clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering is a set of useful machine learning models that is used to build groups of data points that have similar features. While there are a variety of different clustering methods, I will give a brief overview of three clustering methods: KMeans, DBSCAN, and Hierarchical Clustering.\n\n\nThe K-Means clustering method is an unsupervised machine lerning algorithm. The goal of K-Means to group points that are similar together into cluseters in an attempt to reveal any underlying patterns. K-Means works only with numerical data, and each data point must be able to be described using numerical coordinates (Burkardt 2009).\n\n\n\nIn order to return clusters of your selected input data, K-Means will create k numbers of clusters, based on a value of k that you select. One way that we can choose an optimal values of k is by using the elbow method, which plots the sum of the squared distances between each cluster as the value of k increases. When we look at the plot, there is a point where “increasing the size of the cluster provides minimal gain to the error function (Artley 2022).” This can be seen in the image to the right.\nAfter we choose our value for k, K-Means will assign each data point to a cluster using the euclidian distance to each centroid. After all the data points are assinged, the centroids of each cluster will be updated by taking the mean of the data points and assigning it to be the new center of the cluster. This reassigning points to clusters and recalcuating the centroids will occur until the centroids values don’t change anymore (Artley 2022).\n\n\n\nDBSCAN, or density-based spatial clustering of applications with noise, is an unsupervised clustering algorithm based on clusters and noise. Any density based clustering method is useful when we have a dataset that has irregular or intertwined clusters or when there is a lot of noise or outliers.\nDBSCAN will first divide the dataset into n number of dimensions. Then it will group points together that are tightly packed by forming an n dimensional shape around each point in the dataset. Clusters will be formed by the points that fall within that shape (Lutins 2017).\n\n\n\n\n\n\nHierarchical clustering is an unsupervised clustering method. It has a nested structure that is somewhat organized like a tree, and does not assume a value of k. Hierarchical clustering can be further subdivided into two types: agglomerative (bottom up) and divisive (top down) hierarchical clustering. We can visualize this technique using a dendrogram.\n\n\n\nIn agglomerative hierarchical clustering, “each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy [how to cite the slide].” The basic algorithm of agglomerative hierarchical clustering includes computing the proximity matrix, letting each data point be its own cluster, and then repreating to merge the two closest clusters and update the proximity matrix, until only one cluster remains (Patlolla 2018).\nDivisive hierarchical clustering is the opposite of agglomerative hierarchical clustering. In divisive hierarchical clustering, all of the observations start in one cluster, and then the splits are performed as one cluster moves down in the hierarchy."
  },
  {
    "objectID": "tabs/clustering/clustering.html#introduction",
    "href": "tabs/clustering/clustering.html#introduction",
    "title": "Clustering",
    "section": "",
    "text": "Clustering is a set of useful machine learning models that is used to build groups of data points that have similar features. While there are a variety of different clustering methods, I will give a brief overview of three clustering methods: KMeans, DBSCAN, and Hierarchical Clustering.\n\n\nThe K-Means clustering method is an unsupervised machine lerning algorithm. The goal of K-Means to group points that are similar together into cluseters in an attempt to reveal any underlying patterns. K-Means works only with numerical data, and each data point must be able to be described using numerical coordinates (Burkardt 2009).\n\n\n\nIn order to return clusters of your selected input data, K-Means will create k numbers of clusters, based on a value of k that you select. One way that we can choose an optimal values of k is by using the elbow method, which plots the sum of the squared distances between each cluster as the value of k increases. When we look at the plot, there is a point where “increasing the size of the cluster provides minimal gain to the error function (Artley 2022).” This can be seen in the image to the right.\nAfter we choose our value for k, K-Means will assign each data point to a cluster using the euclidian distance to each centroid. After all the data points are assinged, the centroids of each cluster will be updated by taking the mean of the data points and assigning it to be the new center of the cluster. This reassigning points to clusters and recalcuating the centroids will occur until the centroids values don’t change anymore (Artley 2022).\n\n\n\nDBSCAN, or density-based spatial clustering of applications with noise, is an unsupervised clustering algorithm based on clusters and noise. Any density based clustering method is useful when we have a dataset that has irregular or intertwined clusters or when there is a lot of noise or outliers.\nDBSCAN will first divide the dataset into n number of dimensions. Then it will group points together that are tightly packed by forming an n dimensional shape around each point in the dataset. Clusters will be formed by the points that fall within that shape (Lutins 2017).\n\n\n\n\n\n\nHierarchical clustering is an unsupervised clustering method. It has a nested structure that is somewhat organized like a tree, and does not assume a value of k. Hierarchical clustering can be further subdivided into two types: agglomerative (bottom up) and divisive (top down) hierarchical clustering. We can visualize this technique using a dendrogram.\n\n\n\nIn agglomerative hierarchical clustering, “each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy [how to cite the slide].” The basic algorithm of agglomerative hierarchical clustering includes computing the proximity matrix, letting each data point be its own cluster, and then repreating to merge the two closest clusters and update the proximity matrix, until only one cluster remains (Patlolla 2018).\nDivisive hierarchical clustering is the opposite of agglomerative hierarchical clustering. In divisive hierarchical clustering, all of the observations start in one cluster, and then the splits are performed as one cluster moves down in the hierarchy."
  },
  {
    "objectID": "tabs/clustering/clustering.html#methods",
    "href": "tabs/clustering/clustering.html#methods",
    "title": "Clustering",
    "section": "Methods",
    "text": "Methods\n\nData Selection\nI’ve decided to use the injury prevention factors data set for clustering because I am curious how different clustering methods would cluster the data based on the different injury prevention method that the athletes used. The goal of the clustering is to predict how many injuries an athlete would have and see what injury prevention methods are most common for athletes who have a certain number of injuries.\n\n\nFeature Selection\nIn the Decision Trees tab, I found that the optimal features are the following six: Prevention Measure Stretching, Prevention Measure Warm Up, Prevention Measure Specific Strength Exercises, Prevention Measure Bracing, Prevention Measure Taping, and Prevention Measure Shoe Insoles. Two features were deamed not important, which were medical corsets and face masks, which makes sense as they had very small sample sizes. I will not perform feature selection again as I have already done it, but will instead use this subset.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd \nimport matplotlib as plt\n\nfile_path = \"../../../../data/01-modified-data/injury_prevention_data_soccer.csv\"\nsoccer_injury = pd.read_csv(file_path)\n\nsoccer_injury.columns = soccer_injury.columns.str.strip()\n\nsoccer_injury = soccer_injury.replace({\"yes\": 1, \"no\": 0})\nsoccer_injury.head()\n\n\n\n\n\n\n\n\n\nID\nAge\nHeight\nMass\nTeam\nPosition\nYears of Football Experience\nPrevious Injuries\nNumber of Injuries\nAnkle Injuries\n...\nImportance Injury Prevention\nKnowledgeability\nPrevention Measure Stretching\nPrevention Measure Warm Up\nPrevention Measure Specific Strength Exercises\nPrevention Measure Bracing\nPrevention Measure Taping\nPrevention Measure Shoe Insoles\nPrevention Measure Face Masks\nPrevention Measure Medical Corset\n\n\n\n\n0\n146\n19\n173.0\n67.6\n1\n3\n1\n1\n6\n1\n...\n2\n1\n1\n0\n1\n0\n0\n0\n0\n0\n\n\n1\n155\n22\n179.5\n71.0\n1\n3\n1\n1\n2\n0\n...\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n\n\n2\n160\n22\n175.5\n71.8\n1\n3\n1\n1\n7\n1\n...\n1\n1\n1\n0\n0\n0\n0\n1\n0\n0\n\n\n3\n164\n23\n190.0\n80.5\n1\n4\n1\n1\n1\n0\n...\n1\n1\n1\n1\n1\n0\n0\n0\n0\n0\n\n\n4\n145\n19\n173.5\n68.7\n1\n3\n1\n1\n2\n1\n...\n1\n2\n1\n1\n0\n0\n1\n0\n0\n0\n\n\n\n\n5 rows × 41 columns\n\n\n\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\n\nimportant_features = [\"Prevention Measure Stretching\", \"Prevention Measure Warm Up\", \"Prevention Measure Specific Strength Exercises\", \"Prevention Measure Bracing\", \"Prevention Measure Taping\", \"Prevention Measure Shoe Insoles\"]\nX = soccer_injury[important_features]\nX_norm = StandardScaler().fit_transform(X)\n\nX.head()\n\n\n\n\n\n\n\n\n\nPrevention Measure Stretching\nPrevention Measure Warm Up\nPrevention Measure Specific Strength Exercises\nPrevention Measure Bracing\nPrevention Measure Taping\nPrevention Measure Shoe Insoles\n\n\n\n\n0\n1\n0\n1\n0\n0\n0\n\n\n1\n1\n1\n0\n0\n0\n0\n\n\n2\n1\n0\n0\n0\n0\n1\n\n\n3\n1\n1\n1\n0\n0\n0\n\n\n4\n1\n1\n0\n0\n1\n0"
  },
  {
    "objectID": "tabs/clustering/clustering.html#hyper-parameter-tuning",
    "href": "tabs/clustering/clustering.html#hyper-parameter-tuning",
    "title": "Clustering",
    "section": "Hyper-Parameter Tuning",
    "text": "Hyper-Parameter Tuning\nHyper-parameter tuning is the process of finding the best and most effective set of parameters for each tuning method. As explained above, two ways we can find the optimal number of clusters are using the elbow method and the silhouette scores.\n\nK-Means: Elbow Method\n\n\nCode\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.cluster import MeanShift\nfrom sklearn.cluster import Birch\nfrom sklearn.cluster import DBSCAN\nimport numpy as np \nimport matplotlib.pyplot as plt\n\nnum_clusters = range(1, 15)\nkmeans_values = []\ninertia_values = []\ndistortion_values = []\n\n\nfor k in num_clusters:\n    kmeans = KMeans(n_clusters=k, n_init=10)\n    kmeans.fit(X_norm) \n    centroids = kmeans.cluster_centers_\n    labels = kmeans.labels_\n\n    distortion = 0\n    for i in range(len(X_norm)):\n        cluster_idx = labels[i]\n        distortion += np.linalg.norm(X_norm[i] - centroids[cluster_idx])**2\n    distortion /= len(X_norm) \n    distortion_values.append(distortion)\n\n    inertia = kmeans.inertia_\n    \n    kmeans_values.append(k)\n    inertia_values.append(inertia) \n\ndf = pd.DataFrame({\"Number of Clusters\": kmeans_values, \"Inertia\": inertia_values, \"Distortion\": distortion_values})\ndf.head()\n\nsecond_derivative = np.diff(np.diff(inertia_values))\nelbow_index = np.argmax(second_derivative) + 1\nprint(f\"The 'Elbow' of the graph is: {elbow_index}\")\n\nplt.figure(figsize=(15, 6))\nplt.scatter(data=df, x=\"Number of Clusters\", y=\"Inertia\")\nplt.title(\"Number of Clusters vs Inertia\")\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"Inertia\")\n\nplt.axvline(x=5, color='r', linestyle='--', label='Elbow Point')\n\n\nThe 'Elbow' of the graph is: 3\n\n\n&lt;matplotlib.lines.Line2D at 0x11bf53f90&gt;\n\n\n\n\n\n\n\nCode\nimport seaborn as sns\nplt.figure(figsize=(4, 1.5))\nsns.lineplot(df, x=\"Number of Clusters\", y=\"Inertia\", color=\"orange\")\nplt.title(\"Inertia\")\nplt.xlabel(\"Number of Clusters\")\n\nplt.figure(figsize=(4, 1.5))\nsns.lineplot(df, x=\"Number of Clusters\", y=\"Distortion\", color=\"green\")\nplt.title(\"Distortion\")\nplt.xlabel(\"Number of Clusters\")\n\n\nText(0.5, 0, 'Number of Clusters')\n\n\n\n\n\n\n\n\nBased on the graphs above, we can determine that the optimal number of clusters to use is 5.\n\n\nK-Means Silhouette Score\n\n\nCode\nimport sklearn.cluster\n\ndef maximize_silhouette(X,algo=\"birch\",nmax=20,i_plot=False):\n\n    i_print=False\n\n    X=np.ascontiguousarray(X) \n\n    params=[]; sil_scores=[]\n    sil_max=-10\n    for param in range(2,nmax+1):\n        if(algo==\"birch\"):\n            model = sklearn.cluster.Birch(n_clusters=param).fit(X)\n            labels=model.predict(X)\n\n        if(algo==\"ag\"):\n            model = sklearn.cluster.AgglomerativeClustering(n_clusters=param).fit(X)\n            labels=model.labels_\n\n        if(algo==\"dbscan\"):\n            param=0.5*(param-1)\n            model = sklearn.cluster.DBSCAN(eps=param).fit(X)\n            labels=model.labels_\n\n        if(algo==\"kmeans\"):\n            model = sklearn.cluster.KMeans(n_clusters=param).fit(X)\n            labels=model.predict(X)\n\n        try:\n            sil_scores.append(sklearn.metrics.silhouette_score(X,labels))\n            params.append(param)\n        except:\n            continue \n\n        if(i_print): print(param,sil_scores[-1])\n        \n        if(sil_scores[-1]&gt;sil_max):\n             opt_param=param\n             sil_max=sil_scores[-1]\n             opt_labels=labels\n\n    print(\"-----------------\")\n    print(\"OPTIMAL PARAMETER =\",opt_param)\n    print(\"-----------------\")\n\n    if(i_plot):\n        fig, ax = plt.subplots()\n        ax.plot(params, sil_scores, \"-o\")  \n        ax.set(xlabel='Hyper-parameter', ylabel='Silhouette')\n        plt.show()\n\n    return opt_labels\n\n\n\n\nCode\ndef plot(X,color_vector):\n    fig, ax = plt.subplots()\n    jitter = 0.01 * np.random.randn(*X.shape)\n    X += jitter \n    ax.scatter(X.iloc[:,0], X.iloc[:,1],c=color_vector, cmap=\"viridis\")\n    ax.set(xlabel='Feature-1 (x_1)', ylabel='Feature-2 (x_2)',\n    title='Cluster data')\n    ax.grid()\n    plt.show()\n\n\n\n\nCode\nimport sklearn.cluster\nopt_labels=maximize_silhouette(X,algo=\"kmeans\",nmax=50, i_plot=True)\nplot(X,opt_labels)\n\n\n-----------------\nOPTIMAL PARAMETER = 31\n-----------------\n\n\n\n\n\n\n\n\n\n\nDBSCAN\n\n\nCode\nopt_labels=maximize_silhouette(X,algo=\"dbscan\",nmax=15, i_plot=True)\nplot(X,opt_labels)\n\n\n-----------------\nOPTIMAL PARAMETER = 0.5\n-----------------\n\n\n\n\n\n\n\n\n\n\nAgglomerative Clustering\n\n\nCode\nopt_labels=maximize_silhouette(X,algo=\"ag\",nmax=50, i_plot=True)\nplot(X,opt_labels)\n\n\n-----------------\nOPTIMAL PARAMETER = 29\n-----------------\n\n\n\n\n\n\n\n\n\n\nCode\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\nlinkage_matrix = linkage(X_norm, method=\"ward\")\ndendrogram(linkage_matrix)\nplt.title('Dendrogram')\n\nplt.show()\n\n\n\n\n\n\n\nFinal Results\n\n\nCode\nimport sklearn.cluster\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef maximize_silhouette(X, algo=\"birch\", nmax=20, i_plot=False):\n    X = np.ascontiguousarray(X)\n    opt_param = 31\n\n    if algo == \"birch\":\n        model = sklearn.cluster.Birch(n_clusters=opt_param).fit(X)\n        labels = model.predict(X)\n\n    elif algo == \"ag\":\n        model = sklearn.cluster.AgglomerativeClustering(n_clusters=opt_param).fit(X)\n        labels = model.labels_\n\n    elif algo == \"dbscan\":\n        param = 0.5 * (opt_param - 1)\n        model = sklearn.cluster.DBSCAN(eps=param).fit(X)\n        labels = model.labels_\n\n    elif algo == \"kmeans\":\n        model = sklearn.cluster.KMeans(n_clusters=opt_param).fit(X)\n        labels = model.predict(X)\n\n    try:\n        sil_score = sklearn.metrics.silhouette_score(X, labels)\n    except:\n        sil_score = None\n\n    print(\"-----------------\")\n    print(\"OPTIMAL PARAMETER =\", opt_param)\n    print(\"-----------------\")\n\n\n    return labels\n\n\n\n\nCode\nagg_cluster = AgglomerativeClustering(n_clusters=5)\nagg_labels = agg_cluster.fit_predict(X_norm)\nplt.scatter(X_norm[:, 0], X_norm[:, 1], c=agg_labels, cmap=\"viridis\", edgecolors=\"k\")\nplt.title(\"Agglomerative Clustering\")\n\n\nText(0.5, 1.0, 'Agglomerative Clustering')\n\n\n\n\n\n\n\nCode\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\nlinkage_matrix = linkage(X_norm, method=\"ward\")\ndendrogram(linkage_matrix)\nplt.title('Dendrogram')\n\nplt.show()\n\n\n\n\n\n\nKMeans\n\n\nCode\nk=5\nkmeans = KMeans(n_clusters=k)\nkmeans.fit(X)\n\nlabels = kmeans.labels_\ncentroids = kmeans.cluster_centers_\n\nplt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=labels, cmap='viridis', alpha=0.5)\nplt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200)\nplt.title('K-Means Clustering')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.show()\n\n\n\n\n\n\n\nDBSCAN\n\n\nCode\ndbscan = DBSCAN(eps=0.5, min_samples=5)\nlabels = dbscan.fit_predict(X)\n\nplt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=labels, cmap='viridis', alpha=0.5)\nplt.title('DBSCAN Clustering')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.show()"
  },
  {
    "objectID": "tabs/clustering/clustering.html#results",
    "href": "tabs/clustering/clustering.html#results",
    "title": "Clustering",
    "section": "Results",
    "text": "Results\nAgglomerative Clustering was the most successful clustering methods as it produced distinct clusters. K-Means and DBSCAN were somewhat successful but the clusters weren’t well defined and the graphs were hard to read. All methods were easy to implement but for this data, agglometative clustering is definitely the best option. I would say that based on the agglomerative clustering results, we can learn that clusters are based off of injury prevention methods that athletes took. Points within similar clusters represent athletes who took similar measures.\nThe optimal number of clusters was determined to be 5. This makes sense based on the nature of the features. There were 5 prevention measures that were very commonly used, which is illustrated in the plot below. I think we can perhaps make a connection between the optimal number of clusters and the years of football experience that an athlete has, though, as there are many labels in the dataset, it is hard to be definitive about this.\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npm_counts = X.sum()\npm_counts = pm_counts.sort_values(ascending=False)\n\nsns.barplot(x=pm_counts.index, y=pm_counts.values, color='skyblue')\nplt.title('Prevention Measures Counts')\nplt.xlabel('Prevention Measures')\nplt.ylabel('Count')\nplt.xticks(rotation=45, ha='right')  \nplt.tight_layout()"
  },
  {
    "objectID": "tabs/clustering/clustering.html#conclusions",
    "href": "tabs/clustering/clustering.html#conclusions",
    "title": "Clustering",
    "section": "Conclusions",
    "text": "Conclusions\nThe exploration of this soccer injuries data set revealed that among the clustering algorithms tested, Agglomerative Clustering emerged as the most effective method. It successfully delineated distinct clusters, offering a clear representation of athletes who implemented similar injury prevention measures.\nThe determination of an optimal number of clusters, set at 5, aligns well with the nature of the dataset. The five prevalent prevention measures identified in the clusters mirror the commonly used strategies among athletes. This finding underscores the practicality of Agglomerative Clustering in uncovering patterns related to injury prevention strategies.\nFurthermore, the potential correlation between the optimal number of clusters and an athlete’s years of football experience hints at a nuanced relationship worth exploring further. While the dataset contains numerous labels, making it challenging to definitively establish this connection, it opens avenues for future research and investigation.\nOverall, the success of Agglomerative Clustering in providing clear and meaningful clusters emphasizes its applicability in understanding patterns of injury prevention among athletes. These insights contribute not only to the field of sports science but also hold relevance for real-life scenarios, potentially informing coaching strategies, training programs, and injury mitigation efforts in sports. As I continue to delve into the intricate details of athletes’ choices in injury prevention, Agglomerative Clustering stands out as a valuable tool for bringing to light meaningful patterns that can positively impact the health of athletes."
  },
  {
    "objectID": "tabs/about/about.html",
    "href": "tabs/about/about.html",
    "title": "About Me!",
    "section": "",
    "text": "Renee DeMaio (she/her) is a fourth year undergraduate student and first year graduate student at Georgetown University. She is pursuing her undergraduate degree in Chinese with a minor in mathematics and her masters in Data Science and Analytics. Renee grew up in Hong Kong and Singapore, but calls Hong Kong home, as she lived there from for most of her adolescence and her family still resides in Hong Kong. At the age of 18, she moved to the United States for the first time to attend university.\n\n\n\nRenee’s undergraduate academic interests include Chinese elite sport. Her undergraduate thesis is about how nationalism changed the nature of sport in China from one filled with pride and a focus on development, to one where the need for success and to prove China’s strength on the international stage has created harsh training and competing conditions for athletes. In her graduate studies, she is interested in artificial intelligence as well as biostatistics. She hopes to be able to further pursue both of these topics more extensively as she begins to pursue her master’s degree full time in the fall of 2024. She hopes to eventually get a data science related job in either the sports world or the medical world.\nRenee has a few different work experiences. First, she worked for four years as a private mathematics tutor for primary, secondary, and university level students focusing on all topics from basic operations to calculus. Next, since April of 2021 she has been working as a student equipment manager in the Georgetown University Athletics Department Equipment Room. She manages day to day operations for all 30 varsity athletics teams including ordering athletic gear, laundry services, and assisting athletes with whatever they may need. She also manages all of the other student employees. Finally, she works for the Splash Foundation in Hong Kong. Splash is a non-profit organization that teaches migrant domestic workers, kids with special needs, and kids from low-income backgrounds how to swim for free. She has only coaches for the adult programme. When she returns to Hong Kong, she works as a lead coach where she leads lessons for 10-12 students along 1-3 other coaches. Through 12 week programmes, swimmers will go from never having been in the water before, and sometimes being immensely afraid of the water, to knowing how to swim freestyle and float on their front and their back.\nOutside of school, Renee spends her time dancing, reading, and running. She is on two dance teams at Georgetown University, Ritmo y Sabor and Groove Theory. She has completed two half marathons, which were the National Women’s Half Marathon in 2022 and 2023. Her favourite books are below!\n\nFavourite Books:\n\nCrying at H Mart by Michelle Zauner\nSidelined: Sports, Culture, and Being a Woman in America by Julie Dicaro\nKillers of a Certain Age by Deanna Raybourn\nJoan is Okay by Weike Wang\nIf He Had Been with Me by Laura Nowlin\nHappy Place by Emily Henry"
  },
  {
    "objectID": "tabs/naive_bayes/naive_bayes.html",
    "href": "tabs/naive_bayes/naive_bayes.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "The Naive Bayes classifier is a supervised machine learning algorithm that is used for classification tasks, such as text classification. The goal of the Naive Bayes classifier is to classify data points into predefined categories, also called classes, by estimating the probability of the data point having each class using a predetermined set of features. At the core of Naive Bayes classifier is Bayes Theorem, a statistical tool that describes the probability of an event with knowledge of conditions that might influence the outcome of the event. When applied in Naive Bayes, it predicts the probability of a given object having a specific class, based on the observed features. The ‘naive’ aspect of Naive Bayes comes from the classifier’s assumption that all the data is independent.\nThere are different versions of Naive Bayes including Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes. The differences between each version is that they are all defined to accommodate different types of data and be used in different scenarios, so it is important to select the variant that best suits your data. Gaussian Naive Bayes is best suited for continuous data whose features are assumed to follow a normal distribution. This variant is best suited for data that consists of continuous, numeric features. Multinomial Naive Bayes is best suited for text or data represented as counts. It is commonly used in text classification, for example in sentiment analysis. Finally, Bernoulli Naive Bayes works best with binary data whose features are binary variables. It is often used in document classification tasks such as spam detection."
  },
  {
    "objectID": "tabs/naive_bayes/naive_bayes.html#introduction",
    "href": "tabs/naive_bayes/naive_bayes.html#introduction",
    "title": "Naive Bayes",
    "section": "",
    "text": "The Naive Bayes classifier is a supervised machine learning algorithm that is used for classification tasks, such as text classification. The goal of the Naive Bayes classifier is to classify data points into predefined categories, also called classes, by estimating the probability of the data point having each class using a predetermined set of features. At the core of Naive Bayes classifier is Bayes Theorem, a statistical tool that describes the probability of an event with knowledge of conditions that might influence the outcome of the event. When applied in Naive Bayes, it predicts the probability of a given object having a specific class, based on the observed features. The ‘naive’ aspect of Naive Bayes comes from the classifier’s assumption that all the data is independent.\nThere are different versions of Naive Bayes including Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes. The differences between each version is that they are all defined to accommodate different types of data and be used in different scenarios, so it is important to select the variant that best suits your data. Gaussian Naive Bayes is best suited for continuous data whose features are assumed to follow a normal distribution. This variant is best suited for data that consists of continuous, numeric features. Multinomial Naive Bayes is best suited for text or data represented as counts. It is commonly used in text classification, for example in sentiment analysis. Finally, Bernoulli Naive Bayes works best with binary data whose features are binary variables. It is often used in document classification tasks such as spam detection."
  },
  {
    "objectID": "tabs/naive_bayes/naive_bayes.html#preparing-data-for-naive-bayes",
    "href": "tabs/naive_bayes/naive_bayes.html#preparing-data-for-naive-bayes",
    "title": "Naive Bayes",
    "section": "Preparing Data for Naive Bayes",
    "text": "Preparing Data for Naive Bayes\nIn order to carry out Naive Bayes, it is important that we preprocess our data. The data cleaning process has happened already and can be found within the data cleaning tab. Since our data is now prepared and cleaned, we need to separate it into training, validation, and testing sets. Training data is typically 80% of a data set and is the data that we give to the model so it can learn the existing releationships and methods that desired outcomes are predicted. The validation set is typically 10% of the data, and is given to the model to help fine tune the model’s hyperparameters and prevent overfitting. Finally, the remaining 10% of the data constitutes the test data, which is the data set given to the model to evaluate the model’s accuracy and performance. It is used to evaluate how well the model performs on unseen data."
  },
  {
    "objectID": "tabs/data_cleaning/data_cleaning.html",
    "href": "tabs/data_cleaning/data_cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "A lot of the time when we look for data on the internet, it is not going to be in the format that we need it to be in, in order to analyse it properly for our goals. Othertimes, the data set is incomplete including missing data points, has inconsistent formatting, or is in a format that does not work for smooth analysis. That is there data cleaning comes in. Cleaning our “dirty” data helps us ensure that the data is ready to be analyzed and that the data we do analyze is accurate and reliable.\nCommon data cleaning tasks for tabular data include deciding how to deal with missing values, duplicate rows or entries, incorrect values, special characters, etc. Additionally, it involves accounting for inconsistent casting (having inconsistent instances of upper and lowercase letters), spelling errors, human errors, inconsistent units, and more.\nWe will know that a tabular data set is clean when every column is a variable, every row is an observation,and every call is a value.\nWhen we clean text data, we often us Natural Language Processing, or NLP. Common cleaning tasks include removing punctuation, dealing with special characters, tokenization, vectorization, stop word removal, lemmatization or stemming, spell check, removing URLs, and more.\nTo help us clean data, we can use R packages such as Tidyverse, dplyr, forcats, tidyr, etc. For Python, we can use packages such as Pandas, RegEx, and NLTK. I utilize Pandas quite a bit in the data cleaning process. I use both R and Python to clean my data."
  },
  {
    "objectID": "tabs/data_cleaning/data_cleaning.html#introduction",
    "href": "tabs/data_cleaning/data_cleaning.html#introduction",
    "title": "Data Cleaning",
    "section": "",
    "text": "A lot of the time when we look for data on the internet, it is not going to be in the format that we need it to be in, in order to analyse it properly for our goals. Othertimes, the data set is incomplete including missing data points, has inconsistent formatting, or is in a format that does not work for smooth analysis. That is there data cleaning comes in. Cleaning our “dirty” data helps us ensure that the data is ready to be analyzed and that the data we do analyze is accurate and reliable.\nCommon data cleaning tasks for tabular data include deciding how to deal with missing values, duplicate rows or entries, incorrect values, special characters, etc. Additionally, it involves accounting for inconsistent casting (having inconsistent instances of upper and lowercase letters), spelling errors, human errors, inconsistent units, and more.\nWe will know that a tabular data set is clean when every column is a variable, every row is an observation,and every call is a value.\nWhen we clean text data, we often us Natural Language Processing, or NLP. Common cleaning tasks include removing punctuation, dealing with special characters, tokenization, vectorization, stop word removal, lemmatization or stemming, spell check, removing URLs, and more.\nTo help us clean data, we can use R packages such as Tidyverse, dplyr, forcats, tidyr, etc. For Python, we can use packages such as Pandas, RegEx, and NLTK. I utilize Pandas quite a bit in the data cleaning process. I use both R and Python to clean my data."
  },
  {
    "objectID": "tabs/data_cleaning/data_cleaning.html#record-data-cleaning",
    "href": "tabs/data_cleaning/data_cleaning.html#record-data-cleaning",
    "title": "Data Cleaning",
    "section": "Record Data Cleaning",
    "text": "Record Data Cleaning\nThe process of cleaning my record data varied from data set to data set. Some of them were very simple to clean and some required some more time and effort. It all depends on the state of the data set when I found it and what I am desiring it to do.\n\nInjury Prevention Factors\nThis data set was relatively clean to begin with, so I did not have to do a big amount of cleaning.\nRaw Data:\n\n\nCode\nimport pandas as pd\n\nfile_path = \"../../../../data/00-raw-data/Data_Injury_Prevention.csv\"\nraw_injury_prevention = pd.read_csv(file_path)\n\nraw_injury_prevention.head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 1\n\n\n\n\nID\nAge\nHeight\nMass\nTeam\nPosition\nYears of Football Experience\nPrevious Injuries (1=yes, 0=no)\nNumber of Injuries\nAnkle Injuries (1= yes, 0=no)\nNumber of Ankle Injuries\nSevere_Ankle_Injuries (1=yes, 0=no)\nNoncontact_Ankle_Injuries ((1=yes, 0=no)\nKnee Injuries (1=yes, 0=no)\nNumber of Knee Injuries\nSevere_Knee_Injuries(1=yes, 0=no)\nNoncontact_Knee_Injuries(1=yes, 0=no)\nThigh_Injuries(1=yes, 0=no)\nNumber of Thigh Injuries\nSevere_Thigh_Injuries (1=yes, 0=no)\nNoncontact_Thigh_Injuries(1=yes, 0=no)\nRisk Factor Condition (1=yes, 0=no)\nRisk Factor Coordination (1=yes, 0=no)\nRisk Factor Muscle Impairments (1=yes, 0=no)\nRisk Factor Fatigue (1=yes, 0=no)\nRisk Factor Previous Injury(1=yes, 0=no)\nRisk Factor Attentiveness (1=yes, 0=no)\nRisk Factor Other Player (1=yes, 0=no)\nRisk Factor Equipment(1=yes, 0=no)\nRisk Factor Climatic Condition (1=yes, 0=no)\nRisk Factor Diet (1=yes, 0=no)\nImportance Injury Prevention\nKnowledgeability (1=yes, 2=no)\nPrevention Measure Stretching (1=yes, 0=no)\nPrevention Measure Warm Up (1=yes, 0=no)\nPrevention Measure Specific Strength Exercises (1=yes, 0=no)\nPrevention Measure Bracing (1=yes, 0=no)\nPrevention Measure Taping (1=yes, 0=no)\nPrevention Measure Shoe Insoles (1=yes, 0=no)\nPrevention Measure Face Masks (1=yes, 0=no)\nPrevention Measure Medical Corset (1=yes, 0=no)\n\n\n146.00\n19.00\n173.00\n67.60\n1.00\n3.00\n1.00\n1.00\n6.00\n1.00\n3.00\n0.00\n1.00\n0.00\n0.00\n0.00\n0.00\n1.00\n3.00\n0.00\n1.00\n0.00\n0.00\n0.00\n1.00\n0.00\n0.00\n0.00\n1.00\n0.00\n0.00\n2.00\n1.00\n1.00\n0.00\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n155.00\n22.00\n179.50\n71.00\n1.00\n3.00\n1.00\n1.00\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.00\n2.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.00\n1.00\n1.00\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n160.00\n22.00\n175.50\n71.80\n1.00\n3.00\n1.00\n1.00\n7.00\n1.00\n4.00\n1.00\n1.00\n0.00\n0.00\n0.00\n0.00\n1.00\n3.00\n1.00\n1.00\n0.00\n0.00\n0.00\n1.00\n0.00\n0.00\n1.00\n0.00\n0.00\n0.00\n1.00\n1.00\n1.00\n0.00\n0.00\n0.00\n0.00\n1.00\n0.00\n0.00\n\n\n164.00\n23.00\n190.00\n80.50\n1.00\n4.00\n1.00\n1.00\n1.00\n0.00\n0.00\n0.00\n0.00\n1.00\n1.00\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.00\n1.00\n1.00\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.00\n1.00\n1.00\n1.00\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n\n\n\n\n\nThe cleaning process involved mainly cleaning the column names, and ensuring that the format of the data in each column was consistent. I didn’t subset the data any further to allow for options when I moved onto my analysis.\nI cleaned this data set using R. The code can be found in here and the final cleaned data set can be found below.\n\n\nCode\nimport pandas as pd\nfile_path = \"../../../../data/01-modified-data/injury_prevention_data_soccer.csv\"\ninjury_prevention_factors = pd.read_csv(file_path)\ninjury_prevention_factors.head()\n\n\n\n\n\n\n\n\n\nID\nAge\nHeight\nMass\nTeam\nPosition\nYears of Football Experience\nPrevious Injuries\nNumber of Injuries\nAnkle Injuries\n...\nImportance Injury Prevention\nKnowledgeability\nPrevention Measure Stretching\nPrevention Measure Warm Up\nPrevention Measure Specific Strength Exercises\nPrevention Measure Bracing\nPrevention Measure Taping\nPrevention Measure Shoe Insoles\nPrevention Measure Face Masks\nPrevention Measure Medical Corset\n\n\n\n\n0\n146\n19\n173.0\n67.6\n1\n3\n1\nyes\n6\nyes\n...\n2\n1\nyes\nno\nyes\nno\nno\nno\nno\nno\n\n\n1\n155\n22\n179.5\n71.0\n1\n3\n1\nyes\n2\nno\n...\n1\n1\nyes\nyes\nno\nno\nno\nno\nno\nno\n\n\n2\n160\n22\n175.5\n71.8\n1\n3\n1\nyes\n7\nyes\n...\n1\n1\nyes\nno\nno\nno\nno\nyes\nno\nno\n\n\n3\n164\n23\n190.0\n80.5\n1\n4\n1\nyes\n1\nno\n...\n1\n1\nyes\nyes\nyes\nno\nno\nno\nno\nno\n\n\n4\n145\n19\n173.5\n68.7\n1\n3\n1\nyes\n2\nyes\n...\n1\n2\nyes\nyes\nno\nno\nyes\nno\nno\nno\n\n\n\n\n5 rows × 41 columns\n\n\n\n\n\nInjuries from Different Activities Data Set\nThis data set required a little more cleaning. The formatting of the data set was very messy and not conducive to analysis at the beginning, so my main goals were to ensure that each age group was a column, and each activity was a different row.\n\n\nCode\nfile_path = \"../../../../data/00-raw-data/sports_injuries_data.csv\"\ninjury_causes_dirty = pd.read_csv(file_path)\ninjury_causes_dirty.head()\n\n\n\n\n\n\n\n\n\nX1\nX2\nX3\nX4\nX5\nX6\nX7\nX8\nX9\nX10\n...\nX195\nX196\nX197\nX198\nX199\nX200\nX201\nX202\nX203\nX204\n\n\n\n\n0\nNumber of injuries by age \\n \\nSport, activity...\nNaN\nNaN\nNaN\nNumber of injuries by age\nNumber of injuries by age\nNumber of injuries by age\nNaN\nSport, activity or equipment\nInjuries (1)\n...\n2,635\n3,261\n572.0\nNonpowder guns, BB'S, pellets\n11,603\n519.0\n3,286\n3,443\n3,869\n487.0\n\n\n1\nNaN\nNaN\nNaN\nNumber of injuries by age\nNumber of injuries by age\nNumber of injuries by age\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\nSport, activity or equipment\nInjuries (1)\nYounger than 5\n5 to 14\n15 to 24\n25 to 64\n65 and older\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\nExercise, exercise equipment\n445,642\n6,662\n36,769\n91,013\n229,640\n81,558\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nBicycles and accessories\n405,411\n13,297\n91,089\n50,863\n195,030\n55,132\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 204 columns\n\n\n\nIn order to turn the above data set into the below one, I first had to read the data in using the read_html function in R, then went on to make a table from the table on the website and write it into a csv file. I removed the first row of the data frame which allowed me to then redefine the columns which fixed the format of the data. I then removed the first row and selected the first seven columns to end up with the data set below. The cleaning was done using R and the coding process can be seen here.\n\n\nCode\nfile_path = \"../../../../data/01-modified-data/sports_injury_data.csv\"\ninjury_causes = pd.read_csv(file_path)\ninjury_causes.head()\n\n\n\n\n\n\n\n\n\nSport, activity or equipment\nInjuries (1)\nYounger than 5\n5 to 14\n15 to 24\n25 to 64\n65 and older\n\n\n\n\n0\nExercise, exercise equipment\n445,642\n6,662\n36,769\n91,013\n229,640\n81,558\n\n\n1\nBicycles and accessories\n405,411\n13,297\n91,089\n50,863\n195,030\n55,132\n\n\n2\nBasketball\n313,924\n1,216\n109,696\n143,773\n57,413\n1,825\n\n\n3\nFootball\n265,747\n581\n145,499\n100,760\n18,527\n381\n\n\n4\nATV's, mopeds, minibikes, etc.\n242,347\n3,688\n42,069\n61,065\n122,941\n12,584\n\n\n\n\n\n\n\n\n\nNBA Injury Data\nIn its raw form, the NBA data set is not extremely dirty, but there is a lot of work that needs to be done on it. As seen in the acquired column, there are many missing values, as well as very inconsistent formatting in the notes tab. I cleaned this data set using Python and my code can be found here.\nRaw data:\n\n\nCode\nfile_path = \"../../../../data/00-raw-data/injuries_2010-2020.csv\"\nnba_injuries = pd.read_csv(file_path)\nnba_injuries.head()\n\n\n\n\n\n\n\n\n\nDate\nTeam\nAcquired\nRelinquished\nNotes\n\n\n\n\n0\n2010-10-03\nBulls\nNaN\nCarlos Boozer\nfractured bone in right pinky finger (out inde...\n\n\n1\n2010-10-06\nPistons\nNaN\nJonas Jerebko\ntorn right Achilles tendon (out indefinitely)\n\n\n2\n2010-10-06\nPistons\nNaN\nTerrico White\nbroken fifth metatarsal in right foot (out ind...\n\n\n3\n2010-10-08\nBlazers\nNaN\nJeff Ayres\ntorn ACL in right knee (out indefinitely)\n\n\n4\n2010-10-08\nNets\nNaN\nTroy Murphy\nstrained lower back (out indefinitely)\n\n\n\n\n\n\n\nThe acquired and relinquished columns are related. When one player ise relinquished, another player is acquired. Since I am not interested in acquired players, I dropped that column. I also extracted any information in the parenthesis in the notes to explore the nature of the injury and how long the player would be out of the game. I pulled that into a separate column. Finally, I made sure that the formatting of the rest of the columns are consistent.\n\n\nCode\nfile_path = \"../../../../data/01-modified-data/basketball_injury_data.csv\"\nnba_injuries_cleaned = pd.read_csv(file_path)\nnba_injuries_cleaned.head()\n\n\n\n\n\n\n\n\n\nDate\nTeam\nRelinquished\nNotes\nInjuryStatus\n\n\n\n\n0\n2010-10-03\nBulls\nCarlos Boozer\nfractured bone in right pinky finger\nout indefinitely\n\n\n1\n2010-10-06\nPistons\nJonas Jerebko\ntorn right Achilles tendon\nout indefinitely\n\n\n2\n2010-10-06\nPistons\nTerrico White\nbroken fifth metatarsal in right foot\nout indefinitely\n\n\n3\n2010-10-08\nBlazers\nJeff Ayres\ntorn ACL in right knee\nout indefinitely\n\n\n4\n2010-10-08\nNets\nTroy Murphy\nstrained lower back\nout indefinitely\n\n\n\n\n\n\n\n\n\nNFL Concussion Data\nThis data set was relatively clean to begin with and was already suitable for my desired analysis. I simply had to drop any unnecessary columns In order to clean this data set, I dropped all unnecessary columns and made the format of the columns consistent by turning all numbers into floats. I also decided to drop all of the rows with NaN values because it is clear to me that there is data missing in those rows due to the story of the injury not making sense. For example, there was one row where someone had a pre-season injury so they did not miss any weeks, but they had missing values for both play time before and anfer the injury. I did that using Python and my code can be found here.\nRaw data:\n\n\nCode\nfile_path = \"../../../../data/00-raw-data/Concussion Injuries 2012-2014.csv\"\nconcussions_dirty = pd.read_csv(file_path)\nconcussions_dirty.head()\n\n\n\n\n\n\n\n\n\nID\nPlayer\nTeam\nGame\nDate\nOpposing Team\nPosition\nPre-Season Injury?\nWinning Team?\nWeek of Injury\nSeason\nWeeks Injured\nGames Missed\nUnknown Injury?\nReported Injury Type\nTotal Snaps\nPlay Time After Injury\nAverage Playtime Before Injury\n\n\n\n\n0\nAldrick Robinson - Washington Redskins vs. Tam...\nAldrick Robinson\nWashington Redskins\nWashington Redskins vs. Tampa Bay Buccaneers (...\n30/09/2012\nTampa Bay Buccaneers\nWide Receiver\nNo\nYes\n4\n2012/2013\n1\n1.0\nNo\nHead\n0\n14 downs\n37.00 downs\n\n\n1\nD.J. Fluker - Tennessee Titans vs. San Diego C...\nD.J. Fluker\nSan Diego Chargers\nTennessee Titans vs. San Diego Chargers (22/9/...\n22/09/2013\nTennessee Titans\nOffensive Tackle\nNo\nNo\n3\n2013/2014\n1\n1.0\nNo\nConcussion\n0\n78 downs\n73.50 downs\n\n\n2\nMarquise Goodwin - Houston Texans vs. Buffalo ...\nMarquise Goodwin\nBuffalo Bills\nHouston Texans vs. Buffalo Bills (28/9/2014)\n28/09/2014\nHouston Texans\nWide Receiver\nNo\nNo\n4\n2014/2015\n1\n1.0\nNo\nConcussion\n0\n25 downs\n17.50 downs\n\n\n3\nBryan Stork - New England Patriots vs. Buffalo...\nBryan Stork\nNew England Patriots\nNew England Patriots vs. Buffalo Bills (12/10/...\n12/10/2014\nBuffalo Bills\nCenter\nNo\nYes\n6\n2014/2015\n1\n1.0\nNo\nHead\n0\n82 downs\n41.50 downs\n\n\n4\nLorenzo Booker - Chicago Bears vs. Indianapoli...\nLorenzo Booker\nChicago Bears\nChicago Bears vs. Indianapolis Colts (9/9/2012)\n9/09/2012\nIndianapolis Colts\nRunning Back\nYes\nYes\n1\n2012/2013\n0\nNaN\nNo\nHead\n0\nDid not return from injury\nNaN\n\n\n\n\n\n\n\nThe below is the data set I used for exploratory data analysis.\nCleaned data:\n\n\nCode\nfile_path = \"../../../../data/01-modified-data/nfl_concussions.csv\"\nconcussions_cleaned = pd.read_csv(file_path)\nconcussions_cleaned.head()\n\n\n\n\n\n\n\n\n\nPlayer\nTeam\nDate\nOpposing Team\nPosition\nPre-Season Injury?\nWinning Team?\nWeek of Injury\nSeason\nWeeks Injured\nGames Missed\nUnknown Injury?\nReported Injury Type\nTotal Snaps\nPlay Time After Injury\nAverage Playtime Before Injury\n\n\n\n\n0\nAldrick Robinson\nWashington Redskins\n30/09/2012\nTampa Bay Buccaneers\nWide Receiver\nNo\nYes\n4.0\n2012/2013\n1.0\n1.0\nNo\nHead\n0.0\n14.0 downs\n37.00 downs\n\n\n1\nD.J. Fluker\nSan Diego Chargers\n22/09/2013\nTennessee Titans\nOffensive Tackle\nNo\nNo\n3.0\n2013/2014\n1.0\n1.0\nNo\nConcussion\n0.0\n78.0 downs\n73.50 downs\n\n\n2\nMarquise Goodwin\nBuffalo Bills\n28/09/2014\nHouston Texans\nWide Receiver\nNo\nNo\n4.0\n2014/2015\n1.0\n1.0\nNo\nConcussion\n0.0\n25.0 downs\n17.50 downs\n\n\n3\nBryan Stork\nNew England Patriots\n12/10/2014\nBuffalo Bills\nCenter\nNo\nYes\n6.0\n2014/2015\n1.0\n1.0\nNo\nHead\n0.0\n82.0 downs\n41.50 downs\n\n\n4\nDaniel Kilgore\nSan Francisco 49ers\n29/10/2012\nArizona Cardinals\nGuard\nNo\nYes\n8.0\n2012/2013\n1.0\n0.0\nNo\nConcussion\n1.0\n8.0 downs\n14.43 downs\n\n\n\n\n\n\n\n\n\nNFL Game Injury Data\nMy NFL game injury data set was composed of two separate data sets that you can see below. The first data set mainly contained information about the injuries that occured as well as the situation that they occured in. The second data set contains more specific information about the field, weather conditions of the game, and the player. These two data sets combined will help to paint a coherent picture of injuries in the NFL. I cleaned both of these data sets using Python and the code can be found here.\nThe raw data can be seen here:\n\n\nCode\nfile_path = \"../../../../data/00-raw-data/InjuryRecord.csv\"\ninjury_record = pd.read_csv(file_path)\n\nprint(\"INJURY RECORD DATA SET:\")\ninjury_record.head()\n\n\nINJURY RECORD DATA SET:\n\n\n\n\n\n\n\n\n\nPlayerKey\nGameID\nPlayKey\nBodyPart\nSurface\nDM_M1\nDM_M7\nDM_M28\nDM_M42\n\n\n\n\n0\n39873\n39873-4\n39873-4-32\nKnee\nSynthetic\n1\n1\n1\n1\n\n\n1\n46074\n46074-7\n46074-7-26\nKnee\nNatural\n1\n1\n0\n0\n\n\n2\n36557\n36557-1\n36557-1-70\nAnkle\nSynthetic\n1\n1\n1\n1\n\n\n3\n46646\n46646-3\n46646-3-30\nAnkle\nNatural\n1\n0\n0\n0\n\n\n4\n43532\n43532-5\n43532-5-69\nAnkle\nSynthetic\n1\n1\n1\n1\n\n\n\n\n\n\n\n\n\nCode\nfile_path = \"../../../../data/00-raw-data/PlayList.csv\"\nplay_list = pd.read_csv(file_path)\nprint(\"PLAY LIST DATA SET:\")\nplay_list.head()\n\n\nPLAY LIST DATA SET:\n\n\n\n\n\n\n\n\n\nPlayerKey\nGameID\nPlayKey\nRosterPosition\nPlayerDay\nPlayerGame\nStadiumType\nFieldType\nTemperature\nWeather\nPlayType\nPlayerGamePlay\nPosition\nPositionGroup\n\n\n\n\n0\n26624\n26624-1\n26624-1-1\nQuarterback\n1\n1\nOutdoor\nSynthetic\n63\nClear and warm\nPass\n1\nQB\nQB\n\n\n1\n26624\n26624-1\n26624-1-2\nQuarterback\n1\n1\nOutdoor\nSynthetic\n63\nClear and warm\nPass\n2\nQB\nQB\n\n\n2\n26624\n26624-1\n26624-1-3\nQuarterback\n1\n1\nOutdoor\nSynthetic\n63\nClear and warm\nRush\n3\nQB\nQB\n\n\n3\n26624\n26624-1\n26624-1-4\nQuarterback\n1\n1\nOutdoor\nSynthetic\n63\nClear and warm\nRush\n4\nQB\nQB\n\n\n4\n26624\n26624-1\n26624-1-5\nQuarterback\n1\n1\nOutdoor\nSynthetic\n63\nClear and warm\nPass\n5\nQB\nQB\n\n\n\n\n\n\n\nIn order to make my final data set, I first cleaned each set individually. For the first one, I simply had to drop the irrelevant columns, which I deamed to be every column starting with DM and the GameID and PlayKey columns. From the second data set, I also only had to drop excess columns. Finally, I merged both data sets together to end up with one bigger set. From there, I cleaned up the stadium type column by dealing with inconsistent spellings, dropped missing values, and dropped duplicate values.\nThe final data set can be found here:\n\n\nCode\nfile_path = \"../../../../data/01-modified-data/nfl_injuries.csv\"\nnfl_injuries = pd.read_csv(file_path)\nnfl_injuries.head()\n\n\n\n\n\n\n\n\n\nPlayerKey\nBodyPart\nRosterPosition\nStadiumType\nFieldType\nTemperature\nWeather\n\n\n\n\n0\n39873\nKnee\nLinebacker\nindoor\nSynthetic\n85\nMostly Cloudy\n\n\n1\n39873\nKnee\nLinebacker\noutdoor\nNatural\n82\nSunny\n\n\n2\n39873\nKnee\nLinebacker\nindoor\nSynthetic\n84\nCloudy\n\n\n3\n39873\nKnee\nLinebacker\nretractable roof\nSynthetic\n78\nPartly Cloudy\n\n\n4\n39873\nKnee\nLinebacker\noutdoor\nNatural\n80\nCloudy"
  },
  {
    "objectID": "tabs/data_cleaning/data_cleaning.html#text-data-cleaning",
    "href": "tabs/data_cleaning/data_cleaning.html#text-data-cleaning",
    "title": "Data Cleaning",
    "section": "Text Data Cleaning",
    "text": "Text Data Cleaning\n\nNews Data Cleaning\nMy news data came from the NewsAPI. In order to get this data clean, I first had to extract the data from the website, then I could clean up the punctuation, convert the data to a data frame, rename the columns, and create a description of each article. I also removed stop words to make the data better suited for analysis. It is important to note that this data set will be different every time my cleaning code is run, because the news will be different at any given day. I updated this data set as I am writing this today on December 4, 2023. My cleaning code can be found here.\nThe cleaned data set can be found here:\n\n\nCode\nfile_path = \"../../../../codes/01-data-gathering/cleaned_news_data.csv\"\ncleaned_news = pd.read_csv(file_path)\ncleaned_news.head()\n\n\n\n\n\n\n\n\n\nsource\nauthor\npublish_date\ncombined_t&d\n\n\n\n\n0\ncnet\nkevin lynch\n2023-11-07T14:45:04Z\nwatch champions league soccer: livestream boru...\n\n\n1\nespn\ncesar hernandez\n2023-11-12T01:57:26Z\nrapinoe's career ends with injury in title gam...\n\n\n2\ncnet\nkevin lynch\n2023-11-25T09:30:05Z\nman city vs liverpool livestream: how to watch...\n\n\n3\ncnet\nadam oram\n2023-11-25T12:00:04Z\nnewcastle vs chelsea livestream: how to watch ...\n\n\n4\ncnet\nkevin lynch\n2023-11-07T15:45:05Z\nwatch champions league soccer: livestream shak...\n\n\n\n\n\n\n\n\n\nInjury Prevention Text Data\nMy last data set is one that I created to be a text data version of my injury prevention factors data set. I researched and wrote definitions for each injury prevention method and created a data frame for it. The original data set can be found below as well as the text data data set that I created. The code for the text data data set is also below.\n\n\nCode\nfile_path = \"../../../../data/01-modified-data/injury_prevention_data_soccer.csv\"\nsoccer_injury = pd.read_csv(file_path)\nprint(\"ORIGINAL INJURY PREVENTION FACTORS DATA SET:\")\nsoccer_injury.head()\n\n\nORIGINAL INJURY PREVENTION FACTORS DATA SET:\n\n\n\n\n\n\n\n\n\nID\nAge\nHeight\nMass\nTeam\nPosition\nYears of Football Experience\nPrevious Injuries\nNumber of Injuries\nAnkle Injuries\n...\nImportance Injury Prevention\nKnowledgeability\nPrevention Measure Stretching\nPrevention Measure Warm Up\nPrevention Measure Specific Strength Exercises\nPrevention Measure Bracing\nPrevention Measure Taping\nPrevention Measure Shoe Insoles\nPrevention Measure Face Masks\nPrevention Measure Medical Corset\n\n\n\n\n0\n146\n19\n173.0\n67.6\n1\n3\n1\nyes\n6\nyes\n...\n2\n1\nyes\nno\nyes\nno\nno\nno\nno\nno\n\n\n1\n155\n22\n179.5\n71.0\n1\n3\n1\nyes\n2\nno\n...\n1\n1\nyes\nyes\nno\nno\nno\nno\nno\nno\n\n\n2\n160\n22\n175.5\n71.8\n1\n3\n1\nyes\n7\nyes\n...\n1\n1\nyes\nno\nno\nno\nno\nyes\nno\nno\n\n\n3\n164\n23\n190.0\n80.5\n1\n4\n1\nyes\n1\nno\n...\n1\n1\nyes\nyes\nyes\nno\nno\nno\nno\nno\n\n\n4\n145\n19\n173.5\n68.7\n1\n3\n1\nyes\n2\nyes\n...\n1\n2\nyes\nyes\nno\nno\nyes\nno\nno\nno\n\n\n\n\n5 rows × 41 columns\n\n\n\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd \nimport matplotlib as plt\n\nfile_path = \"../../../../data/01-modified-data/injury_prevention_data_soccer.csv\"\nsoccer_injury = pd.read_csv(file_path)\nsoccer_injury.columns = soccer_injury.columns.str.strip()\nsoccer_injury = soccer_injury.replace({\"yes\":1, \"no\":0})\n\ncolumns_keep = ['Prevention Measure Stretching', 'Prevention Measure Warm Up',\n       'Prevention Measure Specific Strength Exercises',\n       'Prevention Measure Bracing', 'Prevention Measure Taping',\n       'Prevention Measure Shoe Insoles', 'Prevention Measure Face Masks',\n       'Prevention Measure Medical Corset', \"Number of Injuries\"]\ndf_features = soccer_injury[columns_keep]\ndf_features.columns\n\nprevention_stretching = \"Athletes can help protect themselves by preparing before and after a game or practice session by warming up muscles and then stretching. Exercises can include forward lunges, side lunges, standing quad stretch, seated straddle lotus, seated side straddle, seated toe touch, and the knees to chest stretch. Hold each stretch for 20 seconds. \"\nprevention_warm_up = \"Warming up involves increasing the body's core temperature, heart rate, respiratory rate, and the body's muscle temperatures. By increasing muscle temperature, muscles become more loose and pliable, and by increasing heart rate and resipiatory rate, blood flood increases which helps to increase delivery of oxygen and nutrients to muscles. Warm up exercises include dynamic stretches, light bike riding, light jogging, jumproping, etc. \"\nprevention_specific_strength = \"Specific strength exercises can refer to a wide range of exercises, from which a few are selected based each athlete. These exercises can range from muscle group specific weight-lifting exercises, to physical therapy-like exercises. These exercises depend on each athlete and are hard to define. \"\nprevention_bracing = \"Basic braces provide general support and compression to specific areas of the body. More complex braces can do the same things, as well as promote healing, necessarily restrict movement, take weight off of an injury, etc. \"\nprevention_taping = \"Taping can be used to reduce the range of motion at a joint and decrease swelling, which in turn can alleviate pain and prevent further injury. \"\nprevention_shoe_insoles = \"Orthotics can alignment of an athlete's feet, ankles, knees, hips and back which can help prevent injuries. They can also absorb shock from impact of running to reduce stress on the athlete's joints and tissues. \"\nprevention_face_masks = \"Athletic face masks can be used to protect maxillary, nasal, zygomatic and orbital injuries. These are worn in sports where a face injury could possible occur. \"\nprevention_medical_corset = \"A medical corset is a corset that can be worn to help an athlete stablize their spine after a fracture or surgery. It will remind the athlete to not move in certain directors or to move more slowly to prevent causing further injury. \"\n\nprevention_definitions = {\n    \"Prevention Measure Stretching\": prevention_stretching,\n    \"Prevention Measure Warm Up\": prevention_warm_up,\n    'Prevention Measure Specific Strength Exercises': prevention_specific_strength,\n    'Prevention Measure Bracing': prevention_bracing, \n    'Prevention Measure Taping': prevention_taping,\n    'Prevention Measure Shoe Insoles': prevention_shoe_insoles, \n    'Prevention Measure Face Masks': prevention_face_masks,\n    'Prevention Measure Medical Corset': prevention_medical_corset\n}\n\ncols_keep = ['Number of Injuries', 'Prevention Measure Stretching', 'Prevention Measure Warm Up',\n       'Prevention Measure Specific Strength Exercises',\n       'Prevention Measure Bracing', 'Prevention Measure Taping',\n       'Prevention Measure Shoe Insoles', 'Prevention Measure Face Masks',\n       'Prevention Measure Medical Corset']\nprevention_cols = ['Prevention Measure Stretching', 'Prevention Measure Warm Up',\n       'Prevention Measure Specific Strength Exercises',\n       'Prevention Measure Bracing', 'Prevention Measure Taping',\n       'Prevention Measure Shoe Insoles', 'Prevention Measure Face Masks',\n       'Prevention Measure Medical Corset']\n\n\nsi_subset = soccer_injury[cols_keep]\nsi_subset_prevention = si_subset[prevention_cols]\nsi_subset_prevention.head()\n\nsi_subset[\"Prevention Measures Definitions\"] = \"\"\n\nfor col in si_subset.columns:\n    if col.startswith(\"Prevention\"):\n        si_subset[\"Prevention Measures Definitions\"] += si_subset[col].apply(\n            lambda x: prevention_definitions[col] if x == 1 else \"\"\n        )\n\nprint(\"INJURY PREVENTION FACTORS TEXT DATA DATA SET:\")\nsi_subset.head()\n\n\nINJURY PREVENTION FACTORS TEXT DATA DATA SET:\n\n\n\n\n\n\n\n\n\nNumber of Injuries\nPrevention Measure Stretching\nPrevention Measure Warm Up\nPrevention Measure Specific Strength Exercises\nPrevention Measure Bracing\nPrevention Measure Taping\nPrevention Measure Shoe Insoles\nPrevention Measure Face Masks\nPrevention Measure Medical Corset\nPrevention Measures Definitions\n\n\n\n\n0\n6\n1\n0\n1\n0\n0\n0\n0\n0\nAthletes can help protect themselves by prepar...\n\n\n1\n2\n1\n1\n0\n0\n0\n0\n0\n0\nAthletes can help protect themselves by prepar...\n\n\n2\n7\n1\n0\n0\n0\n0\n1\n0\n0\nAthletes can help protect themselves by prepar...\n\n\n3\n1\n1\n1\n1\n0\n0\n0\n0\n0\nAthletes can help protect themselves by prepar...\n\n\n4\n2\n1\n1\n0\n0\n1\n0\n0\n0\nAthletes can help protect themselves by prepar..."
  }
]