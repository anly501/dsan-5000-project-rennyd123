[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Homepage",
    "section": "",
    "text": "Hello! My name is Renee DeMaio (jrd154) and this is the homepage for my DSAN 5000 website which will be home to my research project about injuries and injury prevention in sports. Feel free to click through the tabs to see more of my work.\nTo learn more about me, click here."
  },
  {
    "objectID": "tabs/data_cleaning/data_cleaning.html",
    "href": "tabs/data_cleaning/data_cleaning.html",
    "title": "General Injury Data",
    "section": "",
    "text": "Injury & Injury Prevention Data Soccer\nIn order to clean this data set, I dropped all unnecessary columns, added binary values to columns that expressed binary data, and renamed columns appropriately.\nThe cleaned data can be found here and the cleaning code can be found here..\n\n\nInjury Data From Different Activites\nIn order to clean this data, I first had to reformat it into a tabular format and then had drop unnecessary columns.\nThe cleaned data can be found here and the cleaning code can be found here.\n\n\nNBA Data\n\nNBA Injury Data\nThe main cleaning that I had to do for this data set was subsetting to ensure that I had the columns that I needed. I dropped the “Acquired” column and then also split the “Notes” column into “Notes” and “Injury Status” which extracts information about the duration of the stint on the Injured List into a new column.\nThe link to my cleaned can be found here and the link my code can be found here.\n\n\nNBA Data\nThe cleaning done for this data set included joining all three data sets - the players, teams, and rankings data sets - together. I dropped unnecessary columns and filtered out NA values. The cleaned data set and associated code can be found below.\nThe cleaned data set can be found here and the cleaning code can be found here.\n\n\n\nNFL Data\n\nNFL Concussion Data\nThe cleaning done to this data set at this point has been minimal. I simply dropped unnecessary columns in order to make the dataset more suitable to my project.\nThe link to the cleaned data set is here and the link to the cleaning code is here.\n\n\nNFL Game Injury Data\nThe data cleaning for this data set involved merging two data sets together and dropping columns to make the data set more suitable for my project.\nThe link to the cleaned data can be found here and the link to the cleaning code can be found here."
  },
  {
    "objectID": "tabs/arm/arm.html",
    "href": "tabs/arm/arm.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "1+1"
  },
  {
    "objectID": "tabs/dimensionality_reduction/dimensionality_reduction.html",
    "href": "tabs/dimensionality_reduction/dimensionality_reduction.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "1+1"
  },
  {
    "objectID": "tabs/code/code.html",
    "href": "tabs/code/code.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "You can access my github repo here."
  },
  {
    "objectID": "tabs/decision_tree/classification.html",
    "href": "tabs/decision_tree/classification.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "1+1"
  },
  {
    "objectID": "tabs/introduction/introduction.html",
    "href": "tabs/introduction/introduction.html",
    "title": "Introduction to sports injuries and injury prevention",
    "section": "",
    "text": "Injuries and injury prevention in sports is an increasingly important topic in sports as athletes continue to increase the ability to perform well. As the world continues to get more competitive and as sport remains an important part of our cultures and societies, keeping athletes healthy becomes more important. So far, the field has not had a lot of research conducted, with studies mainly focusing on how we can use machine learning and artificial intelligence to predict and prevent sports injuries. There are many different voices in this realm including voices discussing the machine learning and artificial intelligence side of things and how we can make those tools more efficient, as well as voices coming from the medical point of view striving to reduce the frequency of injuries. I would like to explore both the machine learning and artificial intelligence perspective and how we can apply those findings to athletes’ daily lives.\n\n10 questions I aim to answer\n\nIn what sports is machine learning and artificial intelligence most useful for predicting and prevention injuries?\nHow can we use machine learning and artificial intelligence to predict and prevent sports injuries?\nIs there a difference in the data between predicting injuries in male athletes versus predicting injuries in female athletes?\nHow can we make the results of many of these studies more useful to clinicians?\nHow can we more efficiently use machine learning to predict and prevent sports injuries?\nHow can we more efficiently use artificial intelligence to predict and prevent sports injuries?\nWhat does it look like on the day to day sports level when advice from machine learning findings are incorporated?\nWhat does using machine learning and artificial intelligence mean for the future of sports?\nWhat effect has machine learning and artificial intelligence had on predicting and preventing sports injuries so far?\nWhat are the goals of the field?\n\n\n\nSummary: The Severity of Sports Injuries by Willem van Mechelen\nThe severity of sports injuries can be broken down into “6 criteria: (i) nature of sports injury; (ii) duration and nature of treatment; (iii) sporting time lost; (iv) working time lost; (v) permanent damage; and (vi) monetary cost” (Mechelen 1997). Sports injuries can be divided into several broad categories based on type of injury. In more traditional and popular sports such as football and basketball, the most common sports injuries are in the contusions or sprains on the lower extremities, though other sports such as skiing, parachute jumping, horse riding, etc, see fractures more often. We can use “data on the duration and nature of treatment” to “determine the severity of an injury” and help us evaluate what medical treatments will be most effective. From there, we can evaluate how much, if any, permanent damage there will be following the injury. (Mechelen 1997). Time away from the sport can also have an effect on an athlete’s psychosocial wellness. Fortunately 50 to 60% of all sports injuries do not lead to a substantial loss of sporting time (Mechelen 1997). Additionally, there are many costs that come from sports injuries including financial, social and financial. The two main types of social costs can be subdivided into quantifiable and unquantifiable. Quantifiable costs include “insurance costs and legal expenses” whereas unquantifiable costs include any harm caused to the athlete’s psychosocial wellness. The two main types of financial costs of sports injuries include direct costs which includes “the cost of medical treatment” and indirect costs which are considered to be “expenditure incurred in connection with the loss of productivity due to increased morbidity and mortality levels” (Mechelen 1997). An integral part to preventing such costs and effects is implementing proper sports injury prevention techniques.\n\n\nSummary: Machine learning methods in sport injury prediction and prevention: a systematic review by Hans Van Eetvelde, Luciana D. Mendonça, Christophe Ley, Romain Seil, and Thomas Tischer\nSports injuries have a complex cause with the “interactions of multiple risk factors and inciting events making a comprehensive model necessary” and difficult due to the large number of factors that play a role in an injury (Van Eetvelde et al. 2021). The first half of Hans Van Eetvelde et. al article covers the way that they selected their sources. Their paper addresses the “currently used definition of ML as well as predominantly used MIL methods”, the “accuracy of the currently used ML methods to predict injury”, and evaluates “the used methods for sport injury prevention purposes” (Van Eetvelde et al. 2021). The inclusion criteria for this article required articles to be original in “investigating the role of machine learning for sport injury prediction” and prevention, in english, and published in a peer-reviewed journal (ibid). The exclusion criteria included articles “not being sport specific, not covering injury prevention or injury prediction, [and] meeting abstracts and proceedings” (ibid). These criteria and their narrowing process left the researchers with 11 studies. The study found that “the most promising results to predict injury risk were obtained in elite youth football players…and in professional soccer based on a pre-season screening evaluation (Van Eetvelde et al. 2021).” It also concluded that “ML methods may be used to identify athletes at high injury risk during sport participation and that it may be helpful to identify risk factors (Van Eetvelde et al. 2021).” However, “the methodological study quality was moderate to very low (ibid).” As the field is growing, the authors expect promising further developments with respect to artificial intelligence and machine learning methods.\n\n\n\n\n\nReferences\n\nMechelen, Willem van. 1997. “The Severity of Sports Injuries.” Sports Medicine 24: 176–80.\n\n\nVan Eetvelde, Hans, Luciana D Mendonça, Christophe Ley, Romain Seil, and Thomas Tischer. 2021. “Machine Learning Methods in Sport Injury Prediction and Prevention: A Systematic Review.” Journal of Experimental Orthopaedics 8: 1–15."
  },
  {
    "objectID": "tabs/data_gathering/data_gathering.html",
    "href": "tabs/data_gathering/data_gathering.html",
    "title": "General Injury Data",
    "section": "",
    "text": "Injury & Injury Prevention Data Soccer\nThis data set focuses on the factors that may have influenced injuries that occured in soccer. The data collects information about the injury that occured, any history of injuries, and factors that may have affected the injury beforehand. This data will be useful to see what factors can influence injuries, such as warming up, previous injuries, etc.\nThe raw data and dataset can be found here.\n\n\nInjury Data From Different Activites\nThis data breaks down the how many injuries per age group come from different types of activities. While this data isn’t directly related to professional sports, it helps provide a basic understanding of what sports or physical activities produce the most injuries.\nThe data source can be found here.\n\n\nNBA Data\n\nNBA Injury Data\nThe first data set that I selected to use is a NBA Injury Data set. This data set records all of the roster moves related to injuries made by each NBA team in every season betwwen the 2010 and 2020 seasons. The raw data contains 5 features, including player name, team name, date of transaction, acquired players, relinquished players, and notes about the injury. I thought this data source would be useful to help gather information about what kinds of injuries are most common and how many injuries each team had per season.\nThe link to the raw data can be found here and the link to the data source can be found here.\n\n\nNBA Data\nThese three data sets are derived from Kaggle and are all related to the NBA. One is a players data set which contains information about players in the NBA including player name, team ID, and the season. Next is the rankings data set which includes information about each team’s record in each season and well as information about their conference. Last is the teams data which includes information about each team and where they play. These datasets combined would provide me with information about the NBA records for each season.\nThe link to raw data and data source can be found here\n\n\n\nNFL Data\n\nNFL Concussion Data\nThis data sources is a list of all the concussion or head injuries in the NFL during the 2012-2014. The data set has information about the time of the injury within the season, the severity of the injury, and the aftermath of the injury. This information is helpful to provide injury context for the NFL seasons.\nThe link to the raw data and data source can be found here\n\n\nNFL Game Injury Data\nThis data set provides anonymous information about NFL injuries and the situation in which the injury occured, such as the stadium type, the field type, and the temperature. This data will be useful for seeing how the weather and game circumstances can influence injuries.\nThe link to the raw data and data source can be found here."
  },
  {
    "objectID": "tabs/eda/eda.html",
    "href": "tabs/eda/eda.html",
    "title": "Data Exploration",
    "section": "",
    "text": "In order to explore my data, I plan to use summary statistics, a variety of graph types including a word cloud, bar graphs, line graphs, pie charts, etc. More specifically, I will use Pandas, Matplotlib, and Seaborn. I hope that through this exploration I can learn more about my data and the relationships that exist within it."
  },
  {
    "objectID": "tabs/eda/eda.html#text-data",
    "href": "tabs/eda/eda.html#text-data",
    "title": "Data Exploration",
    "section": "Text Data",
    "text": "Text Data\n\nNews API\n\nimport pandas as pd \n\nfile_path = \"../../../../data/01-modified-data/cleaned.csv\"\ndf = pd.read_csv(file_path)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nsource\nauthor\npublish_date\ncombined\n\n\n\n\n0\ncnet\nadam oram\n2023-09-30T12:02:09Z\nman united vs crystal palace livestream: how t...\n\n\n1\ncnet\nadam oram\n2023-09-23T12:00:09Z\nman city vs nottingham forest livestream: how ...\n\n\n2\nhuffpost\nap\n2023-09-19T11:28:10Z\nspanish soccer star accuses federation of thre...\n\n\n3\nslate magazine\njosh levin and stefan fatsis\n2023-09-18T21:43:37Z\nis there any part of aaron rodgers first game ...\n\n\n4\ndeadspin\nsam fels\n2023-10-03T11:27:00Z\nmls is playing pretty fast and loose with lion...\n\n\n\n\n\n\n\n\nStop Word Removal\n\nall_text = \"\"\nfor row in df[\"combined\"]:\n    all_text = all_text + \" \" + row \n\nall_text = \" \".join(list(df[\"combined\"].values))\n\n\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\n\nwords = all_text.split()\n\nstop_words = set(stopwords.words('english'))\nfiltered_words = [word for word in words if word.lower() not in stop_words]\n\nfiltered_text = ' '.join(filtered_words)\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/reneedemaio/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n\n\n\n\nWord Cloud\n\ndef generate_word_cloud(my_text): \n    from wordcloud import WordCloud, STOPWORDS\n    import matplotlib.pyplot as plt\n\n    def plot_cloud(wordcloud):\n        plt.figure(figsize=(40, 30))\n        plt.imshow(wordcloud)\n        plt.axis(\"off\");\n\n    wordcloud = WordCloud(\n        width = 3000, \n        height = 2000,\n        random_state=1, \n        background_color='lavender',\n        colormap= \"rainbow\",\n        collocations=False,\n        stopwords = STOPWORDS).generate(my_text)\n    plot_cloud(wordcloud)\n    plt.show()\n\n\ngenerate_word_cloud(filtered_text)\n\n\n\n\n\n\nBar Chart\n\nfrom collections import Counter\n\nwords = filtered_text.split()\nword_freq = Counter(words)\n\n\nimport matplotlib.pyplot as plt\n\nfrequency_threshold = 10\n\ncommon_words = [(word, freq) for word, freq in word_freq.items() if freq &gt; frequency_threshold]\ncommon_words.sort(key=lambda x: x[1], reverse=True)\n\nwords, frequencies = zip(*common_words)\n\nplt.figure(figsize=(10, 6))\nplt.bar(words, frequencies)\nplt.xlabel(\"Words\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Word Frequency Bar Chart\")\nplt.xticks(rotation=45)\n\nplt.tight_layout()\nplt.show\n\n&lt;function matplotlib.pyplot.show(close=None, block=None)&gt;\n\n\n\n\n\n\nmost_common_words = word_freq.most_common(10)  \n\nwords, frequencies = zip(*most_common_words)\n\nplt.figure(figsize=(10, 6))\nplt.bar(words, frequencies)\nplt.xlabel('Words')\nplt.ylabel('Frequency')\nplt.title('Word Frequency Bar Chart')\nplt.xticks(rotation=45)  \n\nplt.tight_layout()\nplt.show()\n\nThe data exploration for this data was successful and revealed that the majority of the topics discussed in the news relating to soccer are about specific players or competitions, and less about injuries. While some information of injuries are present, based on the lack of words that could relate to discussion of injury, it appears that these mentions are surface level."
  },
  {
    "objectID": "tabs/eda/eda.html#tabular-data",
    "href": "tabs/eda/eda.html#tabular-data",
    "title": "Data Exploration",
    "section": "Tabular Data",
    "text": "Tabular Data\n\nBasketball Injury Data\n\nimport pandas as pd \nimport numpy as np\nimport gdown\n\n# the csv url is: https://drive.google.com/file/d/1tdKeSFi492daHWh8Laqb7e3_68o3kaqD/view?usp=share_link\nfile_id = \"1tdKeSFi492daHWh8Laqb7e3_68o3kaqD\"\nurl = f\"https://drive.google.com/uc?id={file_id}\"\noutput = \"basketball_injuries.csv\"\ngdown.download(url, output, quiet=False)\n\nDownloading...\nFrom: https://drive.google.com/uc?id=1tdKeSFi492daHWh8Laqb7e3_68o3kaqD\nTo: /Users/reneedemaio/Desktop/git_repo/dsan-5000-project-rennyd123/dsan-website/5000-website/tabs/eda/basketball_injuries.csv\n100%|██████████| 1.57M/1.57M [00:00&lt;00:00, 5.38MB/s]\n\n\n'basketball_injuries.csv'\n\n\n\nfile_path = \"../../../../data/01-modified-data/basketball_injury_data.csv\"\n\nbball_injury_data = pd.read_csv(file_path)\nprint(bball_injury_data.head())\n\n         Date     Team   Relinquished  \\\n0  2010-10-03    Bulls  Carlos Boozer   \n1  2010-10-06  Pistons  Jonas Jerebko   \n2  2010-10-06  Pistons  Terrico White   \n3  2010-10-08  Blazers     Jeff Ayres   \n4  2010-10-08     Nets    Troy Murphy   \n\n                                               Notes      InjuryStatus  \n0  fractured bone in right pinky finger (out inde...  out indefinitely  \n1      torn right Achilles tendon (out indefinitely)  out indefinitely  \n2  broken fifth metatarsal in right foot (out ind...  out indefinitely  \n3          torn ACL in right knee (out indefinitely)  out indefinitely  \n4             strained lower back (out indefinitely)  out indefinitely  \n\n\n\nInjury Status Bar Chart\n\nimport matplotlib.pyplot as plt\n\ninjury_counts = bball_injury_data[\"InjuryStatus\"].value_counts()\n\ntop_5_injury_counts = injury_counts.head(5)\n\nplt.figure(figsize=(8, 6))\nplt.bar(top_5_injury_counts.index, top_5_injury_counts.values, color='pink')\nplt.xlabel('Injury Status')\nplt.ylabel('Count')\nplt.title('Top 5 Injury Status Bar Chart')\n\nplt.show()\n\n\n\n\n\n\nInjuries Per Team\n\nteam_counts = bball_injury_data[\"Team\"].value_counts()\nteam_names = bball_injury_data[\"Team\"].unique()\n\nplt.figure(figsize=(8, 8))\nplt.pie(team_counts, labels=team_names, autopct='%1.1f%%', startangle=140, shadow=True)\nplt.axis(\"equal\")\nplt.title(\"Injury Count per Team\")\nplt.show()\n\n\n\n\n\n\nInjuries by Dates\n\nbball_injury_data['Date'] = pd.to_datetime(bball_injury_data['Date'])\ndate_counts_2010 = bball_injury_data[bball_injury_data['Date'].dt.year == 2010]\ndate_counts_2010 = date_counts_2010['Date'].value_counts().sort_index()\n\nplt.figure(figsize=(12, 6))\nplt.plot(date_counts_2010.index, date_counts_2010.values, marker='o', linestyle='-', color=\"green\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Injury Count\")\nplt.title(\"Injury Counts by Day in 2010\")\nplt.xticks(rotation=45)\n\nplt.grid(True)\nplt.show()\n\n\n\n\nThe first graph created for this data set reveals that the most common reason given for an athlete’s absence from an NBA game is simply that they did not play. This can be for any variety of reasons from personal reasons, issues with the coach, or injury as well. The next most common is did not dress, which could imply that the athlete is injuries, but doesn’t specifically state that. The next graph shows that most teams in the NBA had relatively similar rates of injury with most of the hovering around 3%. Finally, the last graph shows one interesting spike in injuries in late October of 2010. There is nothing notable in NBA history that day that would cause so many more athletes to be out, so this can be considered an outlier. Outside of that, the average number of injuries across all teams per day hovers between 0-15.\n\n\n\nNFL Concussion Data\n\nfile_path = \"../../../../data/01-modified-data/nfl_concussions.csv\"\n\nconcussion_data = pd.read_csv(file_path)\nprint(concussion_data.head())\n\n             Player                  Team        Date         Opposing Team  \\\n0  Aldrick Robinson   Washington Redskins  30/09/2012  Tampa Bay Buccaneers   \n1       D.J. Fluker    San Diego Chargers  22/09/2013      Tennessee Titans   \n2  Marquise Goodwin         Buffalo Bills  28/09/2014        Houston Texans   \n3       Bryan Stork  New England Patriots  12/10/2014         Buffalo Bills   \n4    Lorenzo Booker         Chicago Bears   9/09/2012    Indianapolis Colts   \n\n           Position Pre-Season Injury? Winning Team?  Week of Injury  \\\n0     Wide Receiver                 No           Yes               4   \n1  Offensive Tackle                 No            No               3   \n2     Wide Receiver                 No            No               4   \n3            Center                 No           Yes               6   \n4      Running Back                Yes           Yes               1   \n\n      Season  Weeks Injured  Games Missed Unknown Injury?  \\\n0  2012/2013              1           1.0              No   \n1  2013/2014              1           1.0              No   \n2  2014/2015              1           1.0              No   \n3  2014/2015              1           1.0              No   \n4  2012/2013              0           NaN              No   \n\n  Reported Injury Type  Total Snaps      Play Time After Injury  \\\n0                 Head            0                    14 downs   \n1           Concussion            0                    78 downs   \n2           Concussion            0                    25 downs   \n3                 Head            0                    82 downs   \n4                 Head            0  Did not return from injury   \n\n  Average Playtime Before Injury  \n0                    37.00 downs  \n1                    73.50 downs  \n2                    17.50 downs  \n3                    41.50 downs  \n4                            NaN  \n\n\n\nInjury Count by Team\n\ninjuries_by_team = concussion_data['Team'].value_counts()\n\n \nplt.figure(figsize=(15, 6))\nplt.bar(injuries_by_team.index, injuries_by_team.values, color=\"skyblue\")\nplt.xlabel(\"Team Name\")\nplt.ylabel(\"Injury Count\")\nplt.title(\"Injury Count by Team\")\nplt.xticks(rotation=60)\n\nplt.show()\n\n\n\n\n\n\nType of Injury\n\ninjury_type = concussion_data[\"Reported Injury Type\"].value_counts()\ninjury_name = injury_type.index\n\npastel_colors = ['#FF9999', '#66B2FF', '#99FF99', '#FFCC99', '#c2c2f0', '#ffb3e6', '#c2f0c2']\n\nplt.figure(figsize=(8, 8))\nplt.pie(injury_type, labels=injury_name, autopct='%1.1f%%', startangle=140, shadow=False, colors=pastel_colors)\nplt.axis(\"equal\")\nplt.title(\"Injury Count by Type of Injury\")\nplt.legend(injury_name, title=\"Injury Types\", loc=\"best\")\nplt.show()\n\n\n\n\n\n\nInjuries by Position\n\ninjuries_by_pos = concussion_data['Position'].value_counts()\n\n \nplt.figure(figsize=(15, 6))\nplt.bar(injuries_by_pos.index, injuries_by_pos.values, color=\"orange\")\nplt.xlabel(\"Position\")\nplt.ylabel(\"Injury Count\")\nplt.title(\"Injury Count by Position\")\nplt.xticks(rotation=60)\n\nplt.show()\n\n\n\n\n\n\nCorrelation Between Week of Injury and Total Snaps\n\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns \nimport matplotlib.pyplot as plt \n\n\nsubset = [\"Week of Injury\", \"Total Snaps\"]\nconcussion_data_subset = concussion_data[subset]\n\ncorr_matrix = concussion_data_subset.corr()\n\nsns.heatmap(corr_matrix, \n            annot=True, \n            cmap=\"coolwarm\")\nplt.title(\"Correlation Map between Week of Injury and Total Snaps\")\nplt.show()\n\n\n\n\nThe graphs made from the NFL concussion data reveal that the Cincinatti Bengals and the Cleveland Browns had the most concussions on their team. The majority of head injuries are concussions (81.7%), but there are also a fair share of non-concussion head injuries (18.0%). Next, the position that sees the most amount of concussion injuries is cornerback, closely followed by wide receiver and safety. Finally, there was a 12% positive correlation between the week of injury and the total number of snaps a player had.\n\n\n\nNFL Game Injury Data\n\nfile_path = \"../../../../data/01-modified-data/nfl_injuries.csv\"\n\nnfl_injuries = pd.read_csv(file_path)\nprint(nfl_injuries.head())\n\n   PlayerKey BodyPart RosterPosition       StadiumType  FieldType  \\\n0      39873     Knee     Linebacker            indoor  Synthetic   \n1      39873     Knee     Linebacker           outdoor    Natural   \n2      39873     Knee     Linebacker            indoor  Synthetic   \n3      39873     Knee     Linebacker  retractable roof  Synthetic   \n4      39873     Knee     Linebacker           outdoor    Natural   \n\n   Temperature        Weather  \n0           85  Mostly Cloudy  \n1           82          Sunny  \n2           84         Cloudy  \n3           78  Partly Cloudy  \n4           80         Cloudy  \n\n\n\nInjury Count by Stadium Type\n\nstadium_type = nfl_injuries[\"StadiumType\"].value_counts()\ncolors = plt.cm.viridis(np.linspace(0, 1, len(stadium_type)))\n\n\nplt.figure(figsize=(12,6))\nplt.bar(stadium_type.index, stadium_type.values, color=colors)\nplt.xlabel(\"Stadium Type\")\nplt.ylabel(\"Injury Count\")\nplt.title(\"Injury Count by Stadium Types\")\nplt.xticks(rotation = 60)\nplt.show()\n\n\n\n\n\n\nInjury Count by Body Part Injured\n\nimport seaborn as sns\n\ninjury_frequency = nfl_injuries[\"BodyPart\"].value_counts()\ninjury_place = nfl_injuries[\"BodyPart\"].unique()\nprint(injury_frequency)\n\npastel_colors = ['#FF9999', '#66B2FF', '#99FF99', '#FFCC99', '#c2c2f0', '#ffb3e6', '#c2f0c2']\n\nplt.figure(figsize=(8, 8))\nplt.pie(injury_frequency, labels=injury_place, autopct='%1.1f%%', startangle=140, shadow=False, colors=pastel_colors)\nplt.axis(\"equal\")\nplt.title(\"Injury Count by Type of Injury\")\nplt.show()\n\nBodyPart\nKnee     825\nAnkle    720\nToes     144\nFoot      96\nHeel      18\nName: count, dtype: int64\n\n\n\n\n\n\n\nInjury Count vs Temperature\n\nnfl_injuries[\"Temperature\"] = pd.to_numeric(nfl_injuries[\"Temperature\"], errors='coerce')\nnfl_injuries = nfl_injuries[nfl_injuries[\"Temperature\"] &gt;= 0]\nnfl_injuries = nfl_injuries.dropna(subset=[\"Temperature\"])\n\ntemperature_counts = nfl_injuries[\"Temperature\"].value_counts()\n\nplt.figure(figsize=(12, 6))\nplt.plot(temperature_counts.values, temperature_counts.index, marker='o', linestyle='-', color=\"green\")\nplt.xlabel(\"Temperature\")\nplt.ylabel(\"Injury Count\")\nplt.title(\"Injury Counts by Temperature\")\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\nThis dataset reveals that the majority of NFL injuries occur in stadiums that are outdoors. Most of the injuries are knee or ankle injuires which makese sense due to the nature of American Football. Finally, there doesn’t seem to be a huge correlation between temperature and number of injuries. Spikes in injuries occur during many points in the year.\n\n\n\nInjury Prevention Data Soccer\n\nfile_path = \"../../../../data/01-modified-data/injury_prevention_data_soccer.csv\"\n\ninjury_prevention = pd.read_csv(file_path)\nprint(injury_prevention.head())\n\n    ID  Age  Height  Mass  Team  Position  Years of Football Experience  \\\n0  146   19   173.0  67.6     1         3                             1   \n1  155   22   179.5  71.0     1         3                             1   \n2  160   22   175.5  71.8     1         3                             1   \n3  164   23   190.0  80.5     1         4                             1   \n4  145   19   173.5  68.7     1         3                             1   \n\n  Previous Injuries   Number of Injuries Ankle Injuries   ...  \\\n0                yes                   6             yes  ...   \n1                yes                   2              no  ...   \n2                yes                   7             yes  ...   \n3                yes                   1              no  ...   \n4                yes                   2             yes  ...   \n\n   Importance Injury Prevention Knowledgeability   \\\n0                             2                 1   \n1                             1                 1   \n2                             1                 1   \n3                             1                 1   \n4                             1                 2   \n\n  Prevention Measure Stretching  Prevention Measure Warm Up   \\\n0                            yes                          no   \n1                            yes                         yes   \n2                            yes                          no   \n3                            yes                         yes   \n4                            yes                         yes   \n\n   Prevention Measure Specific Strength Exercises   \\\n0                                              yes   \n1                                               no   \n2                                               no   \n3                                              yes   \n4                                               no   \n\n  Prevention Measure Bracing  Prevention Measure Taping   \\\n0                          no                         no   \n1                          no                         no   \n2                          no                         no   \n3                          no                         no   \n4                          no                        yes   \n\n  Prevention Measure Shoe Insoles   Prevention Measure Face Masks   \\\n0                               no                              no   \n1                               no                              no   \n2                              yes                              no   \n3                               no                              no   \n4                               no                              no   \n\n  Prevention Measure Medical Corset   \n0                                 no  \n1                                 no  \n2                                 no  \n3                                 no  \n4                                 no  \n\n[5 rows x 41 columns]\n\n\n\nSummary Statistics on Personal Information\n\ninjury_prevention.columns\nprint(injury_prevention.dtypes)\n\nID                                                   int64\nAge                                                  int64\nHeight                                             float64\nMass                                               float64\nTeam                                                 int64\nPosition                                             int64\nYears of Football Experience                         int64\nPrevious Injuries                                   object\nNumber of Injuries                                   int64\nAnkle Injuries                                      object\nNumber of Ankle Injuries                             int64\nSevere_Ankle_Injuries                               object\nNoncontact_Ankle_Injuries                           object\nKnee Injuries                                       object\nNumber of Knee Injuries                              int64\nSevere_Knee_Injuries                                object\nNoncontact_Knee_Injuries                            object\nThigh_Injuries                                      object\nNumber of Thigh Injuries                             int64\nSevere_Thigh_Injuries                               object\nNoncontact_Thigh_Injuries                           object\nRisk Factor Condition                               object\nRisk Factor Coordination                            object\nRisk Factor Muscle Impairments                      object\nRisk Factor Fatigue                                 object\nRisk Factor Previous Injury                         object\nRisk Factor Attentiveness                           object\nRisk Factor Other Player                            object\nRisk Factor Equipment                               object\nRisk Factor Climatic Condition                      object\nRisk Factor Diet                                    object\nImportance Injury Prevention                         int64\nKnowledgeability                                     int64\nPrevention Measure Stretching                       object\nPrevention Measure Warm Up                          object\nPrevention Measure Specific Strength Exercises      object\nPrevention Measure Bracing                          object\nPrevention Measure Taping                           object\nPrevention Measure Shoe Insoles                     object\nPrevention Measure Face Masks                       object\nPrevention Measure Medical Corset                   object\ndtype: object\n\n\n\ninjury_prevention[\"Height\"] = injury_prevention[\"Height\"].astype(int)\ninjury_prevention[\"Mass\"] = injury_prevention[\"Mass\"].astype(int)\n\n\nprint(injury_prevention.dtypes)\n\nID                                                  int64\nAge                                                 int64\nHeight                                              int64\nMass                                                int64\nTeam                                                int64\nPosition                                            int64\nYears of Football Experience                        int64\nPrevious Injuries                                  object\nNumber of Injuries                                  int64\nAnkle Injuries                                     object\nNumber of Ankle Injuries                            int64\nSevere_Ankle_Injuries                              object\nNoncontact_Ankle_Injuries                          object\nKnee Injuries                                      object\nNumber of Knee Injuries                             int64\nSevere_Knee_Injuries                               object\nNoncontact_Knee_Injuries                           object\nThigh_Injuries                                     object\nNumber of Thigh Injuries                            int64\nSevere_Thigh_Injuries                              object\nNoncontact_Thigh_Injuries                          object\nRisk Factor Condition                              object\nRisk Factor Coordination                           object\nRisk Factor Muscle Impairments                     object\nRisk Factor Fatigue                                object\nRisk Factor Previous Injury                        object\nRisk Factor Attentiveness                          object\nRisk Factor Other Player                           object\nRisk Factor Equipment                              object\nRisk Factor Climatic Condition                     object\nRisk Factor Diet                                   object\nImportance Injury Prevention                        int64\nKnowledgeability                                    int64\nPrevention Measure Stretching                      object\nPrevention Measure Warm Up                         object\nPrevention Measure Specific Strength Exercises     object\nPrevention Measure Bracing                         object\nPrevention Measure Taping                          object\nPrevention Measure Shoe Insoles                    object\nPrevention Measure Face Masks                      object\nPrevention Measure Medical Corset                  object\ndtype: object\n\n\n\nnumerical_columns = injury_prevention.select_dtypes(include='int64')\nnumerical_columns = injury_prevention.drop(columns=\"ID\")\nnumerical_columns.describe()\n\n\n\n\n\n\n\n\nAge\nHeight\nMass\nTeam\nPosition\nYears of Football Experience\nNumber of Injuries\nNumber of Ankle Injuries\nNumber of Knee Injuries\nNumber of Thigh Injuries\nImportance Injury Prevention\nKnowledgeability\n\n\n\n\ncount\n139.000000\n139.000000\n139.000000\n139.000000\n139.000000\n139.000000\n139.000000\n139.000000\n139.000000\n139.000000\n139.000000\n139.000000\n\n\nmean\n17.597122\n177.043165\n68.417266\n3.690647\n2.546763\n1.733813\n1.906475\n0.683453\n0.517986\n0.726619\n1.345324\n0.827338\n\n\nstd\n4.601070\n9.416198\n11.781156\n1.825142\n0.853153\n0.913470\n1.614661\n0.932784\n0.684746\n0.778383\n0.586221\n0.415777\n\n\nmin\n13.000000\n141.000000\n31.000000\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n14.000000\n172.500000\n61.000000\n2.000000\n2.000000\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n1.000000\n1.000000\n\n\n50%\n16.000000\n178.000000\n70.000000\n4.000000\n3.000000\n2.000000\n2.000000\n0.000000\n0.000000\n1.000000\n1.000000\n1.000000\n\n\n75%\n19.000000\n184.000000\n76.000000\n5.500000\n3.000000\n2.000000\n2.000000\n1.000000\n1.000000\n1.000000\n2.000000\n1.000000\n\n\nmax\n35.000000\n196.000000\n101.000000\n6.000000\n4.000000\n4.000000\n10.000000\n4.000000\n3.000000\n3.000000\n4.000000\n2.000000\n\n\n\n\n\n\n\n\nobject_columns = injury_prevention.select_dtypes(include='object')\nfor column in object_columns:\n    plt.figure(figsize=(4, 4))\n    sns.countplot(data=injury_prevention, x=column, order=['yes', 'no'], palette=['pink', 'lightblue'])\n    plt.title(f'{column}')\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis set of graphs show counts of how many athletes used each prevention method, how many athletes had each time of injury, and what risk factors are present.\n\n\n\nSports Injury Data\n\nfile_path = \"../../../../data/01-modified-data/sports_injury_data.csv\"\n\nsports_injuries = pd.read_csv(file_path)\nprint(sports_injuries.head())\n\n     Sport, activity or equipment Injuries (1) Younger than 5  5 to 14  \\\n0    Exercise, exercise equipment      445,642          6,662   36,769   \n1        Bicycles and accessories      405,411         13,297   91,089   \n2                      Basketball      313,924          1,216  109,696   \n3                        Football      265,747            581  145,499   \n4  ATV's, mopeds, minibikes, etc.      242,347          3,688   42,069   \n\n  15 to 24 25 to 64 65 and older  \n0   91,013  229,640       81,558  \n1   50,863  195,030       55,132  \n2  143,773   57,413        1,825  \n3  100,760   18,527          381  \n4   61,065  122,941       12,584  \n\n\n\nsports_injuries_subset = sports_injuries[['Injuries (1)', 'Younger than 5',\n       '5 to 14', '15 to 24', '25 to 64', '65 and older']]\nactivity_type = sports_injuries[\"Sport, activity or equipment\"]\n\n\nsports_injuries.dtypes\n\nSport, activity or equipment    object\nInjuries (1)                    object\nYounger than 5                  object\n5 to 14                         object\n15 to 24                        object\n25 to 64                        object\n65 and older                    object\ndtype: object\n\n\n\nsports_injuries[\"Injuries (1)\"] = sports_injuries[\"Injuries (1)\"].str.replace(',', '').astype(float)\nsports_injuries[\"Younger than 5\"] = sports_injuries[\"Younger than 5\"].str.replace(',', '').astype(float)\nsports_injuries[\"5 to 14\"] = sports_injuries[\"5 to 14\"].str.replace(',', '').astype(float)\nsports_injuries[\"15 to 24\"] = sports_injuries[\"15 to 24\"].str.replace(',', '').astype(float)\nsports_injuries[\"25 to 64\"] = sports_injuries[\"25 to 64\"].str.replace(',', '').astype(float)\nsports_injuries[\"65 and older\"] = sports_injuries[\"65 and older\"].str.replace(',', '').astype(float)\n\n\nprint(sports_injuries.head())\nprint(sports_injuries.dtypes)\n\n     Sport, activity or equipment  Injuries (1)  Younger than 5   5 to 14  \\\n0    Exercise, exercise equipment      445642.0          6662.0   36769.0   \n1        Bicycles and accessories      405411.0         13297.0   91089.0   \n2                      Basketball      313924.0          1216.0  109696.0   \n3                        Football      265747.0           581.0  145499.0   \n4  ATV's, mopeds, minibikes, etc.      242347.0          3688.0   42069.0   \n\n   15 to 24  25 to 64  65 and older  \n0   91013.0  229640.0       81558.0  \n1   50863.0  195030.0       55132.0  \n2  143773.0   57413.0        1825.0  \n3  100760.0   18527.0         381.0  \n4   61065.0  122941.0       12584.0  \nSport, activity or equipment     object\nInjuries (1)                    float64\nYounger than 5                  float64\n5 to 14                         float64\n15 to 24                        float64\n25 to 64                        float64\n65 and older                    float64\ndtype: object\n\n\n\nsubsetted_sports_injuries = sports_injuries.head(5)\nmelted_df = pd.melt(subsetted_sports_injuries, id_vars=['Sport, activity or equipment'], var_name='Age group', value_name='Injuries')\nplt.figure(figsize=(12, 6))\nsns.barplot(data=melted_df, x='Sport, activity or equipment', y='Injuries', hue='Age group')\nplt.title('Injuries by Age Group for Different Activities')\nplt.xlabel('Sport, activity or equipment')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\nsubsetted_sports_injuries = sports_injuries.tail(5)\nmelted_df = pd.melt(subsetted_sports_injuries, id_vars=['Sport, activity or equipment'], var_name='Age group', value_name='Injuries')\nplt.figure(figsize=(12, 6))\nsns.barplot(data=melted_df, x='Sport, activity or equipment', y='Injuries', hue='Age group')\nplt.title('Injuries by Age Group for Different Activities')\nplt.xlabel('Sport, activity or equipment')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\nThis set of graphs shows a count of how many injuries each sport or type of active activity causes by age group.\n\n\nNBA Player and Team Data\n\nfile_path = \"../../../../data/01-modified-data/nba_data.csv\"\n\nnba = pd.read_csv(file_path)\nprint(nba.head())\n\n   Unnamed: 0       PLAYER_NAME     TEAM_ID  PLAYER_ID  SEASON  LEAGUE_ID  \\\n0           1     Royce O'Neale  1610612762    1626220    2019          0   \n1           2  Bojan Bogdanovic  1610612762     202711    2019          0   \n2           3       Rudy Gobert  1610612762     203497    2019          0   \n3           4  Donovan Mitchell  1610612762    1628378    2019          0   \n4           5       Mike Conley  1610612762     201144    2019          0   \n\n   MIN_YEAR  MAX_YEAR ABBREVIATION NICKNAME  ...    HEADCOACH  \\\n0      1974      2019          UTA     Jazz  ...  Quin Snyder   \n1      1974      2019          UTA     Jazz  ...  Quin Snyder   \n2      1974      2019          UTA     Jazz  ...  Quin Snyder   \n3      1974      2019          UTA     Jazz  ...  Quin Snyder   \n4      1974      2019          UTA     Jazz  ...  Quin Snyder   \n\n     DLEAGUEAFFILIATION SEASON_ID  STANDINGSDATE CONFERENCE  TEAM   G   W   L  \\\n0  Salt Lake City Stars     22022     2022-12-22       West  Utah  35  19  16   \n1  Salt Lake City Stars     22022     2022-12-22       West  Utah  35  19  16   \n2  Salt Lake City Stars     22022     2022-12-22       West  Utah  35  19  16   \n3  Salt Lake City Stars     22022     2022-12-22       West  Utah  35  19  16   \n4  Salt Lake City Stars     22022     2022-12-22       West  Utah  35  19  16   \n\n   W_PCT  \n0  0.543  \n1  0.543  \n2  0.543  \n3  0.543  \n4  0.543  \n\n[5 rows x 26 columns]\n\n\n\nnba.columns\n\nIndex(['Unnamed: 0', 'PLAYER_NAME', 'TEAM_ID', 'PLAYER_ID', 'SEASON',\n       'LEAGUE_ID', 'MIN_YEAR', 'MAX_YEAR', 'ABBREVIATION', 'NICKNAME',\n       'YEARFOUNDED', 'CITY', 'ARENA', 'ARENACAPACITY', 'OWNER',\n       'GENERALMANAGER', 'HEADCOACH', 'DLEAGUEAFFILIATION', 'SEASON_ID',\n       'STANDINGSDATE', 'CONFERENCE', 'TEAM', 'G', 'W', 'L', 'W_PCT'],\n      dtype='object')\n\n\n\nWin Percentage by Team\n\nteam = nba[\"TEAM\"]\nwin_pct = nba[\"W_PCT\"]\n\nplt.figure(figsize=(12, 6))\nplt.bar(team, win_pct)\nplt.title('2019 Win Percentage by Team in the NBA')\nplt.xlabel('Team')\nplt.ylabel('Win Percentage')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\nWins vs Arena Capacity\n\nheatmap_data = nba.pivot_table(index='ARENACAPACITY', values='W_PCT', aggfunc='mean')\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(heatmap_data, cmap='YlGnBu', annot=True, fmt='.2f', cbar=True)\nplt.title('Heatmap: ARENACAPACITY vs. W_PCT')\nplt.xlabel('W_PCT (Win Percentage)')\nplt.ylabel('ARENACAPACITY (Arena Capacity)')\nplt.show()\n\n\n\n\nFinally, these graphs show two different representations of win percentage in the 2019 NBA season. The first graph shows win percentage by team, and the second shows a correlation between arena capacity and win percentage. It appears that win percentage and arena capacity are not strongly positively correlated."
  },
  {
    "objectID": "tabs/eda/eda.html#hypothesis-refinement",
    "href": "tabs/eda/eda.html#hypothesis-refinement",
    "title": "Data Exploration",
    "section": "Hypothesis Refinement",
    "text": "Hypothesis Refinement\nBased off of the exploratory data analysis process, it is necessary that I refine my original hypotheses and identify next steps. Firstly, I think that it is important to continue to further explore that factors can affect an athletes susceptibility to injury to understand confounding variables in sport. That will be involved in my next steps with this project. Secondly, I would refine my hypothesis to include more sport specific hypotheses to create more specific hypotheses for soccer, American football, and basketball. In order to do so, I will need to collect more data."
  },
  {
    "objectID": "tabs/decision_tree/regression.html",
    "href": "tabs/decision_tree/regression.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "1+1"
  },
  {
    "objectID": "tabs/clustering/clustering.html",
    "href": "tabs/clustering/clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Build out your website tab for “clustering”"
  },
  {
    "objectID": "tabs/about/about.html",
    "href": "tabs/about/about.html",
    "title": "Introduction",
    "section": "",
    "text": "Renee DeMaio (she/her) is a fourth year undergraduate and first year graduate student at Georgetown University. Her undergraduate degree is in Chinese with a minor in mathematics. Her graduate degree is in Data Science and Analytics. She is from Hong Kong, and spent all of her adolecent life in Hong Kong and Singapore. At the age of 18, she moved to the United States for the first time to attend university. Her academic interests include how LGBTQ+ identities exist and flourish within a rigid Chinese social and familial structure, as well as artificial intelligence and machine learning. She hopes to further explore the latter two throughout the duration of her masters. Outside of school, her hobbies include dance, running, cooking, and reading.\n\nFavourite Books:\n\nCrying at H Mart by Michelle Zauner\nSidelined: Sports, Culture, and Being a Woman in America by Julie Dicaro\nKillers of a Certain Age by Deanna Raybourn\nJoan is Okay by Weike Wang\nIf He Had Been with Me by Laura Nowlin\nHappy Place by Emily Henry\n\n     \n\n\nRecent Personal Accomplishments\n\n2023: Acceptance into the DSAN Masters Programme\n2021 & 2022: Completing the National Women’s Half Marathon\n2020: Becoming a Lead Coach at Splash Foundation Hong Kong, an organization that teaches foreign domestic helpers water safety skills and how to swim."
  },
  {
    "objectID": "tabs/naive_bayes/naive_bayes.html",
    "href": "tabs/naive_bayes/naive_bayes.html",
    "title": "Introduction to Naive Bayes",
    "section": "",
    "text": "The Naive Bayes classifier is a supervised machine learning algorithm that is used for classification tasks, such as text classification. The goal of the Naive Bayes classifier is to classify data points into predefined categories, also called classes, by estimating the probability of the data point having each class using a predetermined set of features. At the core of Naive Bayes classifier is Bayes Theorem, a statistical tool that describes the probability of an event with knowledge of conditions that might influence the outcome of the event. When applied in Naive Bayes, it predicts the probability of a given object having a specific class, based on the observed features. The ‘naive’ aspect of Naive Bayes comes from the classifier’s assumption that all the data is independent.\nThere are different versions of Naive Bayes including Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes. The differences between each version is that they are all defined to accommodate different types of data and be used in different scenarios, so it is important to select the variant that best suits your data. Gaussian Naive Bayes is best suited for continuous data whose features are assumed to follow a normal distribution. This variant is best suited for data that consists of continuous, numeric features. Multinomial Naive Bayes is best suited for text or data represented as counts. It is commonly used in text classification, for example in sentiment analysis. Finally, Bernoulli Naive Bayes works best with binary data whose features are binary variables. It is often used in document classification tasks such as spam detection.\n\nPreparing Data for Naive Bayes\nIn order to carry out Naive Bayes, it is important that we preprocess our data. The data cleaning process has happened already and can be found within the data cleaning tab. Since our data is now prepared and cleaned, we need to separate it into training, validation, and testing sets. Training data is typically 80% of a data set and is the data that we give to the model so it can learn the existing releationships and methods that desired outcomes are predicted. The validation set is typically 10% of the data, and is given to the model to help fine tune the model’s hyperparameters and prevent overfitting. Finally, the remaining 10% of the data constitutes the test data, which is the data set given to the model to evaluate the model’s accuracy and performance. It is used to evaluate how well the model performs on unseen data.\n\n\nFeature Selection for Record Data\n\nData Set Up\n\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd \nimport matplotlib as plt\n\nfile_path = \"../../../../data/01-modified-data/injury_prevention_data_soccer.csv\"\nsoccer_injury = pd.read_csv(file_path)\n\nsoccer_injury.head()\n\n\n\n\n\n\n\n\nID\nAge\nHeight\nMass\nTeam\nPosition\nYears of Football Experience\nPrevious Injuries\nNumber of Injuries\nAnkle Injuries\n...\nImportance Injury Prevention\nKnowledgeability\nPrevention Measure Stretching\nPrevention Measure Warm Up\nPrevention Measure Specific Strength Exercises\nPrevention Measure Bracing\nPrevention Measure Taping\nPrevention Measure Shoe Insoles\nPrevention Measure Face Masks\nPrevention Measure Medical Corset\n\n\n\n\n0\n146\n19\n173.0\n67.6\n1\n3\n1\nyes\n6\nyes\n...\n2\n1\nyes\nno\nyes\nno\nno\nno\nno\nno\n\n\n1\n155\n22\n179.5\n71.0\n1\n3\n1\nyes\n2\nno\n...\n1\n1\nyes\nyes\nno\nno\nno\nno\nno\nno\n\n\n2\n160\n22\n175.5\n71.8\n1\n3\n1\nyes\n7\nyes\n...\n1\n1\nyes\nno\nno\nno\nno\nyes\nno\nno\n\n\n3\n164\n23\n190.0\n80.5\n1\n4\n1\nyes\n1\nno\n...\n1\n1\nyes\nyes\nyes\nno\nno\nno\nno\nno\n\n\n4\n145\n19\n173.5\n68.7\n1\n3\n1\nyes\n2\nyes\n...\n1\n2\nyes\nyes\nno\nno\nyes\nno\nno\nno\n\n\n\n\n5 rows × 41 columns\n\n\n\n\nsoccer_injury[\"Number of Injuries\"].unique()\n\narray([ 6,  2,  7,  1,  4,  3,  5,  0, 10])\n\n\n\nsoccer_injury.columns = soccer_injury.columns.str.strip()\nsoccer_injury.columns\n\nIndex(['ID', 'Age', 'Height', 'Mass', 'Team', 'Position',\n       'Years of Football Experience', 'Previous Injuries',\n       'Number of Injuries', 'Ankle Injuries', 'Number of Ankle Injuries',\n       'Severe_Ankle_Injuries', 'Noncontact_Ankle_Injuries', 'Knee Injuries',\n       'Number of Knee Injuries', 'Severe_Knee_Injuries',\n       'Noncontact_Knee_Injuries', 'Thigh_Injuries',\n       'Number of Thigh Injuries', 'Severe_Thigh_Injuries',\n       'Noncontact_Thigh_Injuries', 'Risk Factor Condition',\n       'Risk Factor Coordination', 'Risk Factor Muscle Impairments',\n       'Risk Factor Fatigue', 'Risk Factor Previous Injury',\n       'Risk Factor Attentiveness', 'Risk Factor Other Player',\n       'Risk Factor Equipment', 'Risk Factor Climatic Condition',\n       'Risk Factor Diet', 'Importance Injury Prevention', 'Knowledgeability',\n       'Prevention Measure Stretching', 'Prevention Measure Warm Up',\n       'Prevention Measure Specific Strength Exercises',\n       'Prevention Measure Bracing', 'Prevention Measure Taping',\n       'Prevention Measure Shoe Insoles', 'Prevention Measure Face Masks',\n       'Prevention Measure Medical Corset'],\n      dtype='object')\n\n\n\nsoccer_injury = soccer_injury.replace({\"yes\": 1, \"no\": 0})\nsoccer_injury.head()\n\n\n\n\n\n\n\n\nID\nAge\nHeight\nMass\nTeam\nPosition\nYears of Football Experience\nPrevious Injuries\nNumber of Injuries\nAnkle Injuries\n...\nImportance Injury Prevention\nKnowledgeability\nPrevention Measure Stretching\nPrevention Measure Warm Up\nPrevention Measure Specific Strength Exercises\nPrevention Measure Bracing\nPrevention Measure Taping\nPrevention Measure Shoe Insoles\nPrevention Measure Face Masks\nPrevention Measure Medical Corset\n\n\n\n\n0\n146\n19\n173.0\n67.6\n1\n3\n1\n1\n6\n1\n...\n2\n1\n1\n0\n1\n0\n0\n0\n0\n0\n\n\n1\n155\n22\n179.5\n71.0\n1\n3\n1\n1\n2\n0\n...\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n\n\n2\n160\n22\n175.5\n71.8\n1\n3\n1\n1\n7\n1\n...\n1\n1\n1\n0\n0\n0\n0\n1\n0\n0\n\n\n3\n164\n23\n190.0\n80.5\n1\n4\n1\n1\n1\n0\n...\n1\n1\n1\n1\n1\n0\n0\n0\n0\n0\n\n\n4\n145\n19\n173.5\n68.7\n1\n3\n1\n1\n2\n1\n...\n1\n2\n1\n1\n0\n0\n1\n0\n0\n0\n\n\n\n\n5 rows × 41 columns\n\n\n\n\n\nRandom Guessing\nI’ve decided to first use a random guessing method to predict my target variable before using Naive Bayes. This is to ensure that I have a point of comparison for when I look at my Naive Bayes accuracy results and to ensure that I can properly contextualize the results.\n\nx_vars = ['Prevention Measure Stretching', 'Prevention Measure Warm Up',\n       'Prevention Measure Specific Strength Exercises',\n       'Prevention Measure Bracing', 'Prevention Measure Taping',\n       'Prevention Measure Shoe Insoles', 'Prevention Measure Face Masks',\n       'Prevention Measure Medical Corset']\ny_var = \"Number of Injuries\"\n\n\ncolumns_keep = ['Prevention Measure Stretching', 'Prevention Measure Warm Up',\n       'Prevention Measure Specific Strength Exercises',\n       'Prevention Measure Bracing', 'Prevention Measure Taping',\n       'Prevention Measure Shoe Insoles', 'Prevention Measure Face Masks',\n       'Prevention Measure Medical Corset', \"Number of Injuries\"]\ndf_features = soccer_injury[columns_keep]\ndf_features.head()\n\n\n\n\n\n\n\n\nPrevention Measure Stretching\nPrevention Measure Warm Up\nPrevention Measure Specific Strength Exercises\nPrevention Measure Bracing\nPrevention Measure Taping\nPrevention Measure Shoe Insoles\nPrevention Measure Face Masks\nPrevention Measure Medical Corset\nNumber of Injuries\n\n\n\n\n0\n1\n0\n1\n0\n0\n0\n0\n0\n6\n\n\n1\n1\n1\n0\n0\n0\n0\n0\n0\n2\n\n\n2\n1\n0\n0\n0\n0\n1\n0\n0\n7\n\n\n3\n1\n1\n1\n0\n0\n0\n0\n0\n1\n\n\n4\n1\n1\n0\n0\n1\n0\n0\n0\n2\n\n\n\n\n\n\n\n\nimport numpy as np\nnum_obs = len(df_features)\nrng = np.random.default_rng(30)\nrandom_guesses = rng.choice(range(0,10), num_obs)\ndf_features[\"random_guesses\"] = random_guesses\ndf_features\n\n/var/folders/kw/p7j6fdpx3vvgm6c0krhwpvb80000gn/T/ipykernel_15152/1720700936.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_features[\"random_guesses\"] = random_guesses\n\n\n\n\n\n\n\n\n\nPrevention Measure Stretching\nPrevention Measure Warm Up\nPrevention Measure Specific Strength Exercises\nPrevention Measure Bracing\nPrevention Measure Taping\nPrevention Measure Shoe Insoles\nPrevention Measure Face Masks\nPrevention Measure Medical Corset\nNumber of Injuries\nrandom_guesses\n\n\n\n\n0\n1\n0\n1\n0\n0\n0\n0\n0\n6\n1\n\n\n1\n1\n1\n0\n0\n0\n0\n0\n0\n2\n2\n\n\n2\n1\n0\n0\n0\n0\n1\n0\n0\n7\n7\n\n\n3\n1\n1\n1\n0\n0\n0\n0\n0\n1\n4\n\n\n4\n1\n1\n0\n0\n1\n0\n0\n0\n2\n3\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n134\n1\n0\n0\n0\n0\n0\n0\n0\n0\n2\n\n\n135\n1\n0\n0\n1\n0\n0\n0\n0\n2\n3\n\n\n136\n1\n1\n1\n0\n1\n0\n0\n1\n2\n6\n\n\n137\n1\n0\n0\n0\n1\n1\n0\n0\n1\n5\n\n\n138\n1\n1\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n\n139 rows × 10 columns\n\n\n\n\nfrom sklearn.metrics import f1_score\n\nf1 = f1_score(df_features[\"Number of Injuries\"], df_features[\"random_guesses\"], average=\"macro\")\nprint(f\"f1 score: {f1}\")\ndf_features[\"random_guesses_correct\"] = df_features[\"Number of Injuries\"] == df_features[\"random_guesses\"]\ncorrect_random_guess_rows = df_features[df_features[\"random_guesses_correct\"] == True]\nrandom_guess_accuracy = len(correct_random_guess_rows) / len(df_features)\nprint(f\"random guess accuracy: {random_guess_accuracy}\")\n\nf1 score: 0.07252856433184302\nrandom guess accuracy: 0.10071942446043165\n\n\n/var/folders/kw/p7j6fdpx3vvgm6c0krhwpvb80000gn/T/ipykernel_15152/2051189337.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_features[\"random_guesses_correct\"] = df_features[\"Number of Injuries\"] == df_features[\"random_guesses\"]\n\n\n\n\nNaive Bayes Evaluation\nNow that I have completed random guessing and gotten an accuracy level of approximately 0.101, I will be able to have proper context for the accuracy levels given by Naive Bayes.\n\nX = soccer_injury[x_vars].values.copy()\ny = soccer_injury[y_var].values.copy()\nprint(X.shape, y.shape)\n\n(139, 8) (139,)\n\n\n\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nimport random\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5000)\n\ndef train_GNB_model(x_train,x_test,y_train,y_test,i_print=False):\n    scaler = StandardScaler()\n    x_train_scaled = scaler.fit_transform(x_train)\n    clf = BernoulliNB()\n    clf.fit(x_train_scaled, y_train)\n\n    y_train_pred = clf.predict(x_train_scaled)\n    y_test_pred = clf.predict(scaler.transform(x_test))\n\n    acc_train = accuracy_score(y_train, y_train_pred)\n    acc_test = accuracy_score(y_test, y_test_pred)\n\n    return acc_train, acc_test, clf \n\n\ntracc, teacc, clf = train_GNB_model(X_train, X_test, y_train, y_test)\n\n\nprint(\"train accuracy: \", tracc)\nprint(\"test accuracy: \", teacc)\nprint(\"classifier: \", clf)\n\ntrain accuracy:  0.34234234234234234\ntest accuracy:  0.14285714285714285\nclassifier:  BernoulliNB()\n\n\n\ny_pred = clf.predict(X_test)\nprint(y_pred)\nprint(y_test)\n\n[1 1 1 0 2 1 0 0 1 0 2 0 0 2 2 1 2 1 0 0 2 2 0 1 2 1 2 2]\n[2 7 2 2 2 2 1 0 2 1 1 1 1 3 1 3 1 1 1 1 6 2 1 3 1 4 3 0]\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_pred, y_test)\ncm\n\narray([[1, 7, 1, 0, 0, 0, 0],\n       [0, 1, 4, 2, 1, 0, 1],\n       [1, 4, 2, 2, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0]])\n\n\nNow that I have done Naive Bayes, we can see that Naive Bayes is a more effective method at predicting the number of injuries that an athlete will have based on the prevention methods that they take. The random guess accuracy rate was approximately 0.101 whereas with Naive Bayes, it is approximately 0.142. While still low, the accuracy with Naive Bayes is higher than the accuracy gained with random guessing which means that we are learning something from the data with Naive Bayes. The low accuracy rate could indicate that the number of injuries an athlete will have is a difficult thing to predict.\n\n\nNaive Bayes + Feature Selection\nIn both of the following feature selection methods, I first create an object for the feature selection method using the chi-squared test and another for the bernoulli Naive Bayes classifier as I am working with binary data. Next the model selects and transforms the training data based on the importance of each feature. The importance is determiend using the chi-squared test. The test data is then transformed using the same selected features as the training set. Finally, the Bernoulli Naive Bayes classifier fits the selected training data from X and y, and creates predictions using the trained data.\nThe following metrics are used to measure the effectiveness of the model: accuracy, precision, recall, and f1 score. The accuracy score is one that measures overall model performance. It quantifies the proprotion of correctly predicted instances, both positive and negative, out of the total number of instaces. Precision calculates the numnber of correctly predicted positives out of all of the predicted positives. Recall is the measure of the model’s ability to corrently identify positive instances. It measures the true positives out of the true positives and false negatives combined. Finally, the F1 score is considered the harmonic mean of precision and recall.\nIt is important that we are conscious of overfitting and underfitting. Overfitting occurs when a model is too complex and fits the training data too closely, including noise in the data. This model typically has low bias and high variance. Underfitting is when a model is too simplistic to capture underlying patterns in the data. Underfit models have high bias and low variance. Optimal fitting is where the model strikes a balance between bias and variance. I think that my model is fit relatively well. I think some of the ambiguity I am facing in my data stems from the target label being difficult to predict in nature.\n\nSelectPercentile\n\nfrom sklearn.feature_selection import SelectKBest, chi2, SelectPercentile\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\nselector = SelectPercentile(chi2, percentile=10)\n\nbernoilli_nb = BernoulliNB()\n\nX_train_selected = selector.fit_transform(X_train, y_train)\nX_test_selected = selector.transform(X_test)\nbernoilli_nb.fit(X_train_selected, y_train)\ny_pred = bernoilli_nb.predict(X_test_selected)\n\n\naccuracy = bernoilli_nb.score(X_test_selected, y_test)\nprint(f\"Accuracy Score: {accuracy}\")\nprecision_macro = precision_score(y_test, y_pred, average=\"macro\")\nprint(f\"Precision (Macro): {precision_macro}\")\nprecision_weighted = precision_score(y_test, y_pred, average=\"weighted\")\nprint(f\"Precision (Weighted): {precision_weighted}\")\nrecall_macro = recall_score(y_test, y_pred, average=\"macro\")\nprint(f\"Recall (Macro): {recall_macro}\")\nrecall_weighted = recall_score(y_test, y_pred, average=\"weighted\")\nprint(f\"Recall (Weighted): {recall_weighted}\")\nf1_macro = f1_score(y_test, y_pred, average=\"macro\")\nprint(f\"F1 Score (Macro): {f1_macro}\")\nf1_weighted = f1_score(y_test, y_pred, average=\"weighted\")\nprint(f\"F1 Score (Weighted): {f1_weighted}\")\n\nAccuracy Score: 0.25\nPrecision (Macro): 0.03571428571428571\nPrecision (Weighted): 0.0625\nRecall (Macro): 0.14285714285714285\nRecall (Weighted): 0.25\nF1 Score (Macro): 0.05714285714285715\nF1 Score (Weighted): 0.1\n\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\nIn my future work on this project, I will need to go back to see how SelectPercentile is subsetting its features.\n\n\nSelectKBest\n\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\nselector = SelectKBest(chi2, k=\"all\")\n\nbernoilli_nb = BernoulliNB()\n\nX_train_selected = selector.fit_transform(X_train, y_train)\nX_test_selected = selector.transform(X_test)\nbernoilli_nb.fit(X_train_selected, y_train)\ny_pred = bernoilli_nb.predict(X_test_selected)\n\n\naccuracy = bernoilli_nb.score(X_test_selected, y_test)\nprint(f\"Accuracy Score: {accuracy}\")\nprecision_macro = precision_score(y_test, y_pred, average=\"macro\")\nprint(f\"Precision (Macro): {precision_macro}\")\nprecision_weighted = precision_score(y_test, y_pred, average=\"weighted\")\nprint(f\"Precision (Weighted): {precision_weighted}\")\nrecall_macro = recall_score(y_test, y_pred, average=\"macro\")\nprint(f\"Recall (Macro): {recall_macro}\")\nrecall_weighted = recall_score(y_test, y_pred, average=\"weighted\")\nprint(f\"Recall (Weighted): {recall_weighted}\")\nf1_macro = f1_score(y_test, y_pred, average=\"macro\")\nprint(f\"F1 Score (Macro): {f1_macro}\")\nf1_weighted = f1_score(y_test, y_pred, average=\"weighted\")\nprint(f\"F1 Score (Weighted): {f1_weighted}\")\n\n\n\nAccuracy Score: 0.14285714285714285\nPrecision (Macro): 0.06031746031746032\nPrecision (Weighted): 0.10555555555555556\nRecall (Macro): 0.12414965986394558\nRecall (Weighted): 0.14285714285714285\nF1 Score (Macro): 0.07319291352904798\nF1 Score (Weighted): 0.11262686892938993\n\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\n\ny_test\n\narray([2, 7, 2, 2, 2, 2, 1, 0, 2, 1, 1, 1, 1, 3, 1, 3, 1, 1, 1, 1, 6, 2,\n       1, 3, 1, 4, 3, 0])\n\n\n\ny_pred\n\narray([0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,\n       1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,\n       0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n       0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n       1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 0, 1, 1], dtype=int32)\n\n\nI tested out two feature selection methods, SelectKBest and SelectPercentile. SelectKBest returned the same accuracy that Naive Bayes did which was 0.142. The accuracy with the SelectPercentile method,however, significantly increased from the one obtained only using Naive Bayes as it was 0.25.\n\nConfusion Matrix\n\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\n\ncm = confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(8,6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\n\nText(0.5, 1.0, 'Confusion Matrix')\n\n\n\n\n\nThe confusion matrix made up of my predicted and test data is quite interesting. I recognize that there is an abudance of zeros in the right side of the data which prompts further analysis about how effective the model is on estimate relationships between the data. Based on what I see, I think that it is likely that the data got complicated with as many features as it selected, so it decided to only focus on predicting instances where athletes had 0, 1, or 2 injuries.\n\n\nActual vs Predicted Number of Injuries Plotted\n\nplt.figure(figsize=(8, 6))\nplt.plot(y_test, label='Actual')\nplt.plot(y_pred, label='Predicted')\nplt.xlabel('Sample')\nplt.ylabel('Number of Injuries')\nplt.title('Comparison between Actual and Predicted Number of Injuries')\nplt.legend()\nplt.show()\n\n\n\n\nThis line graph of the predicted data vs actual data is not very specific, however, it does show us that based on the features that the model used, the number of injuries an athlete may have is a complicated thing to predict.\n\n\n\nChi-Squared Test Scores of Selected Features\n\nfeature_scores = selector.scores_\nselected_feature_names = [col for col, mask in zip(X, selector.get_support()) if mask]\nplt.figure(figsize=(10, 6))\nplt.bar(selected_feature_names, feature_scores)\nplt.xlabel('Features')\nplt.ylabel('Score')\nplt.title('Chi-Squared Test Scores of Selected Features')\nplt.xticks(rotation=90)\nplt.show()\n\n\n\n\nThis graph shows what features are more associated with a higher number of injuries. The graph indicates that instances where athletes braced, taped, or used shoe insoles were more associated with a higher number of injuries, whereas instances where athletes utilized stretching, warming up, and strength exercises were more associated with a lower number of instances. In other words, stretching, warming up, and performing strengthening exercises are more likely to help prevent an injury. When we contextualize the results we understand that the instances where face masks or medical corsets are less common as they are used when the injury is more severe.\n\n\n\nConclusion Record Data\nThe findings of using Naive Bayes and Feature Selection on this set of record data is that overall, the number of injuries that an athlete has is a hard thing to predict based on the prevention measures that they utilize. This data will be documented using this website, as well as on my GitHub. When formatted in presentations or reports, this data would be best represented in a tabular or graphic format for easy readability.\n\n\n\nFeature Selection for Text Data\n\nData Set Up\n\ndf_features.columns\n\nprevention_stretching = \"Athletes can help protect themselves by preparing before and after a game or practice session by warming up muscles and then stretching. Exercises can include forward lunges, side lunges, standing quad stretch, seated straddle lotus, seated side straddle, seated toe touch, and the knees to chest stretch. Hold each stretch for 20 seconds. \"\nprevention_warm_up = \"Warming up involves increasing the body's core temperature, heart rate, respiratory rate, and the body's muscle temperatures. By increasing muscle temperature, muscles become more loose and pliable, and by increasing heart rate and resipiatory rate, blood flood increases which helps to increase delivery of oxygen and nutrients to muscles. Warm up exercises include dynamic stretches, light bike riding, light jogging, jumproping, etc. \"\nprevention_specific_strength = \"Specific strength exercises can refer to a wide range of exercises, from which a few are selected based each athlete. These exercises can range from muscle group specific weight-lifting exercises, to physical therapy-like exercises. These exercises depend on each athlete and are hard to define. \"\nprevention_bracing = \"Basic braces provide general support and compression to specific areas of the body. More complex braces can do the same things, as well as promote healing, necessarily restrict movement, take weight off of an injury, etc. \"\nprevention_taping = \"Taping can be used to reduce the range of motion at a joint and decrease swelling, which in turn can alleviate pain and prevent further injury. \"\nprevention_shoe_insoles = \"Orthotics can alignment of an athlete's feet, ankles, knees, hips and back which can help prevent injuries. They can also absorb shock from impact of running to reduce stress on the athlete's joints and tissues. \"\nprevention_face_masks = \"Athletic face masks can be used to protect maxillary, nasal, zygomatic and orbital injuries. These are worn in sports where a face injury could possible occur. \"\nprevention_medical_corset = \"A medical corset is a corset that can be worn to help an athlete stablize their spine after a fracture or surgery. It will remind the athlete to not move in certain directors or to move more slowly to prevent causing further injury. \"\n\nprevention_definitions = {\n    \"Prevention Measure Stretching\": prevention_stretching,\n    \"Prevention Measure Warm Up\": prevention_warm_up,\n    'Prevention Measure Specific Strength Exercises': prevention_specific_strength,\n    'Prevention Measure Bracing': prevention_bracing, \n    'Prevention Measure Taping': prevention_taping,\n    'Prevention Measure Shoe Insoles': prevention_shoe_insoles, \n    'Prevention Measure Face Masks': prevention_face_masks,\n    'Prevention Measure Medical Corset': prevention_medical_corset\n}\n\ncols_keep = ['Number of Injuries', 'Prevention Measure Stretching', 'Prevention Measure Warm Up',\n       'Prevention Measure Specific Strength Exercises',\n       'Prevention Measure Bracing', 'Prevention Measure Taping',\n       'Prevention Measure Shoe Insoles', 'Prevention Measure Face Masks',\n       'Prevention Measure Medical Corset']\nprevention_cols = ['Prevention Measure Stretching', 'Prevention Measure Warm Up',\n       'Prevention Measure Specific Strength Exercises',\n       'Prevention Measure Bracing', 'Prevention Measure Taping',\n       'Prevention Measure Shoe Insoles', 'Prevention Measure Face Masks',\n       'Prevention Measure Medical Corset']\n\n\nsi_subset = soccer_injury[cols_keep]\nsi_subset_prevention = si_subset[prevention_cols]\nsi_subset_prevention.head()\n\n\n\n\n\n\n\n\nPrevention Measure Stretching\nPrevention Measure Warm Up\nPrevention Measure Specific Strength Exercises\nPrevention Measure Bracing\nPrevention Measure Taping\nPrevention Measure Shoe Insoles\nPrevention Measure Face Masks\nPrevention Measure Medical Corset\n\n\n\n\n0\n1\n0\n1\n0\n0\n0\n0\n0\n\n\n1\n1\n1\n0\n0\n0\n0\n0\n0\n\n\n2\n1\n0\n0\n0\n0\n1\n0\n0\n\n\n3\n1\n1\n1\n0\n0\n0\n0\n0\n\n\n4\n1\n1\n0\n0\n1\n0\n0\n0\n\n\n\n\n\n\n\n\nsi_subset[\"Prevention Measures Definitions\"] = \"\"\n\n/var/folders/kw/p7j6fdpx3vvgm6c0krhwpvb80000gn/T/ipykernel_15152/2335233446.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  si_subset[\"Prevention Measures Definitions\"] = \"\"\n\n\n\nfor col in si_subset.columns:\n    if col.startswith(\"Prevention\"):\n        si_subset[\"Prevention Measures Definitions\"] += si_subset[col].apply(\n            lambda x: prevention_definitions[col] if x == 1 else \"\"\n        )\n\n/var/folders/kw/p7j6fdpx3vvgm6c0krhwpvb80000gn/T/ipykernel_15152/236569242.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  si_subset[\"Prevention Measures Definitions\"] += si_subset[col].apply(\n\n\n\nsi_subset.head()\n\n\n\n\n\n\n\n\nNumber of Injuries\nPrevention Measure Stretching\nPrevention Measure Warm Up\nPrevention Measure Specific Strength Exercises\nPrevention Measure Bracing\nPrevention Measure Taping\nPrevention Measure Shoe Insoles\nPrevention Measure Face Masks\nPrevention Measure Medical Corset\nPrevention Measures Definitions\n\n\n\n\n0\n6\n1\n0\n1\n0\n0\n0\n0\n0\nAthletes can help protect themselves by prepar...\n\n\n1\n2\n1\n1\n0\n0\n0\n0\n0\n0\nAthletes can help protect themselves by prepar...\n\n\n2\n7\n1\n0\n0\n0\n0\n1\n0\n0\nAthletes can help protect themselves by prepar...\n\n\n3\n1\n1\n1\n1\n0\n0\n0\n0\n0\nAthletes can help protect themselves by prepar...\n\n\n4\n2\n1\n1\n0\n0\n1\n0\n0\n0\nAthletes can help protect themselves by prepar...\n\n\n\n\n\n\n\n\n\nText Pre-processing\n\nimport nltk\nnltk.download('punkt')\nnltk.download('wordnet')\n\n[nltk_data] Downloading package punkt to\n[nltk_data]     /Users/reneedemaio/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/reneedemaio/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n\n\nTrue\n\n\n\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem.wordnet import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n\n\nfrom nltk.corpus import stopwords\n\nfrom tqdm import tqdm\ntqdm.pandas()\n\nstopwords = set(stopwords.words(\"english\"))\n\n\ncustom_stopwords = [\"athlete\", \"include\", \"these\", \"further\", \"like\", 'a', 'about', 'all', 'also', 'an', 'and', 'are', 'as', 'at',\n    'be', 'both', 'but', 'by',\n    'can',\n    'do', \"etc\",\n    'for', 'from',\n    'get', 'go',\n    'had', 'have',\n    'i', 'if', 'in', 'is', 'it',\n    'me', 'more', 'my',\n    'no', 'not',\n    'of', 'on', 'one', 'or', 'out',\n    'should', \"should've\", 'so',\n    'take', 'than', 'that', 'the', 'this', 'to', 'too',\n    'up',\n    'very',\n    'want', 'was', 'we', 'were', 'what', 'where', 'which', 'with', 'would', \"would've\",\n    'you', 'your']\ncustom_stoplemmas = [\n    's'\n]\n\n\nimport string\nfrom collections import Counter\ntoken_counter = Counter()\n\ndef remove_special_chars(token):\n  return token.translate(str.maketrans('', '', string.punctuation))\n\ndef remove_digits(token):\n  return ''.join([c for c in token if not c.isdigit()])\n\n\ndef clean_def(definitions):\n  definitions_cleaned = definitions.lower()\n  definitions_applied = sent_tokenize(definitions_cleaned)\n  clean_applied = []\n  for d in definitions_applied:\n    def_tokens = word_tokenize(d)\n    df_tokens_cleaned = [t for t in def_tokens if t not in custom_stopwords]\n    df_tokens_cleaned = [remove_digits(t) for t in df_tokens_cleaned]\n    df_tokens_cleaned = [t.replace(\"-\", \" \") for t in df_tokens_cleaned]\n    df_tokens_cleaned = [remove_special_chars(t) for t in df_tokens_cleaned]\n    df_tokens_cleaned = [t for t in df_tokens_cleaned if len(t) &gt; 0]\n    df_tokens_cleaned = [lemmatizer.lemmatize(t) for t in df_tokens_cleaned]\n    df_tokens_cleaned = [t for t in df_tokens_cleaned if t not in custom_stoplemmas]\n    token_counter.update(df_tokens_cleaned)\n    clean_apply = ' '.join(df_tokens_cleaned)\n    clean_applied.append(clean_apply)\n  review_final = \". \".join(clean_applied)\n  return review_final\nsi_subset[\"Prevention Measures Definitions Cleaned\"] = si_subset[\"Prevention Measures Definitions\"].progress_apply(clean_def)\n\n100%|██████████| 139/139 [00:00&lt;00:00, 828.23it/s]\n/var/folders/kw/p7j6fdpx3vvgm6c0krhwpvb80000gn/T/ipykernel_15152/3215868529.py:32: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  si_subset[\"Prevention Measures Definitions Cleaned\"] = si_subset[\"Prevention Measures Definitions\"].progress_apply(clean_def)\n\n\n\nsi_subset.head()\n\n\n\n\n\n\n\n\nNumber of Injuries\nPrevention Measure Stretching\nPrevention Measure Warm Up\nPrevention Measure Specific Strength Exercises\nPrevention Measure Bracing\nPrevention Measure Taping\nPrevention Measure Shoe Insoles\nPrevention Measure Face Masks\nPrevention Measure Medical Corset\nPrevention Measures Definitions\nPrevention Measures Definitions Cleaned\n\n\n\n\n0\n6\n1\n0\n1\n0\n0\n0\n0\n0\nAthletes can help protect themselves by prepar...\nathlete help protect themselves preparing befo...\n\n\n1\n2\n1\n1\n0\n0\n0\n0\n0\n0\nAthletes can help protect themselves by prepar...\nathlete help protect themselves preparing befo...\n\n\n2\n7\n1\n0\n0\n0\n0\n1\n0\n0\nAthletes can help protect themselves by prepar...\nathlete help protect themselves preparing befo...\n\n\n3\n1\n1\n1\n1\n0\n0\n0\n0\n0\nAthletes can help protect themselves by prepar...\nathlete help protect themselves preparing befo...\n\n\n4\n2\n1\n1\n0\n0\n1\n0\n0\n0\nAthletes can help protect themselves by prepar...\nathlete help protect themselves preparing befo...\n\n\n\n\n\n\n\n\ntoken_counter.most_common(25)\n\n[('exercise', 478),\n ('muscle', 473),\n ('stretch', 456),\n ('seated', 381),\n ('rate', 300),\n ('lunge', 254),\n ('side', 254),\n ('straddle', 254),\n ('help', 241),\n ('increasing', 225),\n ('temperature', 225),\n ('each', 219),\n ('warming', 202),\n ('body', 166),\n ('knee', 165),\n ('heart', 150),\n ('increase', 150),\n ('light', 150),\n ('range', 142),\n ('after', 128),\n ('athlete', 127),\n ('protect', 127),\n ('themselves', 127),\n ('preparing', 127),\n ('before', 127)]\n\n\n\n\nVectorization\nThis text data is trained using vectorization. First the corpus is created and is fitted using CountVectorizer. This finds the frequency of the words so the most common words can then be easily found. Then, KMeans clustering will be used to assign each data point to a cluster.\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncorpus = si_subset[\"Prevention Measures Definitions Cleaned\"].values\n\n\nmax_document_freq = 0.4\nmin_document_count = 6\n\ncv = CountVectorizer(max_df=max_document_freq, min_df=min_document_count)\nX = cv.fit_transform(corpus)\nprint(X)\n\nfeature_names = cv.get_feature_names_out()\nfeature_names\n\ncv_ng = CountVectorizer(max_df=max_document_freq, min_df=min_document_count,\n                              stop_words=custom_stopwords)\nX_ng = cv_ng.fit_transform(corpus)\nX_ng.shape\n\n  (0, 39)   1\n  (0, 33)   1\n  (0, 51)   1\n  (0, 14)   1\n  (0, 37)   1\n  (0, 6)    1\n  (0, 17)   1\n  (0, 22)   1\n  (0, 30)   1\n  (0, 44)   1\n  (0, 23)   1\n  (0, 13)   1\n  (0, 18)   1\n  (0, 12)   1\n  (2, 28)   1\n  (2, 1)    1\n  (2, 15)   1\n  (2, 3)    1\n  (2, 20)   1\n  (2, 5)    1\n  (2, 45)   1\n  (2, 0)    1\n  (2, 38)   1\n  (2, 21)   1\n  (2, 35)   1\n  : :\n  (136, 42) 1\n  (136, 48) 1\n  (136, 2)  1\n  (136, 29) 1\n  (137, 28) 1\n  (137, 1)  1\n  (137, 15) 1\n  (137, 3)  1\n  (137, 20) 1\n  (137, 5)  1\n  (137, 45) 1\n  (137, 0)  1\n  (137, 38) 1\n  (137, 21) 1\n  (137, 35) 1\n  (137, 40) 1\n  (137, 47) 1\n  (137, 43) 1\n  (137, 49) 1\n  (137, 24) 1\n  (137, 11) 1\n  (137, 42) 1\n  (137, 48) 1\n  (137, 2)  1\n  (137, 29) 1\n\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ve'] not in stop_words.\n  warnings.warn(\n\n\n(139, 51)\n\n\n\n\nKMeans Clustering\n\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(init=\"random\", n_clusters=2, n_init=4, random_state=5000)\nkmeans.fit(X)\ny_pred = kmeans.predict(X)\ny_pred\n\narray([0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,\n       1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,\n       0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n       0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n       1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 0, 1, 1], dtype=int32)\n\n\n\nlen(y_pred[y_pred==0])\n\n46\n\n\n\nlen(y_pred[y_pred==1])\n\n93\n\n\n\nfrom sklearn.metrics import pairwise_distances_argmin_min\n\ncentroids = kmeans.cluster_centers_\ncentroids.shape\n\n(2, 52)\n\n\n\nargmins, mins = pairwise_distances_argmin_min(centroids, X)\nargmins, mins\n\n(array([0, 1]), array([1.63260725, 1.44662624]))\n\n\n\nargmins_0 = argmins[0]\ny_pred[argmins_0]\n\n0\n\n\n\nsi_subset.iloc[argmins_0][[\"Prevention Measures Definitions Cleaned\"]]\n\nPrevention Measures Definitions Cleaned    athlete help protect themselves preparing befo...\nName: 0, dtype: object\n\n\n\nargmins_1 = argmins[1]\ny_pred[argmins_1]\n\n1\n\n\n\nsi_subset.iloc[argmins_1][\"Prevention Measures Definitions Cleaned\"]\n\n'athlete help protect themselves preparing before after game practice session warming muscle then stretching. exercise forward lunge side lunge standing quad stretch seated straddle lotus seated side straddle seated toe touch knee chest stretch. hold each stretch second. warming involves increasing body core temperature heart rate respiratory rate body muscle temperature. increasing muscle temperature muscle become loose pliable increasing heart rate resipiatory rate blood flood increase help increase delivery oxygen nutrient muscle. warm exercise dynamic stretch light bike riding light jogging jumproping'\n\n\n\nsi_subset[\"y_pred\"] = y_pred\nsi_subset.head()\n\n/var/folders/kw/p7j6fdpx3vvgm6c0krhwpvb80000gn/T/ipykernel_15152/419468994.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  si_subset[\"y_pred\"] = y_pred\n\n\n\n\n\n\n\n\n\nNumber of Injuries\nPrevention Measure Stretching\nPrevention Measure Warm Up\nPrevention Measure Specific Strength Exercises\nPrevention Measure Bracing\nPrevention Measure Taping\nPrevention Measure Shoe Insoles\nPrevention Measure Face Masks\nPrevention Measure Medical Corset\nPrevention Measures Definitions\nPrevention Measures Definitions Cleaned\ny_pred\n\n\n\n\n0\n6\n1\n0\n1\n0\n0\n0\n0\n0\nAthletes can help protect themselves by prepar...\nathlete help protect themselves preparing befo...\n0\n\n\n1\n2\n1\n1\n0\n0\n0\n0\n0\n0\nAthletes can help protect themselves by prepar...\nathlete help protect themselves preparing befo...\n1\n\n\n2\n7\n1\n0\n0\n0\n0\n1\n0\n0\nAthletes can help protect themselves by prepar...\nathlete help protect themselves preparing befo...\n1\n\n\n3\n1\n1\n1\n1\n0\n0\n0\n0\n0\nAthletes can help protect themselves by prepar...\nathlete help protect themselves preparing befo...\n0\n\n\n4\n2\n1\n1\n0\n0\n1\n0\n0\n0\nAthletes can help protect themselves by prepar...\nathlete help protect themselves preparing befo...\n1\n\n\n\n\n\n\n\n\npd.crosstab(si_subset['Number of Injuries'], si_subset['y_pred'])\n\n\n\n\n\n\n\ny_pred\n0\n1\n\n\nNumber of Injuries\n\n\n\n\n\n\n0\n6\n16\n\n\n1\n11\n30\n\n\n2\n15\n27\n\n\n3\n5\n10\n\n\n4\n5\n6\n\n\n5\n2\n1\n\n\n6\n2\n0\n\n\n7\n0\n2\n\n\n10\n0\n1\n\n\n\n\n\n\n\n\npd.crosstab(si_subset['Number of Injuries'], si_subset['y_pred'], normalize='all', margins=True)\n\n\n\n\n\n\n\ny_pred\n0\n1\nAll\n\n\nNumber of Injuries\n\n\n\n\n\n\n\n0\n0.043165\n0.115108\n0.158273\n\n\n1\n0.079137\n0.215827\n0.294964\n\n\n2\n0.107914\n0.194245\n0.302158\n\n\n3\n0.035971\n0.071942\n0.107914\n\n\n4\n0.035971\n0.043165\n0.079137\n\n\n5\n0.014388\n0.007194\n0.021583\n\n\n6\n0.014388\n0.000000\n0.014388\n\n\n7\n0.000000\n0.014388\n0.014388\n\n\n10\n0.000000\n0.007194\n0.007194\n\n\nAll\n0.330935\n0.669065\n1.000000\n\n\n\n\n\n\n\n\npd.crosstab(si_subset['Number of Injuries'], si_subset['y_pred'], normalize='index')\n\n\n\n\n\n\n\ny_pred\n0\n1\n\n\nNumber of Injuries\n\n\n\n\n\n\n0\n0.272727\n0.727273\n\n\n1\n0.268293\n0.731707\n\n\n2\n0.357143\n0.642857\n\n\n3\n0.333333\n0.666667\n\n\n4\n0.454545\n0.545455\n\n\n5\n0.666667\n0.333333\n\n\n6\n1.000000\n0.000000\n\n\n7\n0.000000\n1.000000\n\n\n10\n0.000000\n1.000000\n\n\n\n\n\n\n\nThis data indicates that if an athlete has 4 or fewer injuries, they are much more likely to be doing a prevention measure than not. If an athlete has 5 or 6 injuries, it is more likely than they are not doing any prevention measures. Athletes with 7 or 10 injuries can be considered outliers.\n\n\nVisualizing Clusters\n\nfrom sklearn.manifold import TSNE\n\ndef plot_top_words(model, feature_names, n_top_words, title):\n    fig, axes = plt.subplots(2, 3, figsize=(30, 15)) #, sharex=True)\n    axes = axes.flatten()\n    for topic_idx, topic in enumerate(model.components_):\n        top_features_ind = topic.argsort()[-n_top_words:]\n        top_features = feature_names[top_features_ind]\n        weights = topic[top_features_ind]\n\n        ax = axes[topic_idx]\n        ax.barh(top_features, weights, height=0.7)\n        ax.set_title(f\"Topic {topic_idx + 1}\", fontdict={\"fontsize\": 30})\n        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n        for i in \"top right left\".split():\n            ax.spines[i].set_visible(False)\n        fig.suptitle(title, fontsize=40)\n\n    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n    plt.show()\n\n\nn_top_words = 20\nnum_topics = 6\n\n\nfrom sklearn.decomposition import LatentDirichletAllocation, NMF\nnmf = NMF(\n    n_components=num_topics,\n    random_state=5000,\n    beta_loss=\"frobenius\",\n)\nnmf.fit(X)\n\nNMF(n_components=6, random_state=5000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.NMFNMF(n_components=6, random_state=5000)\n\n\n\nplot_top_words(nmf, feature_names, n_top_words, \"Topics in NMF model\")\n\n\n\n\nThis set of graphs groups together language that tends to be correlated. For example, topic 3 is likely to be grouped around words that relate to bracing as it includes “brace”, “movement”, “compression”, “restrict”, etc.\n\nfrom sklearn.manifold import TSNE\n\nnmf_ng = NMF(\n    n_components=num_topics,\n    random_state=5000,\n    beta_loss=\"frobenius\",\n    max_iter=1000\n)\nnmf_ng.fit(X_ng)\n\nnmf_ng_components = nmf_ng.components_\nnmf_ng_components.shape\n\n(6, 51)\n\n\n\nW = nmf_ng.transform(X_ng)\nW.shape\n\n(139, 6)\n\n\n\nW[:5,:]\n\narray([[5.95204182e-10, 5.90725194e-01, 0.00000000e+00, 0.00000000e+00,\n        0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n        0.00000000e+00, 0.00000000e+00],\n       [4.34564308e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n        7.35380448e-01, 0.00000000e+00],\n       [5.95204182e-10, 5.90725194e-01, 0.00000000e+00, 0.00000000e+00,\n        0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 2.12556806e-19, 4.61762617e-01,\n        0.00000000e+00, 0.00000000e+00]])\n\n\n\nW_df = pd.DataFrame(W)\nW_df[\"cluster\"] = W_df.idxmax(axis=1)\n\n\nW_df[\"cluster\"].value_counts\n\n&lt;bound method IndexOpsMixin.value_counts of 0      1\n1      0\n2      4\n3      1\n4      3\n      ..\n134    0\n135    2\n136    1\n137    4\n138    0\nName: cluster, Length: 139, dtype: int64&gt;\n\n\n\nW_df.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\ncluster\n\n\n\n\n0\n5.952042e-10\n0.590725\n0.000000e+00\n0.000000\n0.00000\n0.0\n1\n\n\n1\n0.000000e+00\n0.000000\n0.000000e+00\n0.000000\n0.00000\n0.0\n0\n\n\n2\n4.345643e-04\n0.000000\n0.000000e+00\n0.000000\n0.73538\n0.0\n4\n\n\n3\n5.952042e-10\n0.590725\n0.000000e+00\n0.000000\n0.00000\n0.0\n1\n\n\n4\n0.000000e+00\n0.000000\n2.125568e-19\n0.461763\n0.00000\n0.0\n3\n\n\n\n\n\n\n\n\ntsne_model = TSNE(\n    init='random',\n    random_state=5000\n)\ntsne_embedding = tsne_model.fit_transform(W)\ntsne_df = pd.DataFrame(tsne_embedding, columns=['x','y'])\ntsne_df.shape\n\n(139, 2)\n\n\n\ntsne_df.head()\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n0.755610\n-11.911394\n\n\n1\n-1.636704\n-0.801197\n\n\n2\n5.845468\n3.555564\n\n\n3\n0.525336\n-11.894978\n\n\n4\n3.503934\n-3.436377\n\n\n\n\n\n\n\n\ntsne_df['cluster'] = W_df['cluster']\ntsne_df['Definitions'] = si_subset[\"Prevention Measures Definitions\"]\n\n\ntsne_df.head()\n\n\n\n\n\n\n\n\nx\ny\ncluster\nDefinitions\n\n\n\n\n0\n0.755610\n-11.911394\n1\nAthletes can help protect themselves by prepar...\n\n\n1\n-1.636704\n-0.801197\n0\nAthletes can help protect themselves by prepar...\n\n\n2\n5.845468\n3.555564\n4\nAthletes can help protect themselves by prepar...\n\n\n3\n0.525336\n-11.894978\n1\nAthletes can help protect themselves by prepar...\n\n\n4\n3.503934\n-3.436377\n3\nAthletes can help protect themselves by prepar...\n\n\n\n\n\n\n\n\ntsne_sample_size = 100\ntsne_sample_df = tsne_df.sample(tsne_sample_size, random_state=5000)\n\n\nsns.scatterplot(tsne_sample_df, x='x', y='y', hue='cluster')\n\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/reneedemaio/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n\n\n&lt;Axes: xlabel='x', ylabel='y'&gt;\n\n\n\n\n\nThis is a general graph of the clusters that are present within the data. These clusters represent different prevention measures that are used. However, this graph isn’t very information on what each cluster represents.\n\nimport textwrap\nmywrap = lambda x: textwrap.wrap(x, width=60)\n\n\ntsne_sample_df.shape\n\n(100, 4)\n\n\n\ntsne_sample_df['Definitions'] = tsne_sample_df['Definitions'].apply(lambda x: x if type(x) == str else '')\ntsne_sample_df['Definitions_wrap'] = tsne_sample_df['Definitions'].apply(lambda x: '&lt;br&gt;'.join(mywrap(x)))\n\n\nimport plotly.express as px\nreview_fig = px.scatter(tsne_sample_df, x='x', y='y', color='cluster',\n                        hover_data=['Definitions_wrap'], template='simple_white')\nreview_fig.update(layout_coloraxis_showscale=False)\nreview_fig.update_traces(marker=dict(size=12),selector=dict(mode='markers'))\nreview_fig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nThis clustering graph is much more specific as its interactive nature allows us to see what factors are grouping each cluster together. For example, the yellow cluster is likely clustered around orthotics as both of these points include orthotics as they main point of commonality. The green cluster is likely using taping as its point of commonality. It is important to note the prevention measures such as stretching and warming up are quite universal, so they don’t hold as much influence in terms of determining their own cluster."
  },
  {
    "objectID": "tabs/conclusion/conclusion.html",
    "href": "tabs/conclusion/conclusion.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "1+1"
  },
  {
    "objectID": "tabs/data/data.html",
    "href": "tabs/data/data.html",
    "title": "Basketball Injury Data from 2010-2018",
    "section": "",
    "text": "This data comes from this website\n\nimport requests\nimport pandas as pd\nimport requests\n\ncsv_url = \"https://raw.githubusercontent.com/anly501/dsan-5000-project-rennyd123/main/dsan-website/5000-website/data/raw/injuries_2010-2020.csv\"\n\n\nbball_injury_data = pd.read_csv(csv_url)\nprint(bball_injury_data.head())\n\n         Date     Team Acquired   Relinquished  \\\n0  2010-10-03    Bulls      NaN  Carlos Boozer   \n1  2010-10-06  Pistons      NaN  Jonas Jerebko   \n2  2010-10-06  Pistons      NaN  Terrico White   \n3  2010-10-08  Blazers      NaN     Jeff Ayres   \n4  2010-10-08     Nets      NaN    Troy Murphy   \n\n                                               Notes  \n0  fractured bone in right pinky finger (out inde...  \n1      torn right Achilles tendon (out indefinitely)  \n2  broken fifth metatarsal in right foot (out ind...  \n3          torn ACL in right knee (out indefinitely)  \n4             strained lower back (out indefinitely)"
  }
]